{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (Detecting pathological gamblers)\n",
    "### Using contextualized language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_FILE = \"../posts.csv\"\n",
    "TRAIN_TOKEN=\"../train_df.csv\"\n",
    "TEST_TOKEN=\"../test_df.csv\"\n",
    "GENERAL_MODELS=\"../Models\"\n",
    "ROLLING_WINDOW_SIZE=10\n",
    "LM_MODEL='all-mpnet-base-v2'\n",
    "#'all-MiniLM-L6-v2'  'all-mpnet-base-v2'\n",
    "TASK=1\n",
    "CONVERTED=True\n",
    "SENT_MEASURED=True\n",
    "BASELINE_COMP=False\n",
    "MODEL_PATH =f\"{GENERAL_MODELS}/LM/win_{ROLLING_WINDOW_SIZE}_{LM_MODEL}\" \n",
    "max_lengths={1:64,3:128,5:256,10:512}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(MODEL_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opening resulting dataset with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(df, window_size,stride, field):\n",
    "    res_map={}\n",
    "    for user in df['User'].unique():\n",
    "        user_df = df[df['User']==user]\n",
    "        res_map[user]=(user_df['Label'].values[0],{})\n",
    "        posts = user_df[field].values\n",
    "        iteration=0\n",
    "        for i in range(0,len(posts),stride):\n",
    "            res_map[user][1][iteration]=' '.join((posts[i:i+window_size]))\n",
    "            iteration+=1\n",
    "    result_df = pd.DataFrame([(k,k1,v1,v[0]) for k,v in res_map.items() for k1,v1 in v[1].items()], columns = ['User','Window_id','Text','Label'])\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Store sentences & embeddings on disc\n",
    "def save_embeddings(filepath, embeddings):\n",
    "    with open(filepath, \"wb\") as fOut:\n",
    "        pickle.dump({ 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    return stored_embeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "seed=23\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "if not SENT_MEASURED:\n",
    "    train_df = pd.read_csv(TRAIN_TOKEN, sep='\\t')\n",
    "    test_df = pd.read_csv(TEST_TOKEN, sep='\\t')\n",
    "    train_df = rolling_window(train_df,ROLLING_WINDOW_SIZE,1,'Raw')\n",
    "    test_df = rolling_window(test_df,ROLLING_WINDOW_SIZE,1,'Raw')\n",
    "\n",
    "    from textblob import TextBlob\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    train_df['TB'] = train_df['Text'].apply(lambda text: TextBlob(text).sentiment)\n",
    "    train_df['VADER'] = train_df['Text'].apply(lambda text: sia.polarity_scores(text))\n",
    "    train_df['polarity'] = train_df['TB'].apply(lambda tb: (tb[0]+1/2))\n",
    "    train_df['subjectivity'] = train_df['TB'].apply(lambda tb: tb[1])\n",
    "    train_df['negativity'] = train_df['VADER'].apply(lambda v: v['neg'])\n",
    "    train_df['positivity'] = train_df['VADER'].apply(lambda v: v['pos'])\n",
    "    train_df['neutrality'] = train_df['VADER'].apply(lambda v: v['neu'])\n",
    "    train_df['compound'] = train_df['VADER'].apply(lambda v: (v['compound']+1)/2)\n",
    "    train_df.drop(['VADER','TB'], inplace=True, axis=1)\n",
    "\n",
    "    test_df['TB'] = test_df['Text'].apply(lambda text: TextBlob(text).sentiment)\n",
    "    test_df['VADER'] = test_df['Text'].apply(lambda text: sia.polarity_scores(text))\n",
    "    test_df['polarity'] = test_df['TB'].apply(lambda tb: (tb[0]+1/2))\n",
    "    test_df['subjectivity'] = test_df['TB'].apply(lambda tb: tb[1])\n",
    "    test_df['negativity'] = test_df['VADER'].apply(lambda v: v['neg'])\n",
    "    test_df['positivity'] = test_df['VADER'].apply(lambda v: v['pos'])\n",
    "    test_df['neutrality'] = test_df['VADER'].apply(lambda v: v['neu'])\n",
    "    test_df['compound'] = test_df['VADER'].apply(lambda v: (v['compound']+1)/2)\n",
    "    test_df.drop(['VADER','TB'], inplace=True, axis=1)\n",
    "\n",
    "    train_df.to_pickle(f\"train_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n",
    "    test_df.to_pickle(f\"test_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n",
    "train_df = pd.read_pickle(f\"train_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n",
    "test_df = pd.read_pickle(f\"test_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Window_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>negativity</th>\n",
       "      <th>positivity</th>\n",
       "      <th>neutrality</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3450</td>\n",
       "      <td>0</td>\n",
       "      <td>sports betting number k in debt, feeling very ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.480570</td>\n",
       "      <td>0.443716</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.00215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3450</td>\n",
       "      <td>1</td>\n",
       "      <td>finally accepted that you cannot win gambling ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.470277</td>\n",
       "      <td>0.503152</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.00430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3450</td>\n",
       "      <td>2</td>\n",
       "      <td>blocking software betfilter has anybody used t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.462110</td>\n",
       "      <td>0.516745</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.00975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3450</td>\n",
       "      <td>3</td>\n",
       "      <td>prone to relapse when in debt? i find that whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.480744</td>\n",
       "      <td>0.531990</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.40110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3450</td>\n",
       "      <td>4</td>\n",
       "      <td>down to my last number on credit card i am num...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444203</td>\n",
       "      <td>0.531141</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.09575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97853</th>\n",
       "      <td>162</td>\n",
       "      <td>670</td>\n",
       "      <td>you sick fuck reeeeeeeeeeeeeeeeeeeeeeeeeee thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.221429</td>\n",
       "      <td>0.510119</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.25895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97854</th>\n",
       "      <td>162</td>\n",
       "      <td>671</td>\n",
       "      <td>reeeeeeeeeeeeeeeeeeeeeeeeeee this is so clearl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.67390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97855</th>\n",
       "      <td>162</td>\n",
       "      <td>672</td>\n",
       "      <td>this is so clearly satire! i really do not wan...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.67390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97856</th>\n",
       "      <td>162</td>\n",
       "      <td>673</td>\n",
       "      <td>he cannot write a story in number minutes pay ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.44965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97857</th>\n",
       "      <td>162</td>\n",
       "      <td>674</td>\n",
       "      <td>pay your respekt people!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.41205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97858 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User  Window_id                                               Text  \\\n",
       "0      3450          0  sports betting number k in debt, feeling very ...   \n",
       "1      3450          1  finally accepted that you cannot win gambling ...   \n",
       "2      3450          2  blocking software betfilter has anybody used t...   \n",
       "3      3450          3  prone to relapse when in debt? i find that whe...   \n",
       "4      3450          4  down to my last number on credit card i am num...   \n",
       "...     ...        ...                                                ...   \n",
       "97853   162        670  you sick fuck reeeeeeeeeeeeeeeeeeeeeeeeeee thi...   \n",
       "97854   162        671  reeeeeeeeeeeeeeeeeeeeeeeeeee this is so clearl...   \n",
       "97855   162        672  this is so clearly satire! i really do not wan...   \n",
       "97856   162        673  he cannot write a story in number minutes pay ...   \n",
       "97857   162        674                           pay your respekt people!   \n",
       "\n",
       "       Label  polarity  subjectivity  negativity  positivity  neutrality  \\\n",
       "0          1  0.480570      0.443716       0.215       0.168       0.617   \n",
       "1          1  0.470277      0.503152       0.220       0.179       0.601   \n",
       "2          1  0.462110      0.516745       0.218       0.193       0.589   \n",
       "3          1  0.480744      0.531990       0.194       0.221       0.585   \n",
       "4          1  0.444203      0.531141       0.206       0.223       0.571   \n",
       "...      ...       ...           ...         ...         ...         ...   \n",
       "97853      0  0.221429      0.510119       0.227       0.107       0.666   \n",
       "97854      0  0.500000      0.291667       0.098       0.147       0.755   \n",
       "97855      0  0.500000      0.291667       0.101       0.152       0.747   \n",
       "97856      0  0.500000      0.000000       0.141       0.108       0.751   \n",
       "97857      0  0.500000      0.000000       0.361       0.000       0.639   \n",
       "\n",
       "       compound  \n",
       "0       0.00215  \n",
       "1       0.00430  \n",
       "2       0.00975  \n",
       "3       0.40110  \n",
       "4       0.09575  \n",
       "...         ...  \n",
       "97853   0.25895  \n",
       "97854   0.67390  \n",
       "97855   0.67390  \n",
       "97856   0.44965  \n",
       "97857   0.41205  \n",
       "\n",
       "[97858 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not CONVERTED:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(LM_MODEL)\n",
    "\n",
    "    model.max_seq_length = max_lengths[ROLLING_WINDOW_SIZE]\n",
    "    train_sentence_embeddings = model.encode(train_df['Text'],show_progress_bar=True,\\\n",
    "                output_value='sentence_embedding', batch_size=64,convert_to_numpy=True)\n",
    "\n",
    "    val_sentence_embeddings = model.encode(test_df['Text'],show_progress_bar=True,\\\n",
    "                output_value='sentence_embedding', batch_size=64,convert_to_numpy=True)\n",
    "\n",
    "    save_embeddings(f\"./train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\",train_sentence_embeddings)\n",
    "    save_embeddings(f\"./val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\",val_sentence_embeddings)\n",
    "else:\n",
    "    train_sentence_embeddings = load_embeddings(f\"./train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "    val_sentence_embeddings = load_embeddings(f\"./val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "   \n",
    "test_df['Vector'] = pd.DataFrame(data=val_sentence_embeddings).values.tolist()\n",
    "train_df['Vector'] = pd.DataFrame(data=train_sentence_embeddings).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = pd.concat([train_df,test_df])\n",
    "full_df = full_df.sample(frac=1, random_state=seed).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import ast\n",
    "#full_df['Vector'] = full_df['Vector'].apply(lambda x: ast.literal_eval(x))\n",
    "#full_df['Vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004810236860066652,\n",
       " 0.09786351025104523,\n",
       " 0.01352181937545538,\n",
       " -0.027487656101584435,\n",
       " 0.02000696212053299,\n",
       " -0.014911876991391182,\n",
       " -0.01871095597743988,\n",
       " -0.007118774112313986,\n",
       " 0.04303239658474922,\n",
       " 0.02504473552107811,\n",
       " 0.05597524717450142,\n",
       " 0.040071628987789154,\n",
       " -0.0868198350071907,\n",
       " 0.07660741358995438,\n",
       " 0.019378187134861946,\n",
       " -0.022471707314252853,\n",
       " 0.006449539680033922,\n",
       " -0.008858040906488895,\n",
       " -0.019853604957461357,\n",
       " -0.02931733801960945,\n",
       " 0.044363025575876236,\n",
       " -0.02347172424197197,\n",
       " 0.034911420196294785,\n",
       " 0.024648107588291168,\n",
       " 0.08573662489652634,\n",
       " 0.0067977155558764935,\n",
       " 0.034997548907995224,\n",
       " 0.0020897609647363424,\n",
       " 0.052580028772354126,\n",
       " -0.011788729578256607,\n",
       " 0.040029577910900116,\n",
       " 0.028584441170096397,\n",
       " -0.003798366989940405,\n",
       " 0.019353466108441353,\n",
       " 2.112770971507416e-06,\n",
       " -0.029993761330842972,\n",
       " -0.013102305121719837,\n",
       " 0.0005886050057597458,\n",
       " -0.00562269426882267,\n",
       " -0.021098123863339424,\n",
       " 0.03521820530295372,\n",
       " -0.10440979897975922,\n",
       " -0.0007471283315680921,\n",
       " 0.012870142236351967,\n",
       " -0.06160895153880119,\n",
       " -0.035281285643577576,\n",
       " -0.00848627183586359,\n",
       " 0.02282089553773403,\n",
       " 0.03439328074455261,\n",
       " 0.000754502892959863,\n",
       " -0.013142069801688194,\n",
       " 0.02081027813255787,\n",
       " 0.012942615896463394,\n",
       " -0.016270115971565247,\n",
       " -0.01607760600745678,\n",
       " 0.03095073439180851,\n",
       " -0.02065642736852169,\n",
       " 0.028355887159705162,\n",
       " 0.0708036869764328,\n",
       " -0.014201744459569454,\n",
       " 0.000659514160361141,\n",
       " 0.03320854902267456,\n",
       " -0.054039131850004196,\n",
       " -0.020309723913669586,\n",
       " 0.008633619174361229,\n",
       " 0.051455091685056686,\n",
       " 0.0006843516021035612,\n",
       " -0.05014825239777565,\n",
       " -0.0262460820376873,\n",
       " 0.003449029987677932,\n",
       " 0.0009518633596599102,\n",
       " 0.015486209653317928,\n",
       " -0.017175300046801567,\n",
       " 0.0349481999874115,\n",
       " -0.03935733065009117,\n",
       " -0.075841523706913,\n",
       " -0.006649855989962816,\n",
       " -0.02252185344696045,\n",
       " -0.012756816111505032,\n",
       " 0.04264591634273529,\n",
       " 0.009031791239976883,\n",
       " 0.014726084657013416,\n",
       " -0.018010910600423813,\n",
       " 0.005325383972376585,\n",
       " -0.010277357883751392,\n",
       " -0.03865611553192139,\n",
       " -0.01746084913611412,\n",
       " -0.02263968624174595,\n",
       " 0.004295095801353455,\n",
       " -0.026229379698634148,\n",
       " -0.0273577943444252,\n",
       " 0.0067994860000908375,\n",
       " 0.012595287524163723,\n",
       " -0.01414530910551548,\n",
       " -0.010413519106805325,\n",
       " 0.00454308046028018,\n",
       " 0.04434405267238617,\n",
       " 0.02939821034669876,\n",
       " 0.06906343251466751,\n",
       " -0.05046378821134567,\n",
       " -0.0018926287302747369,\n",
       " 0.04938961938023567,\n",
       " -0.020355738699436188,\n",
       " 0.006270646583288908,\n",
       " 0.03322777524590492,\n",
       " -0.027221757918596268,\n",
       " -0.053973179310560226,\n",
       " 0.02726644091308117,\n",
       " -0.007569791283458471,\n",
       " 0.0582798533141613,\n",
       " -0.02686418779194355,\n",
       " 0.022659793496131897,\n",
       " 0.03241404518485069,\n",
       " -0.007979319430887699,\n",
       " 0.004333227872848511,\n",
       " -0.003983291797339916,\n",
       " -0.005156959872692823,\n",
       " 0.015161040239036083,\n",
       " -0.04279499873518944,\n",
       " 0.006719638127833605,\n",
       " 0.008669267408549786,\n",
       " 0.0268061775714159,\n",
       " 0.0003144298680126667,\n",
       " 0.014749974012374878,\n",
       " 0.03419196233153343,\n",
       " 0.061306182295084,\n",
       " -0.0038915113545954227,\n",
       " -0.016935110092163086,\n",
       " -0.019208339974284172,\n",
       " -0.03476034477353096,\n",
       " -0.01736566051840782,\n",
       " -0.01380475889891386,\n",
       " 0.027767149731516838,\n",
       " -0.008776333183050156,\n",
       " 0.004124225117266178,\n",
       " 0.012294279411435127,\n",
       " -0.018122578039765358,\n",
       " 0.05520879849791527,\n",
       " 0.032822247594594955,\n",
       " -0.02048000507056713,\n",
       " -0.04730481281876564,\n",
       " 0.027417942881584167,\n",
       " -0.013937175273895264,\n",
       " -0.02233816124498844,\n",
       " 0.0027928815688937902,\n",
       " 0.017082469537854195,\n",
       " -0.00165511853992939,\n",
       " 0.006157497875392437,\n",
       " -0.007511825300753117,\n",
       " -0.01624736562371254,\n",
       " -0.06881236284971237,\n",
       " 0.022594505921006203,\n",
       " -0.059989411383867264,\n",
       " 0.02445746399462223,\n",
       " 0.03245101496577263,\n",
       " 0.046106740832328796,\n",
       " 0.017757419496774673,\n",
       " -0.0446012057363987,\n",
       " 0.006146201863884926,\n",
       " 0.029100734740495682,\n",
       " -0.005669475998729467,\n",
       " 0.03271983563899994,\n",
       " 0.05989667773246765,\n",
       " -0.00034564605448395014,\n",
       " -0.024182042106986046,\n",
       " 0.002966785803437233,\n",
       " 0.07420049607753754,\n",
       " 0.0013928246917203069,\n",
       " 0.020300056785345078,\n",
       " -0.0008346938993781805,\n",
       " 0.007580035366117954,\n",
       " 0.017745161429047585,\n",
       " -0.011947200633585453,\n",
       " 0.015077677555382252,\n",
       " 0.010746004991233349,\n",
       " -0.12539415061473846,\n",
       " -0.013095162808895111,\n",
       " -0.06546942889690399,\n",
       " 0.0030785463750362396,\n",
       " -0.0019662517588585615,\n",
       " -0.042474281042814255,\n",
       " -0.0022594991605728865,\n",
       " 0.03248509019613266,\n",
       " -0.08178739249706268,\n",
       " 0.06527383625507355,\n",
       " 0.00913938321173191,\n",
       " 0.020814133808016777,\n",
       " 0.010247130878269672,\n",
       " 0.0057967412285506725,\n",
       " 0.025036871433258057,\n",
       " -0.03326169028878212,\n",
       " 0.015597504563629627,\n",
       " -0.0050951153971254826,\n",
       " 0.002652885625138879,\n",
       " -0.009005269967019558,\n",
       " -0.028300916776061058,\n",
       " 0.0145651176571846,\n",
       " -0.11654864251613617,\n",
       " 0.04443484917283058,\n",
       " -0.025015922263264656,\n",
       " -0.017186041921377182,\n",
       " -0.051769718527793884,\n",
       " -0.0015118187293410301,\n",
       " -0.022093884646892548,\n",
       " 0.0284471046179533,\n",
       " 0.04071936756372452,\n",
       " -0.03573140129446983,\n",
       " -0.015273429453372955,\n",
       " 0.03810916841030121,\n",
       " 0.05802452564239502,\n",
       " 0.02334773726761341,\n",
       " -0.03262067958712578,\n",
       " -0.010172433219850063,\n",
       " 0.008471721783280373,\n",
       " 0.0007110998267307878,\n",
       " 0.03485266864299774,\n",
       " -0.014267432503402233,\n",
       " -0.018499935045838356,\n",
       " -0.04172709211707115,\n",
       " -0.007488098926842213,\n",
       " 0.017466533929109573,\n",
       " 0.08068134635686874,\n",
       " -0.05878312885761261,\n",
       " -0.011505281552672386,\n",
       " 0.045943837612867355,\n",
       " -0.019546054303646088,\n",
       " 0.0052666449919342995,\n",
       " 0.01694176346063614,\n",
       " 0.055716678500175476,\n",
       " -0.005766164977103472,\n",
       " -0.006662230938673019,\n",
       " 0.0020987577736377716,\n",
       " 0.02366054058074951,\n",
       " -0.05345149710774422,\n",
       " 0.06434947997331619,\n",
       " 0.014214342460036278,\n",
       " 0.06309809535741806,\n",
       " 0.0024520449806004763,\n",
       " -0.004411541856825352,\n",
       " 0.016722077503800392,\n",
       " 0.06606272608041763,\n",
       " 0.028728781268000603,\n",
       " -0.0007426509982906282,\n",
       " 0.01693435199558735,\n",
       " 0.03213350474834442,\n",
       " 0.043197546154260635,\n",
       " 0.025869643315672874,\n",
       " -0.010493170469999313,\n",
       " -0.0378168448805809,\n",
       " 0.02375430054962635,\n",
       " 0.06632965803146362,\n",
       " -0.009259223006665707,\n",
       " -0.04860860854387283,\n",
       " -0.018243202939629555,\n",
       " -0.050596002489328384,\n",
       " 0.010020575486123562,\n",
       " -0.057130660861730576,\n",
       " -0.03448280319571495,\n",
       " -0.018063664436340332,\n",
       " 0.07238712161779404,\n",
       " -0.0790102556347847,\n",
       " 0.02487826719880104,\n",
       " -0.001512036076746881,\n",
       " -0.02010742388665676,\n",
       " 0.054473649710416794,\n",
       " -0.005764565896242857,\n",
       " 0.04093749448657036,\n",
       " -0.005150589160621166,\n",
       " 0.0044936202466487885,\n",
       " 0.019288020208477974,\n",
       " 0.04135412350296974,\n",
       " 0.029294518753886223,\n",
       " -0.022062314674258232,\n",
       " 0.00970622431486845,\n",
       " -0.026097683236002922,\n",
       " -0.005173648241907358,\n",
       " -0.002604210516437888,\n",
       " 0.013182866387069225,\n",
       " 8.49884600029327e-05,\n",
       " -0.03047812357544899,\n",
       " 0.031248630955815315,\n",
       " -0.03264347463846207,\n",
       " 0.005431067664176226,\n",
       " 0.053447142243385315,\n",
       " -0.05365414917469025,\n",
       " -0.03445212543010712,\n",
       " -0.06693451851606369,\n",
       " -0.05212554335594177,\n",
       " 0.07124653458595276,\n",
       " -0.09106588363647461,\n",
       " -0.01892912946641445,\n",
       " -0.014470458962023258,\n",
       " -0.06756459176540375,\n",
       " -0.04722407087683678,\n",
       " -0.0019393846159800887,\n",
       " -0.009250757284462452,\n",
       " 0.08667430281639099,\n",
       " -0.013727221637964249,\n",
       " 0.008448897860944271,\n",
       " 0.0513760931789875,\n",
       " 0.0540078841149807,\n",
       " 0.0808861181139946,\n",
       " -0.033056654036045074,\n",
       " 0.006191754247993231,\n",
       " -0.02282113954424858,\n",
       " -0.03163127973675728,\n",
       " 0.033009931445121765,\n",
       " -0.04795541614294052,\n",
       " -0.035444777458906174,\n",
       " 0.020683491602540016,\n",
       " 0.03369414433836937,\n",
       " 0.020871983841061592,\n",
       " -0.015512264333665371,\n",
       " 0.0205660592764616,\n",
       " 0.01546971034258604,\n",
       " -0.008315050974488258,\n",
       " -0.03989618271589279,\n",
       " -0.025677800178527832,\n",
       " -0.010023977607488632,\n",
       " -0.03546314314007759,\n",
       " -0.005870438180863857,\n",
       " -0.014340675435960293,\n",
       " -0.047033630311489105,\n",
       " 0.016618113964796066,\n",
       " -0.006268823053687811,\n",
       " -0.016645677387714386,\n",
       " -0.05667388439178467,\n",
       " 0.005169864743947983,\n",
       " -0.016172677278518677,\n",
       " 0.012453719973564148,\n",
       " 0.0439143180847168,\n",
       " 0.03571823239326477,\n",
       " 0.00024523676256649196,\n",
       " 0.025223657488822937,\n",
       " -0.04705800116062164,\n",
       " -0.11172893643379211,\n",
       " -0.020000973716378212,\n",
       " -0.06319617480039597,\n",
       " -0.039846841245889664,\n",
       " 0.008003313094377518,\n",
       " -0.0165419839322567,\n",
       " -0.011555804871022701,\n",
       " 0.014844311401247978,\n",
       " -0.028484169393777847,\n",
       " -0.049511637538671494,\n",
       " 0.06956865638494492,\n",
       " -0.008363480679690838,\n",
       " 0.023039503023028374,\n",
       " 0.07295224070549011,\n",
       " 0.03266299515962601,\n",
       " 0.027346959337592125,\n",
       " 0.07403817772865295,\n",
       " 0.0024363554548472166,\n",
       " 0.01788541115820408,\n",
       " 0.02648869715631008,\n",
       " -0.023846857249736786,\n",
       " -0.010027782991528511,\n",
       " -0.018099842593073845,\n",
       " 0.01105005107820034,\n",
       " 0.0011447317665442824,\n",
       " 0.021414225921034813,\n",
       " 0.0027971635572612286,\n",
       " 0.007326896768063307,\n",
       " 0.056879978626966476,\n",
       " 0.005070442799478769,\n",
       " 0.036714378744363785,\n",
       " -0.002974518807604909,\n",
       " 0.012972112745046616,\n",
       " 0.019054612144827843,\n",
       " -0.02738974802196026,\n",
       " -0.010320553556084633,\n",
       " 0.009099741466343403,\n",
       " -0.044568583369255066,\n",
       " -0.005781715735793114,\n",
       " -0.012499301694333553,\n",
       " 0.005865262821316719,\n",
       " 0.007234492339193821,\n",
       " -0.036139871925115585,\n",
       " 0.009955926798284054,\n",
       " -0.019761614501476288,\n",
       " 0.008874834515154362,\n",
       " -0.03639823570847511,\n",
       " 0.027301369234919548,\n",
       " -0.02098703384399414,\n",
       " 0.05065907537937164,\n",
       " -0.0694950595498085,\n",
       " 0.0074490574188530445,\n",
       " -0.014693130739033222,\n",
       " 0.08581961691379547,\n",
       " 0.015528777614235878,\n",
       " 0.007518581580370665,\n",
       " -0.016756780445575714,\n",
       " 0.05055637285113335,\n",
       " -0.0013553291792050004,\n",
       " 0.02129335328936577,\n",
       " -0.02118879370391369,\n",
       " -0.029931139200925827,\n",
       " 0.029775643721222878,\n",
       " 0.026963423937559128,\n",
       " -0.010037087835371494,\n",
       " 0.0065217409282922745,\n",
       " -0.015530368313193321,\n",
       " 0.031093737110495567,\n",
       " 0.04080439731478691,\n",
       " 0.01692032814025879,\n",
       " -0.004793269094079733,\n",
       " 0.0020386455580592155,\n",
       " 0.02055130898952484,\n",
       " 0.036700572818517685,\n",
       " -0.029149502515792847,\n",
       " 0.03355950862169266,\n",
       " -0.034865088760852814,\n",
       " 0.014983432367444038,\n",
       " -0.00011229699157411233,\n",
       " 0.07140093296766281,\n",
       " -0.039652254432439804,\n",
       " -0.0039028695318847895,\n",
       " -0.05487412214279175,\n",
       " 0.04981799051165581,\n",
       " -0.060359857976436615,\n",
       " 0.0019611669704318047,\n",
       " -0.0009742174297571182,\n",
       " 0.05748317018151283,\n",
       " 0.01951882429420948,\n",
       " -0.09376657754182816,\n",
       " 0.00988245289772749,\n",
       " 0.0602513924241066,\n",
       " 0.02854837104678154,\n",
       " 0.004573551006615162,\n",
       " 0.005548406392335892,\n",
       " 0.05022944509983063,\n",
       " 0.053680699318647385,\n",
       " 0.03366543725132942,\n",
       " 0.007517368532717228,\n",
       " -0.09137167036533356,\n",
       " -0.011968282982707024,\n",
       " 0.022349845618009567,\n",
       " -0.04462205991148949,\n",
       " -0.017980024218559265,\n",
       " 0.018356049433350563,\n",
       " -0.029772041365504265,\n",
       " -0.005410672165453434,\n",
       " 0.05834098160266876,\n",
       " -0.03237442672252655,\n",
       " -0.05145927518606186,\n",
       " -0.017639197409152985,\n",
       " -0.020312877371907234,\n",
       " 0.04763583466410637,\n",
       " -0.023168541491031647,\n",
       " -0.014489050954580307,\n",
       " 0.034540098160505295,\n",
       " -0.011693413369357586,\n",
       " -0.012257084250450134,\n",
       " -0.013092889450490475,\n",
       " 0.03988990560173988,\n",
       " -0.011543730273842812,\n",
       " -0.004507414996623993,\n",
       " 0.018128037452697754,\n",
       " -0.023655416443943977,\n",
       " 0.10076904296875,\n",
       " 0.018949858844280243,\n",
       " 0.01988692581653595,\n",
       " -0.04450381547212601,\n",
       " 0.0380777008831501,\n",
       " 0.0358317568898201,\n",
       " 0.08056305348873138,\n",
       " -0.0024206240195780993,\n",
       " 0.027304038405418396,\n",
       " 0.010057066567242146,\n",
       " 0.003539787605404854,\n",
       " 0.0403340645134449,\n",
       " -0.03774939104914665,\n",
       " -0.01322179939597845,\n",
       " 0.040107328444719315,\n",
       " -0.014651025645434856,\n",
       " 0.013778949156403542,\n",
       " 0.0474485345184803,\n",
       " -0.0831407755613327,\n",
       " 0.050623197108507156,\n",
       " -0.006335006561130285,\n",
       " -0.04024173319339752,\n",
       " 0.006337156984955072,\n",
       " -0.0699867531657219,\n",
       " -0.03176092356443405,\n",
       " -0.011728832498192787,\n",
       " -0.003678716951981187,\n",
       " -0.010191643610596657,\n",
       " -0.0013385004131123424,\n",
       " 0.0013578629586845636,\n",
       " -0.08302342891693115,\n",
       " -0.004432769026607275,\n",
       " -0.06428951770067215,\n",
       " -0.06617061793804169,\n",
       " 0.059547074139118195,\n",
       " -0.13089913129806519,\n",
       " 0.016334548592567444,\n",
       " -0.034254785627126694,\n",
       " -0.013843527995049953,\n",
       " 0.005066845566034317,\n",
       " 0.030346717685461044,\n",
       " 0.05403947830200195,\n",
       " -0.03472653031349182,\n",
       " 0.015075015835464,\n",
       " 0.025109419599175453,\n",
       " 0.010496893897652626,\n",
       " 0.02938145212829113,\n",
       " -0.010855869390070438,\n",
       " 0.01650620996952057,\n",
       " -0.057090405374765396,\n",
       " -0.04047326743602753,\n",
       " -0.016709886491298676,\n",
       " 0.046530138701200485,\n",
       " -0.01935432106256485,\n",
       " -0.11051944643259048,\n",
       " -0.034102004021406174,\n",
       " -0.01246758084744215,\n",
       " -0.0451778881251812,\n",
       " -0.00965022761374712,\n",
       " 0.03560716286301613,\n",
       " 0.003633261425420642,\n",
       " 0.011959617957472801,\n",
       " -0.026025919243693352,\n",
       " 0.03781742975115776,\n",
       " -0.0441780760884285,\n",
       " 0.021979017183184624,\n",
       " -0.008472614921629429,\n",
       " -0.03342082351446152,\n",
       " -0.03071499802172184,\n",
       " 0.01928836666047573,\n",
       " 0.04272354394197464,\n",
       " -0.03832630813121796,\n",
       " -0.01001809537410736,\n",
       " 0.037155717611312866,\n",
       " -0.004576415754854679,\n",
       " 0.027899907901883125,\n",
       " -0.005688618402928114,\n",
       " 0.005147087853401899,\n",
       " -0.020475801080465317,\n",
       " -0.06567972898483276,\n",
       " 0.025975462049245834,\n",
       " -0.015874791890382767,\n",
       " 0.020583780482411385,\n",
       " -0.05959829315543175,\n",
       " -0.015107606537640095,\n",
       " -0.03387046232819557,\n",
       " -0.004453479312360287,\n",
       " -0.039385512471199036,\n",
       " -0.06115008890628815,\n",
       " 0.017826257273554802,\n",
       " -0.04591519758105278,\n",
       " 0.01519257202744484,\n",
       " 0.02230014279484749,\n",
       " -0.024206064641475677,\n",
       " 0.03690449893474579,\n",
       " 0.02982044219970703,\n",
       " -5.858371072936529e-33,\n",
       " 0.013544597662985325,\n",
       " 0.013491827063262463,\n",
       " -0.029629038646817207,\n",
       " -0.055568210780620575,\n",
       " -0.06325864791870117,\n",
       " -0.00103700440376997,\n",
       " -0.023748306557536125,\n",
       " 0.019085220992565155,\n",
       " -0.06735694408416748,\n",
       " -0.017868703231215477,\n",
       " 0.007263286504894495,\n",
       " -0.032581280916929245,\n",
       " -0.006104454398155212,\n",
       " -0.019251395016908646,\n",
       " -0.009248350746929646,\n",
       " 0.010353443212807178,\n",
       " -0.00967637449502945,\n",
       " 0.05115228891372681,\n",
       " 0.03732972964644432,\n",
       " -0.029729826375842094,\n",
       " 0.051718298345804214,\n",
       " 0.06701695173978806,\n",
       " 0.013040761463344097,\n",
       " -0.05271070823073387,\n",
       " 0.031306978315114975,\n",
       " 0.06868816912174225,\n",
       " 0.054182976484298706,\n",
       " 0.014848761260509491,\n",
       " 0.04827498644590378,\n",
       " 0.00658072205260396,\n",
       " 0.008823986165225506,\n",
       " 0.000649120775051415,\n",
       " 0.018369700759649277,\n",
       " 0.03986426815390587,\n",
       " 0.027925550937652588,\n",
       " 0.00870306883007288,\n",
       " 0.0038237685803323984,\n",
       " 0.02823529578745365,\n",
       " -0.007530803792178631,\n",
       " -0.05618422478437424,\n",
       " -0.03864208981394768,\n",
       " -0.05914546176791191,\n",
       " 0.028416821733117104,\n",
       " -0.01673736423254013,\n",
       " -0.031599875539541245,\n",
       " 0.022696102038025856,\n",
       " -0.011481435969471931,\n",
       " -0.014221534132957458,\n",
       " -0.0002765881654340774,\n",
       " 0.03718797490000725,\n",
       " -0.0009737631771713495,\n",
       " 0.008458920754492283,\n",
       " -0.008863194845616817,\n",
       " -0.04236994683742523,\n",
       " -0.15438275039196014,\n",
       " 0.06179630756378174,\n",
       " -0.005974192172288895,\n",
       " 0.04987533763051033,\n",
       " -0.02131747081875801,\n",
       " -0.016414254903793335,\n",
       " -0.052752066403627396,\n",
       " -0.01398169994354248,\n",
       " 0.06793858855962753,\n",
       " 0.0017999819247052073,\n",
       " 0.050173964351415634,\n",
       " -0.03235278278589249,\n",
       " -0.07654838263988495,\n",
       " 0.032475363463163376,\n",
       " -0.0342801995575428,\n",
       " 0.026127666234970093,\n",
       " -0.0017649008659645915,\n",
       " -0.053093355149030685,\n",
       " -0.04887229949235916,\n",
       " -0.018706200644373894,\n",
       " -0.00907472800463438,\n",
       " -0.01218758337199688,\n",
       " -0.026475487276911736,\n",
       " 0.019494755193591118,\n",
       " -0.042494215071201324,\n",
       " 0.09822876006364822,\n",
       " -0.04396998882293701,\n",
       " 0.0461774580180645,\n",
       " -0.0007643912686035037,\n",
       " 0.015102976001799107,\n",
       " 0.012151665054261684,\n",
       " -0.040912989526987076,\n",
       " -0.004575529135763645,\n",
       " -0.011621779762208462,\n",
       " -0.020847730338573456,\n",
       " 0.005206739529967308,\n",
       " -0.04169616475701332,\n",
       " -0.010173901915550232,\n",
       " 0.007680906448513269,\n",
       " 0.02124483324587345,\n",
       " -0.002127357991412282,\n",
       " -0.1000123992562294,\n",
       " 0.006261180154979229,\n",
       " 0.002809037221595645,\n",
       " 0.008873208425939083,\n",
       " 0.008330839686095715,\n",
       " -0.0019983816891908646,\n",
       " 0.048962775617837906,\n",
       " -0.039603229612112045,\n",
       " -0.007250825874507427,\n",
       " 0.008062832057476044,\n",
       " 0.022764243185520172,\n",
       " 0.0361136719584465,\n",
       " 9.901211160467938e-05,\n",
       " 0.027409784495830536,\n",
       " 0.003916288260370493,\n",
       " 0.015392964705824852,\n",
       " 0.004966794513165951,\n",
       " 0.009547311812639236,\n",
       " 0.053732626140117645,\n",
       " -0.0292525552213192,\n",
       " -0.005219151731580496,\n",
       " 0.028181152418255806,\n",
       " 0.0016541226068511605,\n",
       " 0.07559293508529663,\n",
       " 0.02787039242684841,\n",
       " 0.016411680728197098,\n",
       " -0.0434734970331192,\n",
       " 0.025142233818769455,\n",
       " -0.0065579949878156185,\n",
       " -0.0058356402441859245,\n",
       " 0.005845213308930397,\n",
       " -0.01232861541211605,\n",
       " 0.0720592811703682,\n",
       " 0.01351555809378624,\n",
       " -0.05117495357990265,\n",
       " 0.046507399529218674,\n",
       " -0.010812393389642239,\n",
       " 2.6927716589852935e-07,\n",
       " 0.0035654085222631693,\n",
       " -0.00460733100771904,\n",
       " -0.009873437695205212,\n",
       " 0.008267564699053764,\n",
       " -0.044857628643512726,\n",
       " 0.008747419342398643,\n",
       " 0.013443231582641602,\n",
       " 0.07748235762119293,\n",
       " -0.08290114998817444,\n",
       " -0.04325574263930321,\n",
       " 0.026676038280129433,\n",
       " -0.005866225343197584,\n",
       " -0.013912354595959187,\n",
       " 0.006559798028320074,\n",
       " 0.028102293610572815,\n",
       " -0.006942069157958031,\n",
       " -0.0048421151004731655,\n",
       " 0.014531612396240234,\n",
       " -0.053557731211185455,\n",
       " -0.00020796465105377138,\n",
       " 0.005269614979624748,\n",
       " 0.028788698837161064,\n",
       " -0.027872759848833084,\n",
       " 0.023970765992999077,\n",
       " 0.04495154693722725,\n",
       " 0.01952269859611988,\n",
       " -0.012163006700575352,\n",
       " 0.008351092226803303,\n",
       " -0.011408495716750622,\n",
       " 0.013989069499075413,\n",
       " -0.00456711370497942,\n",
       " -0.018914857879281044,\n",
       " -0.03433069959282875,\n",
       " -0.061948489397764206,\n",
       " 0.01845681294798851,\n",
       " 0.0009844728047028184,\n",
       " -0.08121185004711151,\n",
       " 0.06168624758720398,\n",
       " -0.02745535410940647,\n",
       " -0.024391289800405502,\n",
       " 0.005859223660081625,\n",
       " -0.06185251474380493,\n",
       " 0.05751851573586464,\n",
       " -0.04321388900279999,\n",
       " 0.010733060538768768,\n",
       " 0.012132538482546806,\n",
       " 0.02206256240606308,\n",
       " 0.02082546427845955,\n",
       " -0.0033450471237301826,\n",
       " 0.009329712018370628,\n",
       " 0.06976252794265747,\n",
       " -0.049180056899785995,\n",
       " -0.10871367901563644,\n",
       " 0.003661922411993146,\n",
       " 0.012205899693071842,\n",
       " 0.042326394468545914,\n",
       " 0.041339289397001266,\n",
       " -0.015999210998415947,\n",
       " -0.004355417564511299,\n",
       " -0.012203220278024673,\n",
       " -0.05101102963089943,\n",
       " -0.04754029959440231,\n",
       " -0.00863417237997055,\n",
       " 0.010995406657457352,\n",
       " -0.037704698741436005,\n",
       " -0.04773630574345589,\n",
       " -0.0025629380252212286,\n",
       " 2.1891617004447324e-34,\n",
       " -0.03089447319507599,\n",
       " 0.041092656552791595,\n",
       " -0.005788855254650116,\n",
       " -0.0110078239813447,\n",
       " 0.03454134985804558,\n",
       " -0.0011980868875980377,\n",
       " -0.013515186496078968,\n",
       " 0.046475835144519806,\n",
       " 0.04615090414881706,\n",
       " -0.06090735271573067,\n",
       " -0.022374531254172325]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['Vector'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Store sentences & embeddings on disc\n",
    "def save_embeddings(filepath, embeddings):\n",
    "    with open(filepath, \"wb\") as fOut:\n",
    "        pickle.dump({ 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    return stored_embeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(model, name):\n",
    "    with open(f\"{MODEL_PATH}/{name}\",'wb') as f:\n",
    "        pickle.dump( model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "def custom_cv(model, df, n_folds=5,sent =False):\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    user_label_df =df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    \n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "    #print(labels)\n",
    "    #print(users.shape,labels.shape)\n",
    "    \n",
    "    f1_scores = []\n",
    "    for train_index, test_index in skf.split(users, labels):\n",
    "        train_users = [users[f] for f in train_index]\n",
    "        test_users = [users[f] for f in test_index]\n",
    "\n",
    "        train_folds = df[df['User'].isin(train_users)].copy()\n",
    "        test_folds = df[df['User'].isin(test_users)].copy()\n",
    "\n",
    "        X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "        #X_train = train_folds['Vector']\n",
    "        y_train = train_folds['Label']\n",
    "        X_test = pd.DataFrame(test_folds['Vector'].values.tolist(), index = test_folds.index)\n",
    "        #X_test = test_folds['Vector']\n",
    "        \n",
    "        y_test = test_folds['Label']\n",
    "        if sent:\n",
    "            X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "            X_test = np.c_[X_test,test_folds['polarity'],test_folds['subjectivity'],test_folds['negativity'],test_folds['positivity'],test_folds['neutrality'], test_folds['compound']] \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        f1_scores.append(f1_score(y_test,model.predict(X_test)))\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "models = {\n",
    "        'sgdLR':SGDClassifier(random_state=seed,loss='log'),\n",
    "        #'NB':MultinomialNB(),\\\n",
    "        'sgdlSVM':SGDClassifier(random_state=seed,loss='hinge'),\n",
    "        'ExtraTrees':ExtraTreesClassifier(random_state=seed,n_jobs=-1),\\\n",
    "        'Perceptron':Perceptron(random_state=seed)}\n",
    "if BASELINE_COMP:\n",
    "        report=\"\"\n",
    "        best_model_name = \"\"\n",
    "        best_model=None\n",
    "        best_f1=0\n",
    "        for model_name, model in models.items():\n",
    "                res = custom_cv(model,full_df)\n",
    "                if np.mean(res) > best_f1:\n",
    "                        best_f1=np.mean(res)\n",
    "                        best_model_name=model_name\n",
    "                        best_model=model\n",
    "                #train_df[train_df['User'].isin(flds[0][0])].describe()\n",
    "                report+=f\"{model_name} f1: {round(np.mean(res),3)}\\n\"\n",
    "                print(f\"{model_name} f1: {round(np.mean(res),3)}\")\n",
    "        with open(f\"{MODEL_PATH}/baseline_report_{LM_MODEL}.txt\",'w') as f:\n",
    "                f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BASELINE_COMP:\n",
    "        res = custom_cv(best_model,full_df, sent=True)\n",
    "        print(f\"{best_model_name} f1: {round(np.mean(res),3)}\")\n",
    "\n",
    "        with open(f\"{MODEL_PATH}/baseline_report_{LM_MODEL}.txt\",'a') as f:\n",
    "                f.write(f\"\\nBest model with sent:\\n{best_model_name} f1: {round(np.mean(res),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best performing model was sgd Logistic Regression for window size 10, using MPNet without SA features, having achieved a 0.823 F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "def train_eval_tuning(trial,params, df, sent=False):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    user_label_df =df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    \n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "\n",
    "    \n",
    "    f1_scores = []\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "        train_users = [users[f] for f in train_index]\n",
    "        test_users = [users[f] for f in test_index]\n",
    "\n",
    "        train_folds = df[df['User'].isin(train_users)]\n",
    "        X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "\n",
    "        test_folds = df[df['User'].isin(test_users)]\n",
    "        X_test = pd.DataFrame(test_folds['Vector'].values.tolist(), index = test_folds.index)\n",
    "\n",
    "        model = SGDClassifier(**params)\n",
    "        if sent:\n",
    "            #scaler = MinMaxScaler()\n",
    "            #train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.fit_transform(train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            #test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.transform(test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "            X_test = np.c_[X_test,test_folds['polarity'],test_folds['subjectivity'],test_folds['negativity'],test_folds['positivity'],test_folds['neutrality'], test_folds['compound']] \n",
    "            \n",
    "\n",
    "        model.fit(X_train, train_folds['Label'])\n",
    "        f1_scores.append(f1_score(test_folds['Label'],model.predict(X_test)))\n",
    "    \n",
    "        trial.report(np.mean(f1_scores), fold)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_objective(trial):\n",
    "    parameters = {\n",
    "        'max_iter':trial.suggest_int('max_iter',1000,2500,step=500),\n",
    "        'loss':trial.suggest_categorical('loss',['log']),\n",
    "        'penalty':trial.suggest_categorical('penalty',['l2','l1','elasticnet']),\n",
    "        'alpha': trial.suggest_float('alpha',0.00001,0.1,log=True),\n",
    "        'random_state':trial.suggest_int('random_state',seed,seed)\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    \n",
    "    avg_f1 = train_eval_tuning(trial,parameters,full_df, sent=False)\n",
    "    return np.mean(avg_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-15 20:01:29,761]\u001b[0m A new study created in memory with name: t1_tuning_10\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:03:46,844]\u001b[0m Trial 0 finished with value: 0.813897147575471 and parameters: {'max_iter': 2500, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 1.8629722287576724e-05, 'random_state': 23}. Best is trial 0 with value: 0.813897147575471.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:05:55,720]\u001b[0m Trial 1 finished with value: 0.8091352749827484 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.0050733694052943114, 'random_state': 23}. Best is trial 0 with value: 0.813897147575471.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:08:17,855]\u001b[0m Trial 2 finished with value: 0.790641379369261 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l1', 'alpha': 0.004308190784212204, 'random_state': 23}. Best is trial 0 with value: 0.813897147575471.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:10:30,752]\u001b[0m Trial 3 finished with value: 0.8189108063290986 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'l1', 'alpha': 7.96322053605296e-05, 'random_state': 23}. Best is trial 3 with value: 0.8189108063290986.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:12:46,900]\u001b[0m Trial 4 finished with value: 0.7184226697364533 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.019497458313271987, 'random_state': 23}. Best is trial 3 with value: 0.8189108063290986.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:15:09,101]\u001b[0m Trial 5 finished with value: 0.8186174454283263 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l1', 'alpha': 0.000434645549936947, 'random_state': 23}. Best is trial 3 with value: 0.8189108063290986.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:17:24,214]\u001b[0m Trial 6 finished with value: 0.8174398054130887 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l1', 'alpha': 6.660866354481666e-05, 'random_state': 23}. Best is trial 3 with value: 0.8189108063290986.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:17:47,495]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:20:02,674]\u001b[0m Trial 8 finished with value: 0.8175558969501253 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'l1', 'alpha': 6.743639422847423e-05, 'random_state': 23}. Best is trial 3 with value: 0.8189108063290986.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:20:27,581]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:22:24,896]\u001b[0m Trial 10 finished with value: 0.8228939063439183 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 9.145036396278381e-05, 'random_state': 23}. Best is trial 10 with value: 0.8228939063439183.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:24:16,092]\u001b[0m Trial 11 finished with value: 0.8241783880410181 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.00014881748206483316, 'random_state': 23}. Best is trial 11 with value: 0.8241783880410181.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:24:38,674]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:26:32,106]\u001b[0m Trial 13 finished with value: 0.8249661653900212 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.000228691565972448, 'random_state': 23}. Best is trial 13 with value: 0.8249661653900212.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:28:27,516]\u001b[0m Trial 14 finished with value: 0.8251670411909877 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.00027033012321142993, 'random_state': 23}. Best is trial 14 with value: 0.8251670411909877.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:30:17,228]\u001b[0m Trial 15 finished with value: 0.8252024570343913 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0011297872167846722, 'random_state': 23}. Best is trial 15 with value: 0.8252024570343913.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:30:38,360]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:30:59,184]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:31:20,093]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:31:40,976]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:33:35,288]\u001b[0m Trial 20 finished with value: 0.8256673804597012 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0003372843595707076, 'random_state': 23}. Best is trial 20 with value: 0.8256673804597012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:35:26,026]\u001b[0m Trial 21 finished with value: 0.8250387744692669 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0003111585425683685, 'random_state': 23}. Best is trial 20 with value: 0.8256673804597012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:37:17,893]\u001b[0m Trial 22 finished with value: 0.8255324360181863 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0009627086091407413, 'random_state': 23}. Best is trial 20 with value: 0.8256673804597012.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:39:08,819]\u001b[0m Trial 23 finished with value: 0.8261536279517447 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0008993784001889559, 'random_state': 23}. Best is trial 23 with value: 0.8261536279517447.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:39:29,968]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:41:21,274]\u001b[0m Trial 25 finished with value: 0.8254666873486771 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0004828255762920802, 'random_state': 23}. Best is trial 23 with value: 0.8261536279517447.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:43:15,797]\u001b[0m Trial 26 finished with value: 0.8260284755338724 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0007889628497005299, 'random_state': 23}. Best is trial 23 with value: 0.8261536279517447.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:43:36,869]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:45:28,891]\u001b[0m Trial 28 finished with value: 0.8259986508230555 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0007533945638354969, 'random_state': 23}. Best is trial 23 with value: 0.8261536279517447.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:45:51,994]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:46:13,264]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:48:05,853]\u001b[0m Trial 31 finished with value: 0.8262780689985009 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0006803245759335398, 'random_state': 23}. Best is trial 31 with value: 0.8262780689985009.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:48:27,071]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:48:49,667]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:49:10,789]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:51:01,137]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:51:24,751]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:51:45,552]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:52:10,559]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:52:31,722]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:52:54,587]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:54:47,890]\u001b[0m Trial 41 finished with value: 0.825714030312867 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0003645563410844826, 'random_state': 23}. Best is trial 31 with value: 0.8262780689985009.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:56:42,822]\u001b[0m Trial 42 finished with value: 0.8261084097275365 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0006188979065175834, 'random_state': 23}. Best is trial 31 with value: 0.8262780689985009.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:57:08,170]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:58:36,062]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 20:58:57,945]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 21:00:50,461]\u001b[0m Trial 46 finished with value: 0.8261709536024864 and parameters: {'max_iter': 2500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0006478131447679978, 'random_state': 23}. Best is trial 31 with value: 0.8262780689985009.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 21:01:15,188]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 21:03:11,732]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 21:05:03,012]\u001b[0m Trial 49 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "        study_name=f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}\",\n",
    "        direction='maximize')\n",
    "study.optimize(tuning_objective, n_trials=50, timeout=(60*60*3))\n",
    "joblib.dump(study,f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "    #study = optuna.create_study(study_name=f\"tfidfvectorizer_cv_{ROLLING_WINDOW_SIZE}\",direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.NopPruner())\n",
    "study = joblib.load(f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_iter': 1000,\n",
       "  'loss': 'log',\n",
       "  'penalty': 'l2',\n",
       "  'alpha': 0.0006803245759335398,\n",
       "  'random_state': 23},\n",
       " 0.8262780689985009)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{MODEL_PATH}/baseline_report_{LM_MODEL}.txt\",'a') as f:\n",
    "                f.write(f\"\\nOptimized model f1: {round(study.best_value,3)}\\nparams: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdLR_params = study.best_trial.params\n",
    "\n",
    "final_model = SGDClassifier(**sgdLR_params)\n",
    "full_train = pd.DataFrame(full_df['Vector'].values.tolist(), index = full_df.index)\n",
    "\n",
    "final_model.fit(full_train, full_df['Label'])\n",
    "save_model(final_model, \"optimized_sgdLR.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
