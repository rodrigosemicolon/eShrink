{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = \"train_df.csv\"\n",
    "\n",
    "TEST_DF = \"test_df.csv\"\n",
    "BOW_MODELS = \"Models/BoW/\" \n",
    "WE_MODELS = \"Models/WE/\" \n",
    "WORD_EMBEDDINGS = \"../word_embeddings/\"\n",
    "PRE_METHOD='Raw' #'Lemmatized' for bow and we, 'Raw' for lm\n",
    "#WE_MODEL=\"GLOVE_CC\"\n",
    "LM_MODEL='all-mpnet-base-v2'\n",
    "FEATURE_TYPE=\"NN\" #\"BoW\" #\"LM\" #\"WE\"\n",
    "ROLLING_WINDOW_SIZE=10\n",
    "SENT=False\n",
    "ROUND_MAX=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nunused_df.to_csv(\"unused_df.csv\",sep=\"\\t\")\\nlm=SentenceTransformer(LM_MODEL)\\nlm.max_seq_length=512 \\nunused_embeddings = lm.encode(unused_df[\\'Text\\'],show_progress_bar=True,            output_value=\\'sentence_embedding\\', batch_size=64,convert_to_numpy=True)\\nsave_embeddings(\"unused_embeddings.pkl\", unused_embeddings)\\n\\nif SENT:\\n    full_df = sa_features(full_df)\\n    unused_df = sa_features(unused_df)\\nfull_df.to_pickle(\"full_df_sent.pkl\")\\n\\nfull_df = pd.read_pickle(\"full_df_sent.pkl\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utilities import rolling_window, sa_features, save_embeddings\n",
    "import optuna\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import joblib\n",
    "seed=23\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DF, sep='\\t')\n",
    "test_df = pd.read_csv(TEST_DF, sep='\\t')\n",
    "#train_df = rolling_window(train_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "#test_df = rolling_window(test_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "full_df = pd.concat([train_df,test_df])\n",
    "####\n",
    "full_df = rolling_window(full_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "####\n",
    "\"\"\"\n",
    "tokens_df = pd.read_csv(\"./tokens.csv\", sep='\\t')\n",
    "total_df = tokens_df.merge(full_df.drop_duplicates(), on=['User','Post_Nr','Raw','Stemmed','Lemmatized','Label' ], \n",
    "                   how='left', indicator=True)\n",
    "unused_df = total_df[total_df['_merge'] == 'left_only'].copy()\n",
    "unused_df.drop(['_merge'], inplace=True, axis=1)\n",
    "unused_df = rolling_window(unused_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "unused_df = unused_df.query(\"Window_id < 90\")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "unused_df.to_csv(\"unused_df.csv\",sep=\"\\t\")\n",
    "lm=SentenceTransformer(LM_MODEL)\n",
    "lm.max_seq_length=512 \n",
    "unused_embeddings = lm.encode(unused_df['Text'],show_progress_bar=True,\\\n",
    "            output_value='sentence_embedding', batch_size=64,convert_to_numpy=True)\n",
    "save_embeddings(\"unused_embeddings.pkl\", unused_embeddings)\n",
    "\n",
    "if SENT:\n",
    "    full_df = sa_features(full_df)\n",
    "    unused_df = sa_features(unused_df)\n",
    "full_df.to_pickle(\"full_df_sent.pkl\")\n",
    "\n",
    "full_df = pd.read_pickle(\"full_df_sent.pkl\")\n",
    "\"\"\"\n",
    "\n",
    "#full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### BoW FEATURES\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom nltk.corpus import stopwords\\nnltk_stop_words = set(stopwords.words(\"english\"))\\nimport optuna\\nimport joblib\\nTFIDF_STUDY_PATH=\"./BagOfWords/t1_tfidfvec_10\"\\nMODEL_STUDY_PATH=\"./BagOfWords/t1_tuning_10.pkl\"\\ntfidf_study = joblib.load(TFIDF_STUDY_PATH)\\nmodel_study = joblib.load(MODEL_STUDY_PATH)\\ntfidf_params = tfidf_study.best_trial.params\\ntfidf_params[\\'stop_words\\'] = nltk_stop_words\\ntfidf_params[\\'ngram_range\\'] = (1,tfidf_params[\\'ngram_range\\'])\\nvectorizer = TfidfVectorizer(**tfidf_params)\\nmodel = SGDClassifier(**model_study.best_params)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "### BoW FEATURES\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "import optuna\n",
    "import joblib\n",
    "TFIDF_STUDY_PATH=\"./BagOfWords/t1_tfidfvec_10\"\n",
    "MODEL_STUDY_PATH=\"./BagOfWords/t1_tuning_10.pkl\"\n",
    "tfidf_study = joblib.load(TFIDF_STUDY_PATH)\n",
    "model_study = joblib.load(MODEL_STUDY_PATH)\n",
    "tfidf_params = tfidf_study.best_trial.params\n",
    "tfidf_params['stop_words'] = nltk_stop_words\n",
    "tfidf_params['ngram_range'] = (1,tfidf_params['ngram_range'])\n",
    "vectorizer = TfidfVectorizer(**tfidf_params)\n",
    "model = SGDClassifier(**model_study.best_params)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### WE FEATURES\\nfrom utilities import WordEmbeddingVectorizer\\nfrom sklearn.linear_model import SGDClassifier\\nimport optuna\\nimport joblib\\nSTUDY_PATH=\"./Embeddings/t1_tuning_10.pkl\"\\nstudy = joblib.load(STUDY_PATH)\\nmodel = SGDClassifier(**study.best_params)\\nwe_vectorizer = WordEmbeddingVectorizer(WE_MODEL, WORD_EMBEDDINGS)\\n#full_df = full_df.sample(frac=1, random_state=seed).reset_index(drop=True) \\nfull_df[\\'Vector\\'] = full_df[\\'Text\\'].apply(lambda text: we_vectorizer.vectorize(text))\\nunused_df[\\'Vector\\'] = unused_df[\\'Text\\'].apply(lambda text: we_vectorizer.vectorize(text))\\n\\nfull_df\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "### WE FEATURES\n",
    "from utilities import WordEmbeddingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import optuna\n",
    "import joblib\n",
    "STUDY_PATH=\"./Embeddings/t1_tuning_10.pkl\"\n",
    "study = joblib.load(STUDY_PATH)\n",
    "model = SGDClassifier(**study.best_params)\n",
    "we_vectorizer = WordEmbeddingVectorizer(WE_MODEL, WORD_EMBEDDINGS)\n",
    "#full_df = full_df.sample(frac=1, random_state=seed).reset_index(drop=True) \n",
    "full_df['Vector'] = full_df['Text'].apply(lambda text: we_vectorizer.vectorize(text))\n",
    "unused_df['Vector'] = unused_df['Text'].apply(lambda text: we_vectorizer.vectorize(text))\n",
    "\n",
    "full_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### LM FEATURES\\nfrom sklearn.linear_model import SGDClassifier\\nimport optuna\\nimport joblib\\nfrom utilities import load_embeddings\\nLM_MODEL=\\'all-mpnet-base-v2\\'\\nSTUDY_PATH = \"./LanguageModels/t1_tuning_10.pkl\"\\nstudy = joblib.load(STUDY_PATH)\\nstudy.best_params\\nmodel = SGDClassifier(**study.best_params)\\n#train_sentence_embeddings = load_embeddings(f\"./LanguageModels/train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\\n#val_sentence_embeddings = load_embeddings(f\"./LanguageModels/val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\\n#unused_embeddings = load_embeddings(f\"./unused_embeddings.pkl\")\\n\\n#full_sentence_embeddings = np.concatenate((train_sentence_embeddings,val_sentence_embeddings), axis=0)\\n#full_df[\\'Vector\\'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\\nfull_df = pd.read_pickle(\"full_df_pkl.pkl\")\\n\\nunused_df = pd.read_pickle(\"final_unused_lm.pkl\")\\n#unused_df[\\'Vector\\'] = pd.DataFrame(data=unused_embeddings).values.tolist()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "### LM FEATURES\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import optuna\n",
    "import joblib\n",
    "from utilities import load_embeddings\n",
    "LM_MODEL='all-mpnet-base-v2'\n",
    "STUDY_PATH = \"./LanguageModels/t1_tuning_10.pkl\"\n",
    "study = joblib.load(STUDY_PATH)\n",
    "study.best_params\n",
    "model = SGDClassifier(**study.best_params)\n",
    "#train_sentence_embeddings = load_embeddings(f\"./LanguageModels/train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "#val_sentence_embeddings = load_embeddings(f\"./LanguageModels/val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "#unused_embeddings = load_embeddings(f\"./unused_embeddings.pkl\")\n",
    "\n",
    "#full_sentence_embeddings = np.concatenate((train_sentence_embeddings,val_sentence_embeddings), axis=0)\n",
    "#full_df['Vector'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\n",
    "full_df = pd.read_pickle(\"full_df_pkl.pkl\")\n",
    "\n",
    "unused_df = pd.read_pickle(\"final_unused_lm.pkl\")\n",
    "#unused_df['Vector'] = pd.DataFrame(data=unused_embeddings).values.tolist()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "### LM FEATURES FOR NN\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torch\n",
    "import optuna\n",
    "import joblib\n",
    "from utilities import load_embeddings, LmNeuralNetwork, WritingWindowDataset\n",
    "STUDY_PATH = \"./LanguageModels/NN_hyperparameterstudy10.pkl\"\n",
    "study = joblib.load(STUDY_PATH)\n",
    "study.best_params\n",
    "params = optuna.trial.FixedTrial(study.best_trial.params)\n",
    "train_sentence_embeddings = load_embeddings(f\"./LanguageModels/train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "val_sentence_embeddings = load_embeddings(f\"./LanguageModels/val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "\n",
    "full_sentence_embeddings = np.concatenate((train_sentence_embeddings,val_sentence_embeddings), axis=0)#full_df['Vector'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\n",
    "full_df['Vector'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\n",
    "unused_df = pd.read_pickle(\"final_unused_lm.pkl\")\n",
    "\n",
    "train_ds = WritingWindowDataset(full_sentence_embeddings, full_df['Label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "#from torchmetrics.functional import f1_score\n",
    "BATCH_SIZE = 32\n",
    "epochs=10\n",
    "\n",
    "def train_nn(trainloader, study):\n",
    "    params = optuna.trial.FixedTrial(study.best_trial.params)\n",
    "    model= LmNeuralNetwork(params)\n",
    "\n",
    "    model = model.cuda()\n",
    "    optimizer = AdamW(model.parameters(), lr=study.best_trial.params['learning_rate'])#, lr= learning_rate)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "            \n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        \n",
    "        total_loss_train = 0\n",
    "        train_preds = np.array([])\n",
    "        train_targets = np.array([])\n",
    "        model.train()\n",
    "        for train_input, train_label in tqdm(trainloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            train_label = train_label.float()\n",
    "            train_label = train_label.unsqueeze(1)\n",
    "            features = train_input.to(device)\n",
    "            \n",
    "\n",
    "            output = model(features)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += float(batch_loss.item())\n",
    "            \n",
    "            train_preds = np.concatenate((train_preds,torch.round(output).detach().cpu().numpy().flatten()),axis=0)\n",
    "            train_targets = np.concatenate((train_targets,train_label.detach().cpu().numpy().flatten()),axis=0)\n",
    "\n",
    "            \n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss=total_loss_train/len(train_preds)\n",
    "        train_f1 = f1_score(train_targets,train_preds)\n",
    "            \n",
    "        print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .4f} \\\n",
    "                | Train F1: {train_f1: .4f}')\n",
    "\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20ab2d7d410>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def nn_cv_classification(full_df, study,filename):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    user_label_df =full_df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "            train_users = [users[f] for f in train_index]\n",
    "            test_users = [users[f] for f in test_index]\n",
    "            train_ids = train_df.index[train_df['User'].isin(train_users)].to_list()\n",
    "            train_folds = full_df[full_df['User'].isin(train_users)].copy()\n",
    "            train_folds = train_folds.sample(frac=1, random_state=seed).reset_index(drop=True) \n",
    "\n",
    "            test_folds = full_df[full_df['User'].isin(test_users)].copy()\n",
    "            test_folds = pd.concat([test_folds,unused_df])\n",
    "            train_data = torch.utils.data.Subset(train_ds, train_ids)\n",
    "            \n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                            train_data, \n",
    "                            batch_size=32, \n",
    "                            shuffle=True)\n",
    "            model = train_nn(trainloader, study)\n",
    "            model.eval()\n",
    "            print(f\"model for fold {fold} trained\")\n",
    "            true_label=test_folds[['User','Label']].groupby('User').max().to_dict()['Label']\n",
    "            true_labels = {subject:true_label[subject] for subject in true_label}\n",
    "            with open(f\"{filename}_{fold}.txt\", \"w\") as f:\n",
    "                for user in true_labels:\n",
    "                    f.write(f\"({user},{true_labels[user]})\")\n",
    "                    for window_id in range(ROUND_MAX-ROLLING_WINDOW_SIZE+1):\n",
    "                        test_f = test_folds.query(\"User==@user and Window_id==@window_id\")\n",
    "                        if len(test_f)>0:\n",
    "                            f.write(\" \")\n",
    "                            \n",
    "                            X_test = test_f['Vector'].values.tolist()[0]\n",
    "                            with torch.no_grad():\n",
    "                                X_test = torch.tensor([X_test]).cuda() #added\n",
    "\n",
    "                                \n",
    "                                pred = model(X_test)[0]\n",
    "                                f.write(f\"({round(pred.item())},{pred.item()})\")\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:26<00:00, 86.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0220                 | Train F1:  0.6431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 309.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0210                 | Train F1:  0.6431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 315.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0194                 | Train F1:  0.6431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 315.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0182                 | Train F1:  0.6473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 312.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0173                 | Train F1:  0.6892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 315.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0165                 | Train F1:  0.7299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 314.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0158                 | Train F1:  0.7414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 316.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0152                 | Train F1:  0.7420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 312.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0147                 | Train F1:  0.7435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2272/2272 [00:07<00:00, 317.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0142                 | Train F1:  0.7402\n",
      "model for fold 0 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:23<00:00, 102.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0211                 | Train F1:  0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:09<00:00, 260.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0199                 | Train F1:  0.4103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 309.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0181                 | Train F1:  0.6594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 311.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0162                 | Train F1:  0.7454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 314.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0146                 | Train F1:  0.7769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 314.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0136                 | Train F1:  0.7909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 314.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0129                 | Train F1:  0.8006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 312.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0124                 | Train F1:  0.8044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 313.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0120                 | Train F1:  0.8114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [00:07<00:00, 316.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0118                 | Train F1:  0.8152\n",
      "model for fold 1 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:10<00:00, 228.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0214                 | Train F1:  0.6317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 314.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0202                 | Train F1:  0.7627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 313.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0184                 | Train F1:  0.7998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 312.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0164                 | Train F1:  0.8105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 313.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0146                 | Train F1:  0.8193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 313.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0133                 | Train F1:  0.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 312.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0125                 | Train F1:  0.8314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 314.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0118                 | Train F1:  0.8374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 314.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0114                 | Train F1:  0.8429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [00:07<00:00, 313.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0111                 | Train F1:  0.8465\n",
      "model for fold 2 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:10<00:00, 203.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0213                 | Train F1:  0.5784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 313.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0201                 | Train F1:  0.6235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 316.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0184                 | Train F1:  0.6610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 316.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0168                 | Train F1:  0.7233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 317.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0154                 | Train F1:  0.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 314.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0141                 | Train F1:  0.8038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 316.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0132                 | Train F1:  0.8256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 317.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0123                 | Train F1:  0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 317.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0117                 | Train F1:  0.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2155/2155 [00:06<00:00, 315.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0111                 | Train F1:  0.8457\n",
      "model for fold 3 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:15<00:00, 195.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0213                 | Train F1:  0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 312.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0197                 | Train F1:  0.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 311.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0171                 | Train F1:  0.7471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 313.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0147                 | Train F1:  0.7933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 311.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0131                 | Train F1:  0.8097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 310.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0122                 | Train F1:  0.8187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 310.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0117                 | Train F1:  0.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 310.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0114                 | Train F1:  0.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 311.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0111                 | Train F1:  0.8330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3059/3059 [00:09<00:00, 310.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0109                 | Train F1:  0.8372\n",
      "model for fold 4 trained\n"
     ]
    }
   ],
   "source": [
    "nn_cv_classification(full_df,study, \"NN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cv_classification(full_df, model,filename):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    user_label_df =full_df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "            if fold!=0:\n",
    "                break\n",
    "            train_users = [users[f] for f in train_index]\n",
    "            test_users = [users[f] for f in test_index]\n",
    "\n",
    "            train_folds = full_df[full_df['User'].isin(train_users)].copy()\n",
    "            train_folds = train_folds.sample(frac=1, random_state=seed).reset_index(drop=True) \n",
    "\n",
    "            test_folds = full_df[full_df['User'].isin(test_users)].copy()\n",
    "            test_folds = pd.concat([test_folds,unused_df])\n",
    "\n",
    "            if FEATURE_TYPE==\"BoW\":\n",
    "                X_train = vectorizer.fit_transform(train_folds['Text'])\n",
    "            else:\n",
    "                X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "            y_train = train_folds['Label']\n",
    "            if SENT:\n",
    "                X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            print(f\"model for fold {fold} trained\")\n",
    "            true_label=test_folds[['User','Label']].groupby('User').max().to_dict()['Label']\n",
    "            true_labels = {subject:true_label[subject] for subject in true_label}\n",
    "            with open(f\"{filename}_{fold}.txt\", \"w\") as f:\n",
    "                for user in true_labels:\n",
    "                    f.write(f\"({user},{true_labels[user]})\")\n",
    "                    for window_id in range(ROUND_MAX-ROLLING_WINDOW_SIZE+1):\n",
    "                        test_f = test_folds.query(\"User==@user and Window_id==@window_id\")\n",
    "                        if len(test_f)>0:\n",
    "                            f.write(\" \")\n",
    "                            if FEATURE_TYPE==\"BoW\":\n",
    "                                X_test = vectorizer.transform(test_f['Text'])\n",
    "                                if SENT:\n",
    "                                    X_test = np.r_[X_test,test_f['polarity'],test_f['subjectivity'],test_f['negativity'],test_f['positivity'],test_f['neutrality'], test_f['compound']] \n",
    "                                X_test.reshape(1,-1)\n",
    "                            else:\n",
    "                                X_test = test_f['Vector'].values.tolist()[0]\n",
    "                                if SENT:\n",
    "                                    X_test = np.r_[X_test,test_f['polarity'],test_f['subjectivity'],test_f['negativity'],test_f['positivity'],test_f['neutrality'], test_f['compound']] \n",
    "                                X_test = [X_test] #added\n",
    "\n",
    "                            \n",
    "                            pred = model.predict(X_test)[0]\n",
    "                            prob = model.predict_proba(X_test)[0].tolist()[1]\n",
    "                            f.write(f\"({pred},{prob})\")\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for fold 0 trained\n"
     ]
    }
   ],
   "source": [
    "#cv_classification(full_df, model, FEATURE_TYPE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
