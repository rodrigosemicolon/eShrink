{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (Detecting depressed subjects)\n",
    "### Using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_FILE = \"../posts.csv\"\n",
    "TRAIN_TOKEN=\"../train_df.csv\"\n",
    "TEST_TOKEN=\"../test_df.csv\"\n",
    "GENERAL_MODELS=\"../Models\"\n",
    "ROLLING_WINDOW_SIZE=10\n",
    "BASELINE_COMP=False\n",
    "TASK=2\n",
    "METHOD='mean'\n",
    "EMBEDDINGS = 'GLOVE_CC'\n",
    "WORD_EMBEDDINGS = \"../../word_embeddings/\"\n",
    "MODEL_PATH =f\"{GENERAL_MODELS}/WE/{EMBEDDINGS}_{ROLLING_WINDOW_SIZE}\" \n",
    "seed=23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(MODEL_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import fasttext\n",
    "we = {\"FASTTEXT_CC\":\"crawl-300d-2M-subword.bin\",\"GLOVE_TT\":\"glove-twitter-200.bin\", \"GLOVE_CC\":\"glove_cc_300d.bin\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDINGS=='GLOVE_TT':\n",
    "    embeddings = gensim.models.KeyedVectors.load_word2vec_format(WORD_EMBEDDINGS + \"glove-twitter-200.bin\", binary=True)\n",
    "    #embeddings.fill_norms()\n",
    "    #glove_weights = torch.FloatTensor(glove_embeddings.vectors)\n",
    "    #glove_embeddings.most_similar(\"night\")\n",
    "elif EMBEDDINGS=='GLOVE_CC':\n",
    "    embeddings = gensim.models.KeyedVectors.load_word2vec_format(WORD_EMBEDDINGS + \"glove_cc_300d.bin\", binary=True)\n",
    "    #embeddings.fill_norms()\n",
    "\n",
    "elif EMBEDDINGS=='FASTTEXT_CC':\n",
    "    embeddings = fasttext.load_model(WORD_EMBEDDINGS + \"crawl-300d-2M-subword.bin\")\n",
    "    #fasttext_embeddings.get_nearest_neighbors(\"night\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opening resulting dataset with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Post_Nr</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Lemmatized</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>0</td>\n",
       "      <td>so many unwanted smith fadeaways.</td>\n",
       "      <td>so mani unwant smith fadeaway .</td>\n",
       "      <td>so many unwanted smith fadeaways .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>1</td>\n",
       "      <td>mid range jumpers hey guys, celtics fan here p...</td>\n",
       "      <td>mid rang jumper hey guy , celtic fan here pull...</td>\n",
       "      <td>mid range jumpers hey guys , celtics fan here ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>2</td>\n",
       "      <td>well he got number tonight so maybe he will b...</td>\n",
       "      <td>well he got number tonight so mayb he will be ...</td>\n",
       "      <td>well he got number tonight so maybe he will ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>3</td>\n",
       "      <td>i mean he will get pinch hits and an occasion...</td>\n",
       "      <td>i mean he will get pinch hit and an occasion d...</td>\n",
       "      <td>i mean he will get pinch hits and an occasio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>4</td>\n",
       "      <td>yeah you are probably right. oh well.</td>\n",
       "      <td>yeah you are probabl right . oh well .</td>\n",
       "      <td>yeah you are probably right . oh well .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174168</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>627</td>\n",
       "      <td>nothing like that clean house feeling</td>\n",
       "      <td>noth like that clean hous feel</td>\n",
       "      <td>nothing like that clean house feeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174169</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>628</td>\n",
       "      <td>there is always that one coworker...</td>\n",
       "      <td>there is alway that one cowork ...</td>\n",
       "      <td>there is always that one coworker ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174170</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>629</td>\n",
       "      <td>there is always that one coworker you just can...</td>\n",
       "      <td>there is alway that one cowork you just can no...</td>\n",
       "      <td>there is always that one coworker you just can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174171</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>630</td>\n",
       "      <td>that moment when you realize you need a new job</td>\n",
       "      <td>that moment when you realiz you need a new job</td>\n",
       "      <td>that moment when you realize you need a new job</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174172</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>631</td>\n",
       "      <td>as an artist, this speaks to me on so many lev...</td>\n",
       "      <td>as an artist , this speak to me on so mani lev...</td>\n",
       "      <td>as an artist , this speaks to me on so many le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174173 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    User  Post_Nr  \\\n",
       "0       test_subject1345        0   \n",
       "1       test_subject1345        1   \n",
       "2       test_subject1345        2   \n",
       "3       test_subject1345        3   \n",
       "4       test_subject1345        4   \n",
       "...                  ...      ...   \n",
       "174168       subject9959      627   \n",
       "174169       subject9959      628   \n",
       "174170       subject9959      629   \n",
       "174171       subject9959      630   \n",
       "174172       subject9959      631   \n",
       "\n",
       "                                                      Raw  \\\n",
       "0                       so many unwanted smith fadeaways.   \n",
       "1       mid range jumpers hey guys, celtics fan here p...   \n",
       "2        well he got number tonight so maybe he will b...   \n",
       "3        i mean he will get pinch hits and an occasion...   \n",
       "4                   yeah you are probably right. oh well.   \n",
       "...                                                   ...   \n",
       "174168             nothing like that clean house feeling    \n",
       "174169              there is always that one coworker...    \n",
       "174170  there is always that one coworker you just can...   \n",
       "174171   that moment when you realize you need a new job    \n",
       "174172  as an artist, this speaks to me on so many lev...   \n",
       "\n",
       "                                                  Stemmed  \\\n",
       "0                         so mani unwant smith fadeaway .   \n",
       "1       mid rang jumper hey guy , celtic fan here pull...   \n",
       "2       well he got number tonight so mayb he will be ...   \n",
       "3       i mean he will get pinch hit and an occasion d...   \n",
       "4                  yeah you are probabl right . oh well .   \n",
       "...                                                   ...   \n",
       "174168                     noth like that clean hous feel   \n",
       "174169                 there is alway that one cowork ...   \n",
       "174170  there is alway that one cowork you just can no...   \n",
       "174171     that moment when you realiz you need a new job   \n",
       "174172  as an artist , this speak to me on so mani lev...   \n",
       "\n",
       "                                               Lemmatized  Label  \n",
       "0                      so many unwanted smith fadeaways .      1  \n",
       "1       mid range jumpers hey guys , celtics fan here ...      1  \n",
       "2         well he got number tonight so maybe he will ...      1  \n",
       "3         i mean he will get pinch hits and an occasio...      1  \n",
       "4                 yeah you are probably right . oh well .      1  \n",
       "...                                                   ...    ...  \n",
       "174168              nothing like that clean house feeling      0  \n",
       "174169              there is always that one coworker ...      0  \n",
       "174170  there is always that one coworker you just can...      0  \n",
       "174171    that moment when you realize you need a new job      0  \n",
       "174172  as an artist , this speaks to me on so many le...      0  \n",
       "\n",
       "[174173 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_TOKEN, sep='\\t')\n",
    "test_df = pd.read_csv(TEST_TOKEN, sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(df, window_size,stride, field):\n",
    "    res_map={}\n",
    "    for user in df['User'].unique():\n",
    "        user_df = df[df['User']==user]\n",
    "        res_map[user]=(user_df['Label'].values[0],{})\n",
    "        posts = user_df[field].values\n",
    "        iteration=0\n",
    "        for i in range(0,len(posts),stride):\n",
    "            res_map[user][1][iteration]=' '.join((posts[i:i+window_size]))\n",
    "            iteration+=1\n",
    "    result_df = pd.DataFrame([(k,k1,v1,v[0]) for k,v in res_map.items() for k1,v1 in v[1].items()], columns = ['User','Window_id','Text','Label'])\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Window_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>0</td>\n",
       "      <td>so many unwanted smith fadeaways . mid range...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>1</td>\n",
       "      <td>mid range jumpers hey guys , celtics fan here ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>2</td>\n",
       "      <td>well he got number tonight so maybe he will ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>3</td>\n",
       "      <td>i mean he will get pinch hits and an occasio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>4</td>\n",
       "      <td>yeah you are probably right . oh well .   i ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174168</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>627</td>\n",
       "      <td>nothing like that clean house feeling there is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174169</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>628</td>\n",
       "      <td>there is always that one coworker ... there is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174170</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>629</td>\n",
       "      <td>there is always that one coworker you just can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174171</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>630</td>\n",
       "      <td>that moment when you realize you need a new jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174172</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>631</td>\n",
       "      <td>as an artist , this speaks to me on so many le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174173 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    User  Window_id  \\\n",
       "0       test_subject1345          0   \n",
       "1       test_subject1345          1   \n",
       "2       test_subject1345          2   \n",
       "3       test_subject1345          3   \n",
       "4       test_subject1345          4   \n",
       "...                  ...        ...   \n",
       "174168       subject9959        627   \n",
       "174169       subject9959        628   \n",
       "174170       subject9959        629   \n",
       "174171       subject9959        630   \n",
       "174172       subject9959        631   \n",
       "\n",
       "                                                     Text  Label  \n",
       "0         so many unwanted smith fadeaways . mid range...      1  \n",
       "1       mid range jumpers hey guys , celtics fan here ...      1  \n",
       "2         well he got number tonight so maybe he will ...      1  \n",
       "3         i mean he will get pinch hits and an occasio...      1  \n",
       "4         yeah you are probably right . oh well .   i ...      1  \n",
       "...                                                   ...    ...  \n",
       "174168  nothing like that clean house feeling there is...      0  \n",
       "174169  there is always that one coworker ... there is...      0  \n",
       "174170  there is always that one coworker you just can...      0  \n",
       "174171  that moment when you realize you need a new jo...      0  \n",
       "174172  as an artist , this speaks to me on so many le...      0  \n",
       "\n",
       "[174173 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = rolling_window(train_df,ROLLING_WINDOW_SIZE,1,'Lemmatized')\n",
    "test_df = rolling_window(test_df,ROLLING_WINDOW_SIZE,1,'Lemmatized')\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df,test_df])\n",
    "train_df = train_df.sample(frac=1, random_state=seed).reset_index(drop=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(model, name):\n",
    "    with open(f\"{MODEL_PATH}/{name}\",'wb') as f:\n",
    "        pickle.dump( model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import operator\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "        for word in re.findall(r\"\\w+|[^\\w\\s]\", sentence, re.UNICODE):            \n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    vocab = dict(sorted(vocab.items(), key=operator.itemgetter(1))[::-1])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210927/210927 [00:34<00:00, 6031.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 4516700,\n",
       " 'the': 2716145,\n",
       " ',': 2668221,\n",
       " 'i': 2416414,\n",
       " 'to': 1963788,\n",
       " 'a': 1738277,\n",
       " 'and': 1637573,\n",
       " 'is': 1447022,\n",
       " 'number': 1251359,\n",
       " 'of': 1245039,\n",
       " 'it': 1212082,\n",
       " 'not': 1112011,\n",
       " 'you': 1035802,\n",
       " 'that': 1025429,\n",
       " 'in': 942184,\n",
       " 'for': 717307,\n",
       " 'have': 655485,\n",
       " 'my': 592988,\n",
       " '?': 578793,\n",
       " 'are': 549075,\n",
       " 'on': 537574,\n",
       " 'but': 523223,\n",
       " 'was': 509307,\n",
       " 'this': 508326,\n",
       " 'do': 496124,\n",
       " 'with': 485730,\n",
       " 'be': 437934,\n",
       " '!': 387048,\n",
       " 'am': 386628,\n",
       " 'they': 373301,\n",
       " 'as': 364101,\n",
       " 'so': 351858,\n",
       " 'if': 349533,\n",
       " 'would': 341663,\n",
       " 'just': 335168,\n",
       " 'me': 334830,\n",
       " 'can': 333336,\n",
       " 'like': 332804,\n",
       " 'he': 323522,\n",
       " 'or': 312890,\n",
       " 'at': 303988,\n",
       " 'what': 300534,\n",
       " 'will': 280931,\n",
       " 'your': 260970,\n",
       " 'about': 259490,\n",
       " 'out': 248961,\n",
       " 'all': 248371,\n",
       " 'from': 247054,\n",
       " 'we': 228046,\n",
       " 'one': 226518,\n",
       " 'there': 225753,\n",
       " 's': 219821,\n",
       " 'up': 215318,\n",
       " 'get': 211755,\n",
       " 'an': 209485,\n",
       " 'people': 203490,\n",
       " 'when': 199618,\n",
       " 'more': 188703,\n",
       " 'how': 186413,\n",
       " 'no': 173545,\n",
       " 'because': 171486,\n",
       " 'did': 168562,\n",
       " 'url': 165262,\n",
       " 'had': 164870,\n",
       " 'know': 164517,\n",
       " 'think': 164260,\n",
       " 'by': 164254,\n",
       " 'some': 163516,\n",
       " 'has': 163013,\n",
       " 'them': 162524,\n",
       " 'time': 161507,\n",
       " 'she': 157518,\n",
       " 'who': 152649,\n",
       " 'really': 152578,\n",
       " 'his': 144819,\n",
       " 'been': 134613,\n",
       " 'good': 133225,\n",
       " 'their': 132290,\n",
       " 'could': 130261,\n",
       " 'her': 129936,\n",
       " 'does': 128443,\n",
       " 'now': 123959,\n",
       " 'then': 122107,\n",
       " 'also': 121639,\n",
       " 'only': 120765,\n",
       " 'want': 118739,\n",
       " 'other': 117521,\n",
       " 'were': 117499,\n",
       " 'much': 115538,\n",
       " 'him': 114754,\n",
       " 'even': 113440,\n",
       " 'any': 111920,\n",
       " 'going': 111501,\n",
       " 'than': 111490,\n",
       " 'here': 108094,\n",
       " 'go': 106946,\n",
       " 'see': 106847,\n",
       " 'make': 106003,\n",
       " 'way': 101701,\n",
       " 'well': 100093,\n",
       " 'too': 99715,\n",
       " 'should': 98138,\n",
       " 'into': 97822,\n",
       " 'being': 94494,\n",
       " 'why': 93549,\n",
       " 'still': 92100,\n",
       " 'something': 92097,\n",
       " 'got': 91708,\n",
       " 'which': 90740,\n",
       " 'money': 89599,\n",
       " 'first': 89012,\n",
       " 'over': 88475,\n",
       " 'back': 87926,\n",
       " 'after': 87553,\n",
       " 'right': 86908,\n",
       " 'very': 86659,\n",
       " 'new': 83613,\n",
       " 'us': 82125,\n",
       " 'never': 81686,\n",
       " 'feel': 80878,\n",
       " 'most': 80722,\n",
       " 'work': 80587,\n",
       " 'things': 79613,\n",
       " 'need': 78250,\n",
       " 'say': 78241,\n",
       " 'off': 77837,\n",
       " 'where': 75116,\n",
       " 'day': 74418,\n",
       " 'someone': 71681,\n",
       " 'same': 71233,\n",
       " 'thing': 70937,\n",
       " 'before': 70424,\n",
       " 'years': 70412,\n",
       " 'lot': 69811,\n",
       " 'said': 68657,\n",
       " 'take': 67025,\n",
       " 'emoji': 66650,\n",
       " 'these': 65852,\n",
       " 'those': 65667,\n",
       " 'better': 65665,\n",
       " 'life': 65160,\n",
       " 'love': 64893,\n",
       " 'sure': 64741,\n",
       " 'down': 63948,\n",
       " 'its': 63598,\n",
       " 'game': 62995,\n",
       " 'two': 62824,\n",
       " 'though': 62726,\n",
       " 'our': 62327,\n",
       " 'find': 61773,\n",
       " 'actually': 61352,\n",
       " 'pretty': 61024,\n",
       " 'use': 60934,\n",
       " 'year': 59510,\n",
       " 'while': 59137,\n",
       " 'help': 58581,\n",
       " 'great': 58518,\n",
       " 'around': 58173,\n",
       " 'made': 57801,\n",
       " 'many': 57308,\n",
       " 'look': 56502,\n",
       " 'few': 55962,\n",
       " 'thanks': 55735,\n",
       " 'every': 55601,\n",
       " 'anything': 55397,\n",
       " 'best': 54954,\n",
       " 'always': 54641,\n",
       " 'through': 54296,\n",
       " 'last': 54283,\n",
       " 'little': 53738,\n",
       " 'long': 51042,\n",
       " 'since': 50474,\n",
       " 'used': 49918,\n",
       " 'probably': 49905,\n",
       " 'bad': 49850,\n",
       " 'maybe': 49834,\n",
       " 'smiling': 49481,\n",
       " 'getting': 48423,\n",
       " 'ever': 48076,\n",
       " 'try': 47803,\n",
       " 'let': 47627,\n",
       " 'man': 47491,\n",
       " 'doing': 47450,\n",
       " 'again': 46937,\n",
       " 'point': 46745,\n",
       " 'thought': 46488,\n",
       " 'anyone': 46020,\n",
       " 'might': 45192,\n",
       " 'post': 44616,\n",
       " 'kind': 44315,\n",
       " 'may': 44189,\n",
       " 'world': 43726,\n",
       " 'person': 43114,\n",
       " 'give': 42838,\n",
       " 'subreddit': 42337,\n",
       " 'own': 41928,\n",
       " 'yeah': 41830,\n",
       " 'having': 41551,\n",
       " 'trying': 41173,\n",
       " 'another': 41134,\n",
       " 'put': 40894,\n",
       " 'old': 40761,\n",
       " 'part': 40675,\n",
       " 'come': 40309,\n",
       " 'myself': 40143,\n",
       " 'both': 39705,\n",
       " 'keep': 39549,\n",
       " 'looking': 39471,\n",
       " 'read': 39169,\n",
       " 'enough': 39140,\n",
       " 'different': 38558,\n",
       " 'play': 38503,\n",
       " 'everyone': 38062,\n",
       " 'shit': 37844,\n",
       " 'mean': 37674,\n",
       " 'bit': 37522,\n",
       " 'thank': 37501,\n",
       " 'without': 37468,\n",
       " 'makes': 37441,\n",
       " 'big': 37405,\n",
       " 'yes': 37257,\n",
       " 'tell': 37219,\n",
       " 'hard': 36986,\n",
       " 'school': 36475,\n",
       " 'guy': 36376,\n",
       " 'away': 36235,\n",
       " 'else': 36067,\n",
       " 'next': 35970,\n",
       " 'end': 35933,\n",
       " 'nothing': 35695,\n",
       " 'oh': 34814,\n",
       " 'went': 34748,\n",
       " 'fuck': 34700,\n",
       " 'least': 34265,\n",
       " 'everything': 34255,\n",
       " 'friends': 34238,\n",
       " 'found': 34236,\n",
       " 'high': 34056,\n",
       " 'done': 34056,\n",
       " 'trump': 33933,\n",
       " 'until': 33815,\n",
       " 'such': 32984,\n",
       " 'once': 32826,\n",
       " 'times': 32774,\n",
       " 'real': 32145,\n",
       " 'live': 32074,\n",
       " 'job': 32030,\n",
       " 'days': 31978,\n",
       " 'wrong': 31927,\n",
       " 'home': 31873,\n",
       " 'start': 31732,\n",
       " 'idea': 31723,\n",
       " 'already': 31576,\n",
       " 'able': 31475,\n",
       " 'etc': 31427,\n",
       " 'far': 31374,\n",
       " 'saying': 31337,\n",
       " 'week': 31271,\n",
       " 'seems': 31001,\n",
       " 'free': 31000,\n",
       " 'believe': 30926,\n",
       " 'looks': 30867,\n",
       " 'show': 30864,\n",
       " 'problem': 30796,\n",
       " 'place': 30726,\n",
       " 'guys': 30703,\n",
       " 'family': 30661,\n",
       " 'stuff': 30131,\n",
       " 'started': 30056,\n",
       " 'told': 29886,\n",
       " 'using': 29147,\n",
       " 'less': 29028,\n",
       " 'laughing': 28946,\n",
       " 'reason': 28907,\n",
       " 'friend': 28881,\n",
       " 'either': 28811,\n",
       " 'hope': 28701,\n",
       " 'each': 28672,\n",
       " 'ago': 28526,\n",
       " 'house': 28516,\n",
       " 'nice': 28441,\n",
       " 'making': 28340,\n",
       " 'seen': 28196,\n",
       " 'story': 28109,\n",
       " 'understand': 27911,\n",
       " 'talk': 27832,\n",
       " 'games': 27679,\n",
       " 'loud': 27337,\n",
       " 'says': 27248,\n",
       " 'sorry': 27244,\n",
       " 'change': 27143,\n",
       " 'today': 27072,\n",
       " 'night': 26941,\n",
       " 'god': 26886,\n",
       " 'almost': 26855,\n",
       " 'ask': 26820,\n",
       " 'between': 26651,\n",
       " 'wanted': 26615,\n",
       " 'd': 26470,\n",
       " 'against': 26446,\n",
       " 'months': 26432,\n",
       " 'care': 26396,\n",
       " 'women': 26335,\n",
       " 'whole': 26266,\n",
       " 'edit': 26126,\n",
       " 'definitely': 25941,\n",
       " 'top': 25662,\n",
       " 'fact': 25532,\n",
       " 'left': 25447,\n",
       " 'reddit': 25338,\n",
       " 'guess': 25323,\n",
       " 'video': 25273,\n",
       " 'stop': 25195,\n",
       " 'question': 25082,\n",
       " 'remember': 25047,\n",
       " 'case': 24827,\n",
       " 'fucking': 24724,\n",
       " 'car': 24698,\n",
       " 'yet': 24696,\n",
       " 'however': 24687,\n",
       " 'small': 24682,\n",
       " 'mind': 24599,\n",
       " 'tried': 24410,\n",
       " 'x': 24397,\n",
       " 'call': 24239,\n",
       " 'pay': 24211,\n",
       " 'came': 23747,\n",
       " 'please': 23650,\n",
       " 'working': 23625,\n",
       " 'name': 23470,\n",
       " 'talking': 23316,\n",
       " 'took': 23261,\n",
       " 'fun': 23248,\n",
       " 'team': 23209,\n",
       " 'during': 23167,\n",
       " 'kids': 22997,\n",
       " 'full': 22917,\n",
       " 'state': 22871,\n",
       " 'sometimes': 22866,\n",
       " 'happy': 22711,\n",
       " 'black': 22609,\n",
       " 'run': 22580,\n",
       " 'buy': 22442,\n",
       " 'true': 22309,\n",
       " 'side': 22273,\n",
       " 'second': 22226,\n",
       " 'gets': 22120,\n",
       " 'white': 22089,\n",
       " 'movie': 22032,\n",
       " 'parents': 22009,\n",
       " 'c': 21965,\n",
       " 'usually': 21859,\n",
       " 'face': 21695,\n",
       " 'saw': 21687,\n",
       " 'food': 21637,\n",
       " 'under': 21610,\n",
       " 'hours': 21533,\n",
       " 'called': 21504,\n",
       " 'b': 21497,\n",
       " 'thinking': 21341,\n",
       " 'agree': 21291,\n",
       " 'instead': 21136,\n",
       " 'watch': 20833,\n",
       " 'e': 20808,\n",
       " 'social': 20794,\n",
       " 'possible': 20738,\n",
       " 'girl': 20723,\n",
       " 'happened': 20653,\n",
       " 'quite': 20592,\n",
       " 'playing': 20538,\n",
       " 'yourself': 20488,\n",
       " 'three': 20457,\n",
       " 'sex': 20406,\n",
       " 'sounds': 20319,\n",
       " 'head': 20313,\n",
       " 'heard': 20266,\n",
       " 'hate': 20240,\n",
       " 'experience': 20215,\n",
       " 'others': 20141,\n",
       " 'past': 19843,\n",
       " 'set': 19810,\n",
       " 'awesome': 19777,\n",
       " 'deal': 19699,\n",
       " 'must': 19672,\n",
       " 'open': 19612,\n",
       " 'half': 19591,\n",
       " 'taking': 19581,\n",
       " 'couple': 19542,\n",
       " 'book': 19435,\n",
       " 'means': 19297,\n",
       " 'check': 19229,\n",
       " 'together': 19164,\n",
       " 'seem': 19147,\n",
       " 'phone': 19145,\n",
       " 'feeling': 19114,\n",
       " 'negative': 19093,\n",
       " 'comes': 19059,\n",
       " 'cool': 19024,\n",
       " 'water': 19009,\n",
       " 'body': 19007,\n",
       " 'completely': 18977,\n",
       " 'depression': 18955,\n",
       " 'exactly': 18777,\n",
       " 'men': 18646,\n",
       " 'later': 18519,\n",
       " 'mom': 18518,\n",
       " 'opinion': 18509,\n",
       " 'power': 18456,\n",
       " 'support': 18455,\n",
       " 'goes': 18409,\n",
       " 'rather': 18369,\n",
       " 'month': 18283,\n",
       " 'needs': 18266,\n",
       " 'close': 18245,\n",
       " 'whatever': 18239,\n",
       " 'sense': 18190,\n",
       " 'felt': 18092,\n",
       " 'often': 18062,\n",
       " 'hear': 18037,\n",
       " 'especially': 17945,\n",
       " 'weeks': 17821,\n",
       " 'fine': 17799,\n",
       " 'wait': 17739,\n",
       " 'link': 17508,\n",
       " 'hand': 17476,\n",
       " 'course': 17445,\n",
       " 'issues': 17439,\n",
       " 'level': 17360,\n",
       " 'coming': 17335,\n",
       " 'matter': 17312,\n",
       " 'system': 17307,\n",
       " 'k': 17291,\n",
       " 'happen': 17261,\n",
       " 'worth': 17203,\n",
       " 'basically': 17187,\n",
       " 'woman': 17071,\n",
       " 'comment': 17070,\n",
       " 'move': 17041,\n",
       " 'wish': 17025,\n",
       " 'lost': 16962,\n",
       " 'super': 16959,\n",
       " 'issue': 16938,\n",
       " 'country': 16932,\n",
       " 'based': 16917,\n",
       " 'self': 16892,\n",
       " 'room': 16835,\n",
       " 'hell': 16747,\n",
       " 'children': 16734,\n",
       " 'interesting': 16641,\n",
       " 'likely': 16617,\n",
       " 'okay': 16592,\n",
       " 'american': 16566,\n",
       " 'dad': 16538,\n",
       " 'government': 16525,\n",
       " 'add': 16477,\n",
       " 'news': 16443,\n",
       " 'ones': 16378,\n",
       " 'amazing': 16334,\n",
       " 'college': 16279,\n",
       " 'f': 16268,\n",
       " 'minutes': 16268,\n",
       " 'soon': 16251,\n",
       " 'eat': 16250,\n",
       " 'leave': 16216,\n",
       " 'hit': 16208,\n",
       " 'media': 16203,\n",
       " 'war': 16155,\n",
       " 'health': 16119,\n",
       " 'works': 15960,\n",
       " 'single': 15951,\n",
       " 'huge': 15950,\n",
       " 'asked': 15798,\n",
       " 'rest': 15798,\n",
       " 'similar': 15775,\n",
       " 'line': 15711,\n",
       " 'example': 15653,\n",
       " 'group': 15637,\n",
       " 'reading': 15577,\n",
       " 'hey': 15544,\n",
       " 'honestly': 15541,\n",
       " 'become': 15523,\n",
       " 'sort': 15452,\n",
       " 'human': 15422,\n",
       " 'type': 15384,\n",
       " 'ok': 15373,\n",
       " 'wants': 15317,\n",
       " 'class': 15285,\n",
       " 'low': 15232,\n",
       " 'public': 15119,\n",
       " 'kid': 15113,\n",
       " 'internet': 15093,\n",
       " 'turn': 15089,\n",
       " 'article': 15030,\n",
       " 'th': 14992,\n",
       " 'pain': 14962,\n",
       " 'city': 14937,\n",
       " 'relationship': 14919,\n",
       " 'music': 14885,\n",
       " 'easy': 14880,\n",
       " 'knew': 14862,\n",
       " 'user': 14855,\n",
       " 'given': 14848,\n",
       " 'death': 14832,\n",
       " 'advice': 14757,\n",
       " 'm': 14653,\n",
       " 'law': 14632,\n",
       " 'played': 14601,\n",
       " 'control': 14591,\n",
       " 'age': 14590,\n",
       " 'anyway': 14582,\n",
       " 'stay': 14521,\n",
       " 'police': 14494,\n",
       " 'source': 14480,\n",
       " 'kill': 14355,\n",
       " 'google': 14250,\n",
       " 'sound': 14234,\n",
       " 'cut': 14226,\n",
       " 'situation': 14145,\n",
       " 'op': 14138,\n",
       " 'living': 14113,\n",
       " 'child': 14079,\n",
       " 'mine': 14068,\n",
       " 'win': 14056,\n",
       " 'word': 13984,\n",
       " 'weird': 13935,\n",
       " 'party': 13904,\n",
       " 'absolutely': 13901,\n",
       " 'favorite': 13900,\n",
       " 'short': 13899,\n",
       " 'normal': 13886,\n",
       " 'looked': 13877,\n",
       " 'future': 13854,\n",
       " 'important': 13831,\n",
       " 'seeing': 13757,\n",
       " 'answer': 13749,\n",
       " 'season': 13740,\n",
       " 'pick': 13711,\n",
       " 'light': 13707,\n",
       " 'list': 13701,\n",
       " 'front': 13664,\n",
       " 'entire': 13645,\n",
       " 'due': 13636,\n",
       " 'amount': 13627,\n",
       " 'recently': 13625,\n",
       " 'behind': 13622,\n",
       " 'area': 13595,\n",
       " 'original': 13535,\n",
       " 'order': 13503,\n",
       " 'worse': 13498,\n",
       " 'along': 13467,\n",
       " 'haha': 13439,\n",
       " 'alone': 13413,\n",
       " 'business': 13409,\n",
       " 'enjoy': 13409,\n",
       " 'series': 13384,\n",
       " 'company': 13382,\n",
       " 'simply': 13378,\n",
       " 'non': 13280,\n",
       " 'information': 13257,\n",
       " 'ass': 13252,\n",
       " 'luck': 13162,\n",
       " 'finally': 13147,\n",
       " 'obama': 13144,\n",
       " 'plan': 13138,\n",
       " 'history': 13125,\n",
       " 'problems': 13096,\n",
       " 'red': 13055,\n",
       " 'picture': 13046,\n",
       " 'unless': 12992,\n",
       " 'learn': 12964,\n",
       " 'mother': 12963,\n",
       " 'questions': 12955,\n",
       " 'p': 12935,\n",
       " 'sub': 12916,\n",
       " 'dog': 12879,\n",
       " 'fight': 12841,\n",
       " 'wow': 12778,\n",
       " 'general': 12771,\n",
       " 'character': 12758,\n",
       " 'totally': 12735,\n",
       " 'longer': 12731,\n",
       " 'sleep': 12718,\n",
       " 'explain': 12704,\n",
       " 'worked': 12691,\n",
       " 'seriously': 12689,\n",
       " 'literally': 12687,\n",
       " 'gave': 12670,\n",
       " 'running': 12662,\n",
       " 'asking': 12655,\n",
       " 'comments': 12592,\n",
       " 'stupid': 12562,\n",
       " 'removed': 12547,\n",
       " 'die': 12531,\n",
       " 'needed': 12506,\n",
       " 'moment': 12448,\n",
       " 'tv': 12379,\n",
       " 'damn': 12359,\n",
       " 'certain': 12343,\n",
       " 'takes': 12323,\n",
       " 'young': 12315,\n",
       " 'words': 12313,\n",
       " 'crazy': 12242,\n",
       " 'whether': 12219,\n",
       " 'obviously': 12188,\n",
       " 'early': 12182,\n",
       " 'funny': 12178,\n",
       " 'currently': 12176,\n",
       " 'dead': 12174,\n",
       " 'outside': 12141,\n",
       " 'watching': 12128,\n",
       " 'main': 12119,\n",
       " 'break': 12111,\n",
       " 'large': 12088,\n",
       " 'gone': 12084,\n",
       " 'taken': 12081,\n",
       " 'weight': 12034,\n",
       " 'content': 11993,\n",
       " 'versus': 11987,\n",
       " 'president': 11954,\n",
       " 'anymore': 11953,\n",
       " 'middle': 11928,\n",
       " 'decided': 11911,\n",
       " 'fan': 11858,\n",
       " 'online': 11856,\n",
       " 'bring': 11817,\n",
       " 'song': 11817,\n",
       " 'chance': 11789,\n",
       " 'facebook': 11723,\n",
       " 't': 11708,\n",
       " 'data': 11706,\n",
       " 'within': 11594,\n",
       " 'america': 11559,\n",
       " 'drive': 11535,\n",
       " 'baby': 11529,\n",
       " 'doctor': 11463,\n",
       " 'wife': 11364,\n",
       " 'posted': 11297,\n",
       " 'research': 11270,\n",
       " 'happens': 11246,\n",
       " 'price': 11244,\n",
       " 'known': 11205,\n",
       " 'shows': 11198,\n",
       " 'interested': 11189,\n",
       " 'serious': 11189,\n",
       " 'space': 11178,\n",
       " 'community': 11177,\n",
       " 'eyes': 11174,\n",
       " 'knows': 11156,\n",
       " 'title': 11144,\n",
       " 'current': 11129,\n",
       " 'feels': 11105,\n",
       " 'clear': 11072,\n",
       " 'difference': 11049,\n",
       " 'themselves': 11027,\n",
       " 'mostly': 11003,\n",
       " 'shot': 10976,\n",
       " 'thread': 10948,\n",
       " 'dark': 10926,\n",
       " 'imagine': 10871,\n",
       " 'build': 10871,\n",
       " 'near': 10868,\n",
       " 'worst': 10866,\n",
       " 'page': 10845,\n",
       " 'hour': 10833,\n",
       " 'states': 10800,\n",
       " 'major': 10796,\n",
       " 'lose': 10734,\n",
       " 'fair': 10734,\n",
       " 'although': 10705,\n",
       " 'store': 10699,\n",
       " 'points': 10672,\n",
       " 'dude': 10663,\n",
       " 'turned': 10650,\n",
       " 'share': 10622,\n",
       " 'books': 10616,\n",
       " 'giving': 10613,\n",
       " 'personal': 10612,\n",
       " 'four': 10588,\n",
       " 'plus': 10579,\n",
       " 'perfect': 10569,\n",
       " 'inside': 10490,\n",
       " 'beautiful': 10474,\n",
       " 'starting': 10469,\n",
       " 'eating': 10450,\n",
       " 'cost': 10441,\n",
       " 'hair': 10437,\n",
       " 'morning': 10434,\n",
       " 'skin': 10433,\n",
       " 'bought': 10421,\n",
       " 'player': 10417,\n",
       " 'supposed': 10377,\n",
       " 'safe': 10339,\n",
       " 'hold': 10323,\n",
       " 'listen': 10321,\n",
       " 'meet': 10293,\n",
       " 'energy': 10264,\n",
       " 'late': 10251,\n",
       " 'poor': 10224,\n",
       " 'realize': 10213,\n",
       " 'across': 10164,\n",
       " 'per': 10144,\n",
       " 'writing': 10124,\n",
       " 'local': 10096,\n",
       " 'spend': 10090,\n",
       " 'telling': 10083,\n",
       " 'term': 10076,\n",
       " 'glad': 10071,\n",
       " 'except': 10069,\n",
       " 'russia': 10032,\n",
       " 'court': 10006,\n",
       " 'version': 9992,\n",
       " 'brother': 9976,\n",
       " 'personally': 9964,\n",
       " 'animal': 9957,\n",
       " 'figure': 9950,\n",
       " 'choice': 9918,\n",
       " 'several': 9913,\n",
       " 'thoughts': 9908,\n",
       " 'girls': 9897,\n",
       " 'actual': 9891,\n",
       " 'account': 9889,\n",
       " 'study': 9886,\n",
       " 'bed': 9881,\n",
       " 'sister': 9869,\n",
       " 'posts': 9865,\n",
       " 'g': 9843,\n",
       " 'hot': 9840,\n",
       " 'fire': 9838,\n",
       " 'attack': 9793,\n",
       " 'stand': 9773,\n",
       " 'office': 9771,\n",
       " 'meant': 9764,\n",
       " 'above': 9763,\n",
       " 'oil': 9748,\n",
       " 'players': 9744,\n",
       " 'ended': 9719,\n",
       " 'film': 9709,\n",
       " 'door': 9697,\n",
       " 'movies': 9693,\n",
       " 'act': 9690,\n",
       " 'test': 9685,\n",
       " 'hands': 9676,\n",
       " 'father': 9642,\n",
       " 'higher': 9633,\n",
       " 'write': 9580,\n",
       " 'anxiety': 9509,\n",
       " 'walk': 9506,\n",
       " 'pm': 9503,\n",
       " 'heart': 9502,\n",
       " 'million': 9492,\n",
       " 'boyfriend': 9481,\n",
       " 'gun': 9441,\n",
       " 'town': 9435,\n",
       " 'wondering': 9432,\n",
       " 'strong': 9431,\n",
       " 'service': 9430,\n",
       " 'process': 9387,\n",
       " 'science': 9358,\n",
       " 'paid': 9341,\n",
       " 'including': 9337,\n",
       " 'common': 9318,\n",
       " 'star': 9297,\n",
       " 'animals': 9290,\n",
       " 'sad': 9278,\n",
       " 'helps': 9277,\n",
       " 'fit': 9272,\n",
       " 'wonder': 9272,\n",
       " 'loved': 9254,\n",
       " 'north': 9246,\n",
       " 'wall': 9245,\n",
       " 'rights': 9240,\n",
       " 'fast': 9239,\n",
       " 'himself': 9194,\n",
       " 'note': 9162,\n",
       " 'terrible': 9146,\n",
       " 'bill': 9141,\n",
       " 'female': 9135,\n",
       " 'simple': 9132,\n",
       " 'w': 9115,\n",
       " 'hurt': 9087,\n",
       " 'evidence': 9063,\n",
       " 'straight': 9053,\n",
       " 'ways': 9050,\n",
       " 'anti': 9050,\n",
       " 'market': 9028,\n",
       " 'itself': 9018,\n",
       " 'rules': 9014,\n",
       " 'stopped': 8973,\n",
       " 'brain': 8955,\n",
       " 'voice': 8944,\n",
       " 'killed': 8938,\n",
       " 'form': 8924,\n",
       " 'towards': 8923,\n",
       " 'force': 8904,\n",
       " 'report': 8902,\n",
       " 'moving': 8865,\n",
       " 'save': 8863,\n",
       " 'consider': 8858,\n",
       " 'south': 8841,\n",
       " 'card': 8840,\n",
       " 'mental': 8839,\n",
       " 'blood': 8837,\n",
       " 'easily': 8836,\n",
       " 'changed': 8835,\n",
       " 'difficult': 8831,\n",
       " 'moved': 8822,\n",
       " 'cat': 8801,\n",
       " 'v': 8788,\n",
       " 'easier': 8765,\n",
       " 'drink': 8753,\n",
       " 'air': 8733,\n",
       " 'follow': 8714,\n",
       " 'extremely': 8701,\n",
       " 'attention': 8700,\n",
       " 'bunch': 8692,\n",
       " 'gives': 8688,\n",
       " 'site': 8687,\n",
       " 'lives': 8685,\n",
       " 'honest': 8664,\n",
       " 'forward': 8638,\n",
       " 'gotten': 8601,\n",
       " 'five': 8598,\n",
       " 'clearly': 8589,\n",
       " 'allowed': 8553,\n",
       " 'apparently': 8546,\n",
       " 'met': 8524,\n",
       " 'miss': 8513,\n",
       " 'fall': 8502,\n",
       " 'waiting': 8502,\n",
       " 'correct': 8498,\n",
       " 'vote': 8491,\n",
       " 'worry': 8465,\n",
       " 'building': 8463,\n",
       " 'specific': 8456,\n",
       " 'sick': 8414,\n",
       " 'certainly': 8395,\n",
       " 'stories': 8376,\n",
       " 'available': 8354,\n",
       " 'despite': 8351,\n",
       " 'mention': 8350,\n",
       " 'art': 8346,\n",
       " 'continue': 8338,\n",
       " 'diet': 8329,\n",
       " 'drop': 8313,\n",
       " 'send': 8294,\n",
       " 'son': 8292,\n",
       " 'reasons': 8276,\n",
       " 'special': 8274,\n",
       " 'conversation': 8257,\n",
       " 'gay': 8227,\n",
       " 'healthy': 8226,\n",
       " 'seemed': 8224,\n",
       " 'lots': 8223,\n",
       " 'date': 8221,\n",
       " 'sell': 8218,\n",
       " 'places': 8209,\n",
       " 'step': 8201,\n",
       " 'bar': 8201,\n",
       " 'position': 8196,\n",
       " 'extra': 8181,\n",
       " 'husband': 8175,\n",
       " 'church': 8123,\n",
       " 'provide': 8114,\n",
       " 'computer': 8113,\n",
       " 'argument': 8101,\n",
       " 'pictures': 8088,\n",
       " 'quality': 8077,\n",
       " 'joke': 8076,\n",
       " 'response': 8066,\n",
       " 'choose': 8064,\n",
       " 'app': 8063,\n",
       " 'green': 8035,\n",
       " 'recommend': 8033,\n",
       " 'eventually': 8033,\n",
       " 'earth': 8030,\n",
       " 'jobs': 8017,\n",
       " 'kept': 8008,\n",
       " 'view': 7986,\n",
       " 'size': 7979,\n",
       " 'spent': 7973,\n",
       " 'tax': 7972,\n",
       " 'older': 7972,\n",
       " 'o': 7947,\n",
       " 'willing': 7942,\n",
       " 'china': 7923,\n",
       " 'considered': 7920,\n",
       " 'youtube': 7917,\n",
       " 'blue': 7911,\n",
       " 'characters': 7903,\n",
       " 'noticed': 7885,\n",
       " 'focus': 7875,\n",
       " 'street': 7872,\n",
       " 'create': 7869,\n",
       " 'eye': 7867,\n",
       " 'piece': 7861,\n",
       " 'website': 7858,\n",
       " 'average': 7843,\n",
       " 'mentioned': 7811,\n",
       " 'helped': 7800,\n",
       " 'english': 7790,\n",
       " 'ideas': 7779,\n",
       " 'screen': 7772,\n",
       " 'access': 7767,\n",
       " 'vegan': 7758,\n",
       " 'biggest': 7731,\n",
       " 'style': 7727,\n",
       " 'trust': 7722,\n",
       " 'deep': 7717,\n",
       " 'uk': 7711,\n",
       " 'somewhere': 7710,\n",
       " 'damage': 7704,\n",
       " 'hopefully': 7702,\n",
       " 'apple': 7699,\n",
       " 'interest': 7686,\n",
       " 'liked': 7679,\n",
       " 'missing': 7670,\n",
       " 'considering': 7669,\n",
       " 'avoid': 7656,\n",
       " 'generally': 7649,\n",
       " 'box': 7643,\n",
       " 'n': 7640,\n",
       " 'appreciate': 7637,\n",
       " 'military': 7634,\n",
       " 'died': 7624,\n",
       " 'nearly': 7618,\n",
       " 'weekend': 7617,\n",
       " 'expect': 7613,\n",
       " 'clean': 7610,\n",
       " 'otherwise': 7599,\n",
       " 'political': 7593,\n",
       " 'software': 7576,\n",
       " 'paper': 7553,\n",
       " 'pro': 7551,\n",
       " 'following': 7544,\n",
       " 'lack': 7543,\n",
       " 'putting': 7538,\n",
       " 'journal': 7531,\n",
       " 'fix': 7531,\n",
       " 'info': 7530,\n",
       " 'multiple': 7523,\n",
       " 'walking': 7518,\n",
       " 'total': 7489,\n",
       " 'quick': 7480,\n",
       " 'program': 7477,\n",
       " 'cover': 7445,\n",
       " 'according': 7444,\n",
       " 'entry': 7430,\n",
       " 'key': 7428,\n",
       " 'pass': 7419,\n",
       " 'ready': 7416,\n",
       " 'yesterday': 7410,\n",
       " 'lead': 7409,\n",
       " 'scene': 7396,\n",
       " 'rate': 7389,\n",
       " 'st': 7386,\n",
       " 'match': 7373,\n",
       " 'possibly': 7359,\n",
       " 'cold': 7357,\n",
       " 'girlfriend': 7351,\n",
       " 'effect': 7349,\n",
       " 'sit': 7348,\n",
       " 'episode': 7343,\n",
       " 'complete': 7327,\n",
       " 'action': 7325,\n",
       " 'companies': 7316,\n",
       " 'finding': 7316,\n",
       " 'option': 7306,\n",
       " 'calling': 7295,\n",
       " 'forget': 7289,\n",
       " 'text': 7288,\n",
       " 'risk': 7277,\n",
       " 'offer': 7259,\n",
       " 'male': 7238,\n",
       " 'ground': 7222,\n",
       " 'wear': 7212,\n",
       " 'quickly': 7211,\n",
       " 'record': 7210,\n",
       " 'knowing': 7207,\n",
       " 'sucks': 7200,\n",
       " 'decision': 7195,\n",
       " 'doubt': 7174,\n",
       " 'search': 7174,\n",
       " 'none': 7172,\n",
       " 'fear': 7162,\n",
       " 'fat': 7159,\n",
       " 'brought': 7158,\n",
       " 'realized': 7155,\n",
       " 'hoping': 7146,\n",
       " 'nobody': 7138,\n",
       " 'cheap': 7112,\n",
       " 'related': 7094,\n",
       " 'product': 7080,\n",
       " 'student': 7076,\n",
       " 'pc': 7072,\n",
       " 'album': 7056,\n",
       " 'random': 7048,\n",
       " 'fans': 7047,\n",
       " 'sent': 7037,\n",
       " 'speak': 7031,\n",
       " 'message': 7030,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = build_vocab(train_df['Text'])\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_embedding_coverage(token_vocab,embeddings):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(token_vocab):\n",
    "        try:\n",
    "            a[word] = embeddings[word]\n",
    "            k += token_vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = token_vocab[word]\n",
    "            i += token_vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(token_vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext_embeddings['dsfjsdkjfh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_embedding_coverage(vocab_dict, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:552: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if 'GLOVE' in EMBEDDINGS:\n",
    "    embeddings.add_vector('<pad>',np.zeros((embeddings.vector_size)))\n",
    "    embeddings.add_vector('<unk>',np.mean(embeddings.vectors,axis=0, keepdims=True)[0])\n",
    "elif EMBEDDINGS=='FASTTEXT':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'GLOVE' in EMBEDDINGS:\n",
    "    \n",
    "    train_df['Tokens'] = train_df['Text'].apply(lambda s:[embeddings.key_to_index.get(tok,embeddings.key_to_index['<unk>']) for tok in re.findall(r\"\\w+|[^\\w\\s]\", s, re.UNICODE)])\n",
    "    #test_df['Tokens'] = test_df['Text'].apply(lambda s:[embeddings.key_to_index.get(tok,glove_embeddings.key_to_index['<unk>']) for tok in s.split(\" \")])\n",
    "#elif EMBEDDINGS=='FASTTEXT':\n",
    "#    train_df['Tokens'] = train_df['Text'].apply(lambda s:[fasttext_embeddings[tok] for tok in s.split(\" \")])\n",
    "#    test_df['Tokens'] = test_df['Text'].apply(lambda s:[fasttext_embeddings[tok] for tok in s.split(\" \")])\n",
    "#X_train = X_train.apply(lambda s:np.mean([glove_embeddings.get(tok,glove_embeddings['<unk>']) for tok in s.split(\" \")]))\n",
    "    #train_df['Tokens']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_post(post, embeddings,method='mean'):\n",
    "    #Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\n",
    "    #TODO: represent oov words with a random vector generated with the embeddings mean and std_dev\n",
    "    \n",
    "    vecs = []\n",
    "    for token in post:\n",
    "        #print(word)\n",
    "        #if word in embeddings:\n",
    "        #vecs+=[embeddings[token]]\n",
    "        vecs+=[embeddings[token]]\n",
    "        \n",
    "        #vecs+=[embeddings.get_vector(token, norm=True)]\n",
    "        \n",
    "        #else:\n",
    "        #    vecs+=[np.zeros(embeddings.vector_size)]\n",
    "    if method=='mean':\n",
    "        return np.mean(vecs, axis=0)\n",
    "    elif method=='max':\n",
    "        return np.max(vecs, axis=0)\n",
    "    elif method=='min':\n",
    "        return np.min(vecs, axis=0)\n",
    "        \n",
    "    \n",
    "    #return embeddings.get_mean_vector(post, pre_normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_train = pd.DataFrame(train_df['Vector'].values.tolist(), index = train_df.index)\\ny_train = train_df['Label']\\nX_test = pd.DataFrame(test_df['Vector'].values.tolist(), index = test_df.index)\\n\\n#X_test = test_df[['User','Window_id','Vector']]\\ny_test = test_df['Label']\\nX_train\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "if 'GLOVE' in EMBEDDINGS:\n",
    "    train_df['Vector'] = train_df['Tokens'].apply(lambda post:vectorize_post(post,embeddings,METHOD))\n",
    "    #test_df['Vector'] = test_df['Tokens'].apply(lambda post:vectorize_post(post,embeddings,METHOD))\n",
    "\n",
    "elif 'FASTTEXT' in EMBEDDINGS:\n",
    "    train_df['Vector'] = train_df['Text'].apply(lambda post:vectorize_post(re.findall(r\"\\w+|[^\\w\\s]\", post, re.UNICODE),embeddings,METHOD))\n",
    "    #test_df['Vector'] = test_df['Text'].apply(lambda post:vectorize_post(post.split(\" \"),embeddings,METHOD))\n",
    "\n",
    "#X_train_df = pd.DataFrame(train_df['Vectors'].to_list(), columns = [i for i in range(200)])\n",
    "#X_train_df\n",
    "\"\"\"\n",
    "X_train = pd.DataFrame(train_df['Vector'].values.tolist(), index = train_df.index)\n",
    "y_train = train_df['Label']\n",
    "X_test = pd.DataFrame(test_df['Vector'].values.tolist(), index = test_df.index)\n",
    "\n",
    "#X_test = test_df[['User','Window_id','Vector']]\n",
    "y_test = test_df['Label']\n",
    "X_train\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef objective(trial):\\n    parameters = {\\n        'embeddings':trial.suggest_categorical('embeddings',list(we.keys())),\\n        'model':trial.suggest_categorical('model',list(models.keys())),\\n        \\n        \\n    }\\n    \\n  \\n    \\n    avg_f1 = train_eval(trial,parameters,5,train_df)\\n    return np.mean(avg_f1)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def objective(trial):\n",
    "    parameters = {\n",
    "        'embeddings':trial.suggest_categorical('embeddings',list(we.keys())),\n",
    "        'model':trial.suggest_categorical('model',list(models.keys())),\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    \n",
    "    avg_f1 = train_eval(trial,parameters,5,train_df)\n",
    "    return np.mean(avg_f1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tqdm import tqdm\\nfrom sklearn.metrics import f1_score\\n\\ndef train_eval(trial,params, n_folds, df):\\n\\n    skf = StratifiedKFold(n_splits=n_folds)\\n    user_label_df =df.drop_duplicates('User')\\n    users = user_label_df['User'].to_numpy()\\n    \\n    labels = user_label_df['Label'].to_numpy()\\n\\n    #format the vectorizer can accept\\n    params['stop_words'] = sw_lists[params['stop_words']]\\n    params['ngram_range'] = (1,params['ngram_range'])\\n    \\n    f1_scores = []\\n    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\\n        transformer = TfidfVectorizer(**params)\\n        train_users = [users[f] for f in train_index]\\n        test_users = [users[f] for f in test_index]\\n\\n        train_folds = df[df['User'].isin(train_users)]\\n        X_train = transformer.fit_transform(train_folds['Text'])\\n        test_folds = df[df['User'].isin(test_users)]\\n        X_test = transformer.transform(test_folds['Text'])\\n    \\n        model = MultinomialNB()\\n        model.fit(X_train, train_folds['Label'])\\n        f1_scores.append(f1_score(test_folds['Label'],model.predict(X_test)))\\n    \\n        trial.report(np.mean(f1_scores), fold)\\n\\n        if trial.should_prune():\\n            raise optuna.exceptions.TrialPruned()\\n\\n    return f1_scores\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_eval(trial,params, n_folds, df):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    user_label_df =df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    \n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "\n",
    "    #format the vectorizer can accept\n",
    "    params['stop_words'] = sw_lists[params['stop_words']]\n",
    "    params['ngram_range'] = (1,params['ngram_range'])\n",
    "    \n",
    "    f1_scores = []\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "        transformer = TfidfVectorizer(**params)\n",
    "        train_users = [users[f] for f in train_index]\n",
    "        test_users = [users[f] for f in test_index]\n",
    "\n",
    "        train_folds = df[df['User'].isin(train_users)]\n",
    "        X_train = transformer.fit_transform(train_folds['Text'])\n",
    "        test_folds = df[df['User'].isin(test_users)]\n",
    "        X_test = transformer.transform(test_folds['Text'])\n",
    "    \n",
    "        model = MultinomialNB()\n",
    "        model.fit(X_train, train_folds['Label'])\n",
    "        f1_scores.append(f1_score(test_folds['Label'],model.predict(X_test)))\n",
    "    \n",
    "        trial.report(np.mean(f1_scores), fold)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return f1_scores\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def custom_cv(model, df, n_folds=5,sent =False ,threshold=None):\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    user_label_df =df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    \n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "    #print(labels)\n",
    "    #print(users.shape,labels.shape)\n",
    "    \n",
    "    f1_scores = []\n",
    "    for train_index, test_index in skf.split(users, labels):\n",
    "        train_users = [users[f] for f in train_index]\n",
    "        test_users = [users[f] for f in test_index]\n",
    "\n",
    "        train_folds = df[df['User'].isin(train_users)].copy()\n",
    "        test_folds = df[df['User'].isin(test_users)].copy()\n",
    "        #train_folds['Tokens'] = train_folds['Text'].apply(lambda s:[glove_embeddings.key_to_index.get(tok,glove_embeddings.key_to_index['<unk>']) for tok in s.split(\" \")])\n",
    "        #test_folds['Tokens'] = test_folds['Text'].apply(lambda s:[glove_embeddings.key_to_index.get(tok,glove_embeddings.key_to_index['<unk>']) for tok in s.split(\" \")])\n",
    "        #train_folds['Vector'] = train_folds['Tokens'].apply(lambda post:vectorize_post(post,glove_embeddings,METHOD))\n",
    "        #test_folds['Vector'] = test_folds['Tokens'].apply(lambda post:vectorize_post(post,glove_embeddings,METHOD))\n",
    "        X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "        y_train = train_folds['Label']\n",
    "        X_test = pd.DataFrame(test_folds['Vector'].values.tolist(), index = test_folds.index)\n",
    "        y_test = test_folds['Label']\n",
    "        if sent:\n",
    "            #scaler = MinMaxScaler()\n",
    "            #train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.fit_transform(train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            #test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.transform(test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "            X_test = np.c_[X_test,test_folds['polarity'],test_folds['subjectivity'],test_folds['negativity'],test_folds['positivity'],test_folds['neutrality'], test_folds['compound']] \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        f1_scores.append(f1_score(y_test,model.predict(X_test)))\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.apply(lambda x: vectorize_post(x, glove_embeddings))\n",
    "#X_train\n",
    "#train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_train[0].shape\n",
    "#if sentiment:\n",
    "#    X_train = np.c_[X_train,train_df['polarity'],train_df['subjectivity'],train_df['negativity'],train_df['positivity'],train_df['neutrality'], train_df['compound']] \n",
    "#    X_test = np.c_[X_test,test_df['polarity'],test_df['subjectivity'],test_df['negativity'],test_df['positivity'],test_df['neutrality'], test_df['compound']] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [4.671169e-05, -0.034060024, -0.021581221, -0....\n",
       "1         [0.007912025, -0.043180943, -0.08589363, -0.12...\n",
       "2         [0.018356469, 0.012983114, -0.0352655, -0.0779...\n",
       "3         [-0.021287752, 0.0005717801, -0.08342346, -0.2...\n",
       "4         [-0.01959065, 0.013299365, -0.057640214, -0.09...\n",
       "                                ...                        \n",
       "210922    [-0.050597902, 0.023176132, -0.09026915, -0.07...\n",
       "210923    [-0.024048736, 0.014176603, -0.045604512, -0.1...\n",
       "210924    [-0.04207734, -0.010719161, -0.09014639, -0.07...\n",
       "210925    [-0.023640716, 0.0012010762, -0.01621502, -0.1...\n",
       "210926    [-0.025706569, -0.043355275, -0.020992147, -0....\n",
       "Name: Vector, Length: 210927, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "models = {\n",
    "        'sgdLR':SGDClassifier(random_state=seed,loss='log'),\n",
    "        #'NB':MultinomialNB(),\\\n",
    "        'sgdlSVM':SGDClassifier(random_state=seed,loss='hinge'),\n",
    "        'ExtraTrees':ExtraTreesClassifier(random_state=seed,n_jobs=-1),\\\n",
    "        'Perceptron':Perceptron(random_state=seed)}\n",
    "if BASELINE_COMP:\n",
    "        report=\"\"\n",
    "        best_model_name = \"\"\n",
    "        best_model=None\n",
    "        best_f1=0\n",
    "        for model_name, model in models.items():\n",
    "                res = custom_cv(model,train_df)\n",
    "                if np.mean(res) > best_f1:\n",
    "                        best_f1=np.mean(res)\n",
    "                        best_model_name=model_name\n",
    "                        best_model=model\n",
    "                #train_df[train_df['User'].isin(flds[0][0])].describe()\n",
    "                report+=f\"{model_name} f1: {round(np.mean(res),3)}\\n\"\n",
    "                print(f\"{model_name} f1: {round(np.mean(res),3)}\")\n",
    "        with open(f\"{MODEL_PATH}/baseline_report_{EMBEDDINGS}.txt\",'w') as f:\n",
    "                f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BASELINE_COMP:\n",
    "    from textblob import TextBlob\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    train_df['TB'] = train_df['Text'].apply(lambda text: TextBlob(text).sentiment)\n",
    "    train_df['VADER'] = train_df['Text'].apply(lambda text: sia.polarity_scores(text))\n",
    "\n",
    "\n",
    "    train_df['polarity'] = train_df['TB'].apply(lambda tb: (tb[0]+1/2))\n",
    "    train_df['subjectivity'] = train_df['TB'].apply(lambda tb: tb[1])\n",
    "    train_df['negativity'] = train_df['VADER'].apply(lambda v: v['neg'])\n",
    "    train_df['positivity'] = train_df['VADER'].apply(lambda v: v['pos'])\n",
    "    train_df['neutrality'] = train_df['VADER'].apply(lambda v: v['neu'])\n",
    "    train_df['compound'] = train_df['VADER'].apply(lambda v: (v['compound']+1)/2)\n",
    "\n",
    "\n",
    "\n",
    "    train_df.drop(['VADER','TB'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_name = \"sgdLR\"\n",
    "#best_model = models[best_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BASELINE_COMP:\n",
    "\n",
    "        res = custom_cv(best_model,train_df, sent=True)\n",
    "        print(f\"{best_model_name} f1: {round(np.mean(res),3)}\")\n",
    "        #ADD CV WHEN CHANGING\n",
    "        with open(f\"{MODEL_PATH}/baseline_report_{EMBEDDINGS}.txt\",'a') as f:\n",
    "                f.write(f\"\\nBest model with sent:\\n{best_model_name} f1: {round(np.mean(res),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best performing model was sgd Logistic Regression for window size 10, GLOVE_CC without SA features, having achieved a 0.664 F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "def train_eval_tuning(trial,params, df, sent=False):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    user_label_df =df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    \n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "\n",
    "    \n",
    "    f1_scores = []\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "        train_users = [users[f] for f in train_index]\n",
    "        test_users = [users[f] for f in test_index]\n",
    "\n",
    "        train_folds = df[df['User'].isin(train_users)]\n",
    "        X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "\n",
    "        test_folds = df[df['User'].isin(test_users)]\n",
    "        X_test = pd.DataFrame(test_folds['Vector'].values.tolist(), index = test_folds.index)\n",
    "\n",
    "        model = SGDClassifier(**params)\n",
    "        if sent:\n",
    "            #scaler = MinMaxScaler()\n",
    "            #train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.fit_transform(train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            #test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.transform(test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "            X_test = np.c_[X_test,test_folds['polarity'],test_folds['subjectivity'],test_folds['negativity'],test_folds['positivity'],test_folds['neutrality'], test_folds['compound']] \n",
    "            \n",
    "\n",
    "        model.fit(X_train, train_folds['Label'])\n",
    "        f1_scores.append(f1_score(test_folds['Label'],model.predict(X_test)))\n",
    "    \n",
    "        trial.report(np.mean(f1_scores), fold)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_objective(trial):\n",
    "    parameters = {\n",
    "        'max_iter':trial.suggest_int('max_iter',1000,2500,step=500),\n",
    "        'loss':trial.suggest_categorical('loss',['log']),\n",
    "        'penalty':trial.suggest_categorical('penalty',['l2','l1','elasticnet']),\n",
    "        'alpha': trial.suggest_float('alpha',0.00001,0.1,log=True),\n",
    "        'random_state':trial.suggest_int('random_state',seed,seed)\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    \n",
    "    avg_f1 = train_eval_tuning(trial,parameters,train_df, sent=False)\n",
    "    return np.mean(avg_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-15 18:57:15,757]\u001b[0m A new study created in memory with name: t2_tuning_10\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 18:58:47,004]\u001b[0m Trial 0 finished with value: 0.6463409252051037 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.0020111304744450066, 'random_state': 23}. Best is trial 0 with value: 0.6463409252051037.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:00:22,923]\u001b[0m Trial 1 finished with value: 0.48546192282948475 and parameters: {'max_iter': 2500, 'loss': 'log', 'penalty': 'l1', 'alpha': 0.011087888556563064, 'random_state': 23}. Best is trial 0 with value: 0.6463409252051037.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:01:47,260]\u001b[0m Trial 2 finished with value: 0.4488368514406533 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.053122523966708794, 'random_state': 23}. Best is trial 0 with value: 0.6463409252051037.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:03:11,626]\u001b[0m Trial 3 finished with value: 0.46664696951417384 and parameters: {'max_iter': 2500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.04828409433448965, 'random_state': 23}. Best is trial 0 with value: 0.6463409252051037.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:04:50,697]\u001b[0m Trial 4 finished with value: 0.6799641042011846 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 4.86518273278276e-05, 'random_state': 23}. Best is trial 4 with value: 0.6799641042011846.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:06:19,531]\u001b[0m Trial 5 finished with value: 0.6379872279537678 and parameters: {'max_iter': 2500, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.003123789335167716, 'random_state': 23}. Best is trial 4 with value: 0.6799641042011846.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:07:53,240]\u001b[0m Trial 6 finished with value: 0.6411410505274177 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'l1', 'alpha': 0.0003786549559831019, 'random_state': 23}. Best is trial 4 with value: 0.6799641042011846.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:08:11,944]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:08:29,971]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:09:58,928]\u001b[0m Trial 9 finished with value: 0.6747767466360891 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 6.864781545115735e-05, 'random_state': 23}. Best is trial 4 with value: 0.6799641042011846.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:11:55,748]\u001b[0m Trial 10 finished with value: 0.6603254049774131 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 1.1687549647049014e-05, 'random_state': 23}. Best is trial 4 with value: 0.6799641042011846.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:13:32,791]\u001b[0m Trial 11 finished with value: 0.6831267114308971 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 5.657160894198064e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:15:07,297]\u001b[0m Trial 12 finished with value: 0.6540732323904458 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.00010230120865557899, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:17:04,199]\u001b[0m Trial 13 finished with value: 0.6641803441515421 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 1.0703540162074367e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:18:40,507]\u001b[0m Trial 14 finished with value: 0.6715591204539317 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 7.151167965440602e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:18:58,070]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:20:40,464]\u001b[0m Trial 16 finished with value: 0.6640179311470327 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 3.338911232757424e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:20:58,383]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:22:28,107]\u001b[0m Trial 18 finished with value: 0.6458699409587072 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.0010108441070461973, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:24:11,932]\u001b[0m Trial 19 finished with value: 0.663070902427265 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 3.0876441383686064e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:25:43,807]\u001b[0m Trial 20 finished with value: 0.6703010088840197 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.0001230521274923015, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:27:15,811]\u001b[0m Trial 21 finished with value: 0.6638736006096361 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 3.1018123156366046e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:28:44,496]\u001b[0m Trial 22 finished with value: 0.6756364124791271 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 6.421231641511565e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:30:09,953]\u001b[0m Trial 23 finished with value: 0.6579551565780207 and parameters: {'max_iter': 1500, 'loss': 'log', 'penalty': 'l2', 'alpha': 0.00021856837188459035, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:30:28,147]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:31:57,242]\u001b[0m Trial 25 finished with value: 0.6819807611661386 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 5.2480887853359184e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:32:14,535]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:32:32,890]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:32:56,375]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:33:13,042]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:34:41,805]\u001b[0m Trial 30 finished with value: 0.681924160119448 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 5.531857946880474e-05, 'random_state': 23}. Best is trial 11 with value: 0.6831267114308971.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:36:11,819]\u001b[0m Trial 31 finished with value: 0.6832701808135332 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 4.405571768858297e-05, 'random_state': 23}. Best is trial 31 with value: 0.6832701808135332.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:37:47,128]\u001b[0m Trial 32 finished with value: 0.6583216202309194 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 1.7867020086618465e-05, 'random_state': 23}. Best is trial 31 with value: 0.6832701808135332.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:38:04,374]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:39:34,225]\u001b[0m Trial 34 finished with value: 0.6832959236146376 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 4.289226463460141e-05, 'random_state': 23}. Best is trial 34 with value: 0.6832959236146376.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:41:04,935]\u001b[0m Trial 35 finished with value: 0.670082269698827 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 4.0679188717460124e-05, 'random_state': 23}. Best is trial 34 with value: 0.6832959236146376.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:41:23,876]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:41:41,130]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:41:58,039]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:42:14,544]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:42:33,194]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:44:01,905]\u001b[0m Trial 41 finished with value: 0.6818709227031174 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 5.7888644216273065e-05, 'random_state': 23}. Best is trial 34 with value: 0.6832959236146376.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:45:31,643]\u001b[0m Trial 42 finished with value: 0.6786922044640377 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 4.792928187419163e-05, 'random_state': 23}. Best is trial 34 with value: 0.6832959236146376.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:46:59,898]\u001b[0m Trial 43 finished with value: 0.6748133483204819 and parameters: {'max_iter': 1000, 'loss': 'log', 'penalty': 'l2', 'alpha': 7.075415844386749e-05, 'random_state': 23}. Best is trial 34 with value: 0.6832959236146376.\u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:47:16,843]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:47:35,060]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:47:54,058]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:48:10,592]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:48:30,533]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-06-15 19:48:47,328]\u001b[0m Trial 49 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "        study_name=f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}\",\n",
    "        direction='maximize')\n",
    "study.optimize(tuning_objective, n_trials=50, timeout=(60*60*3))\n",
    "joblib.dump(study,f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "    #study = optuna.create_study(study_name=f\"tfidfvectorizer_cv_{ROLLING_WINDOW_SIZE}\",direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.NopPruner())\n",
    "study = joblib.load(f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_iter': 1000,\n",
       "  'loss': 'log',\n",
       "  'penalty': 'l2',\n",
       "  'alpha': 4.289226463460141e-05,\n",
       "  'random_state': 23},\n",
       " 0.6832959236146376)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{MODEL_PATH}/baseline_report_{EMBEDDINGS}.txt\",'a') as f:\n",
    "                f.write(f\"\\nOptimized model f1: {round(study.best_value,3)}\\nparams: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdLR_params = study.best_trial.params\n",
    "\n",
    "final_model = SGDClassifier(**sgdLR_params)\n",
    "full_train = pd.DataFrame(train_df['Vector'].values.tolist(), index = train_df.index)\n",
    "\n",
    "final_model.fit(full_train, train_df['Label'])\n",
    "save_model(final_model, \"optimized_sgdLR.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
