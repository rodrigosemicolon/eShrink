{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (Detecting depressed subjects)\n",
    "### Using contextualized language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_FILE = \"../posts.csv\"\n",
    "TRAIN_TOKEN=\"../train_df.csv\"\n",
    "TEST_TOKEN=\"../test_df.csv\"\n",
    "GENERAL_MODELS=\"../Models\"\n",
    "ROLLING_WINDOW_SIZE=10\n",
    "LM_MODEL='all-mpnet-base-v2'\n",
    "#'all-MiniLM-L6-v2'  'all-mpnet-base-v2'\n",
    "TASK=2\n",
    "CONVERTED=True\n",
    "SENT_MEASURED=True\n",
    "BASELINE_COMP=False\n",
    "MODEL_PATH =f\"{GENERAL_MODELS}/LM/win_{ROLLING_WINDOW_SIZE}_{LM_MODEL}\" \n",
    "max_lengths={1:64,3:128,5:256,10:512}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(MODEL_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opening resulting dataset with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(df, window_size,stride, field):\n",
    "    res_map={}\n",
    "    for user in df['User'].unique():\n",
    "        user_df = df[df['User']==user]\n",
    "        res_map[user]=(user_df['Label'].values[0],{})\n",
    "        posts = user_df[field].values\n",
    "        iteration=0\n",
    "        for i in range(0,len(posts),stride):\n",
    "            res_map[user][1][iteration]=' '.join((posts[i:i+window_size]))\n",
    "            iteration+=1\n",
    "    result_df = pd.DataFrame([(k,k1,v1,v[0]) for k,v in res_map.items() for k1,v1 in v[1].items()], columns = ['User','Window_id','Text','Label'])\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Store sentences & embeddings on disc\n",
    "def save_embeddings(filepath, embeddings):\n",
    "    with open(filepath, \"wb\") as fOut:\n",
    "        pickle.dump({ 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    return stored_embeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "seed=23\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "if not SENT_MEASURED:\n",
    "    train_df = pd.read_csv(TRAIN_TOKEN, sep='\\t')\n",
    "    test_df = pd.read_csv(TEST_TOKEN, sep='\\t')\n",
    "    train_df = rolling_window(train_df,ROLLING_WINDOW_SIZE,1,'Raw')\n",
    "    test_df = rolling_window(test_df,ROLLING_WINDOW_SIZE,1,'Raw')\n",
    "\n",
    "    from textblob import TextBlob\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    train_df['TB'] = train_df['Text'].apply(lambda text: TextBlob(text).sentiment)\n",
    "    train_df['VADER'] = train_df['Text'].apply(lambda text: sia.polarity_scores(text))\n",
    "    train_df['polarity'] = train_df['TB'].apply(lambda tb: (tb[0]+1/2))\n",
    "    train_df['subjectivity'] = train_df['TB'].apply(lambda tb: tb[1])\n",
    "    train_df['negativity'] = train_df['VADER'].apply(lambda v: v['neg'])\n",
    "    train_df['positivity'] = train_df['VADER'].apply(lambda v: v['pos'])\n",
    "    train_df['neutrality'] = train_df['VADER'].apply(lambda v: v['neu'])\n",
    "    train_df['compound'] = train_df['VADER'].apply(lambda v: (v['compound']+1)/2)\n",
    "    train_df.drop(['VADER','TB'], inplace=True, axis=1)\n",
    "\n",
    "    test_df['TB'] = test_df['Text'].apply(lambda text: TextBlob(text).sentiment)\n",
    "    test_df['VADER'] = test_df['Text'].apply(lambda text: sia.polarity_scores(text))\n",
    "    test_df['polarity'] = test_df['TB'].apply(lambda tb: (tb[0]+1/2))\n",
    "    test_df['subjectivity'] = test_df['TB'].apply(lambda tb: tb[1])\n",
    "    test_df['negativity'] = test_df['VADER'].apply(lambda v: v['neg'])\n",
    "    test_df['positivity'] = test_df['VADER'].apply(lambda v: v['pos'])\n",
    "    test_df['neutrality'] = test_df['VADER'].apply(lambda v: v['neu'])\n",
    "    test_df['compound'] = test_df['VADER'].apply(lambda v: (v['compound']+1)/2)\n",
    "    test_df.drop(['VADER','TB'], inplace=True, axis=1)\n",
    "\n",
    "    train_df.to_pickle(f\"train_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n",
    "    test_df.to_pickle(f\"test_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n",
    "train_df = pd.read_pickle(f\"train_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n",
    "test_df = pd.read_pickle(f\"test_df_{ROLLING_WINDOW_SIZE}_sent.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Window_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>negativity</th>\n",
       "      <th>positivity</th>\n",
       "      <th>neutrality</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>0</td>\n",
       "      <td>so many unwanted smith fadeaways. mid range j...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.663051</td>\n",
       "      <td>0.516980</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.98315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>1</td>\n",
       "      <td>mid range jumpers hey guys, celtics fan here p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650092</td>\n",
       "      <td>0.517633</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.98440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>2</td>\n",
       "      <td>well he got number tonight so maybe he will b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588214</td>\n",
       "      <td>0.594881</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.75595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>3</td>\n",
       "      <td>i mean he will get pinch hits and an occasion...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573929</td>\n",
       "      <td>0.588929</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.60815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>4</td>\n",
       "      <td>yeah you are probably right. oh well.  i gues...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.626531</td>\n",
       "      <td>0.626531</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.63660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174168</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>627</td>\n",
       "      <td>nothing like that clean house feeling  there i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.834343</td>\n",
       "      <td>0.551515</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.28280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174169</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>628</td>\n",
       "      <td>there is always that one coworker...  there is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174170</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>629</td>\n",
       "      <td>there is always that one coworker you just can...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174171</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>630</td>\n",
       "      <td>that moment when you realize you need a new jo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174172</th>\n",
       "      <td>subject9959</td>\n",
       "      <td>631</td>\n",
       "      <td>as an artist, this speaks to me on so many lev...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174173 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    User  Window_id  \\\n",
       "0       test_subject1345          0   \n",
       "1       test_subject1345          1   \n",
       "2       test_subject1345          2   \n",
       "3       test_subject1345          3   \n",
       "4       test_subject1345          4   \n",
       "...                  ...        ...   \n",
       "174168       subject9959        627   \n",
       "174169       subject9959        628   \n",
       "174170       subject9959        629   \n",
       "174171       subject9959        630   \n",
       "174172       subject9959        631   \n",
       "\n",
       "                                                     Text  Label  polarity  \\\n",
       "0        so many unwanted smith fadeaways. mid range j...      1  0.663051   \n",
       "1       mid range jumpers hey guys, celtics fan here p...      1  0.650092   \n",
       "2        well he got number tonight so maybe he will b...      1  0.588214   \n",
       "3        i mean he will get pinch hits and an occasion...      1  0.573929   \n",
       "4        yeah you are probably right. oh well.  i gues...      1  0.626531   \n",
       "...                                                   ...    ...       ...   \n",
       "174168  nothing like that clean house feeling  there i...      0  0.834343   \n",
       "174169  there is always that one coworker...  there is...      0  0.818182   \n",
       "174170  there is always that one coworker you just can...      0  0.818182   \n",
       "174171  that moment when you realize you need a new jo...      0  0.818182   \n",
       "174172  as an artist, this speaks to me on so many lev...      0  1.000000   \n",
       "\n",
       "        subjectivity  negativity  positivity  neutrality  compound  \n",
       "0           0.516980       0.053       0.145       0.802   0.98315  \n",
       "1           0.517633       0.048       0.147       0.805   0.98440  \n",
       "2           0.594881       0.082       0.121       0.797   0.75595  \n",
       "3           0.588929       0.088       0.107       0.805   0.60815  \n",
       "4           0.626531       0.091       0.118       0.792   0.63660  \n",
       "...              ...         ...         ...         ...       ...  \n",
       "174168      0.551515       0.097       0.033       0.869   0.28280  \n",
       "174169      0.477273       0.000       0.000       1.000   0.50000  \n",
       "174170      0.477273       0.000       0.000       1.000   0.50000  \n",
       "174171      0.477273       0.000       0.000       1.000   0.50000  \n",
       "174172      0.500000       0.000       0.000       1.000   0.50000  \n",
       "\n",
       "[174173 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not CONVERTED:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(LM_MODEL)\n",
    "\n",
    "    model.max_seq_length = max_lengths[ROLLING_WINDOW_SIZE]\n",
    "    train_sentence_embeddings = model.encode(train_df['Text'],show_progress_bar=True,\\\n",
    "                output_value='sentence_embedding', batch_size=64,convert_to_numpy=True)\n",
    "\n",
    "    val_sentence_embeddings = model.encode(test_df['Text'],show_progress_bar=True,\\\n",
    "                output_value='sentence_embedding', batch_size=64,convert_to_numpy=True)\n",
    "\n",
    "    save_embeddings(f\"./train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\",train_sentence_embeddings)\n",
    "    save_embeddings(f\"./val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\",val_sentence_embeddings)\n",
    "else:\n",
    "    train_sentence_embeddings = load_embeddings(f\"./train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "    val_sentence_embeddings = load_embeddings(f\"./val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "   \n",
    "test_df['Vector'] = pd.DataFrame(data=val_sentence_embeddings).values.tolist()\n",
    "train_df['Vector'] = pd.DataFrame(data=train_sentence_embeddings).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = pd.concat([train_df,test_df])\n",
    "full_df = full_df.sample(frac=1, random_state=seed).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import ast\n",
    "#full_df['Vector'] = full_df['Vector'].apply(lambda x: ast.literal_eval(x))\n",
    "#full_df['Vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.006825853139162064,\n",
       " -0.03172358125448227,\n",
       " 0.0039428649470210075,\n",
       " 0.029382312670350075,\n",
       " 0.04264417663216591,\n",
       " -0.00017471550381742418,\n",
       " 0.030590660870075226,\n",
       " -0.006566774565726519,\n",
       " -0.04553942382335663,\n",
       " 0.008927689865231514,\n",
       " -0.0451522096991539,\n",
       " 0.012555192224681377,\n",
       " 0.002309935400262475,\n",
       " 0.005999535787850618,\n",
       " -0.001322921016253531,\n",
       " -0.018175924196839333,\n",
       " 0.020022952929139137,\n",
       " 0.016082672402262688,\n",
       " -0.01639033667743206,\n",
       " -0.00021614790603052825,\n",
       " 0.014634969644248486,\n",
       " 0.02281537838280201,\n",
       " 0.07282295823097229,\n",
       " 0.005920167081058025,\n",
       " -0.07031570374965668,\n",
       " -0.005808057263493538,\n",
       " 0.03189411759376526,\n",
       " 0.0314079225063324,\n",
       " -0.03451507166028023,\n",
       " -0.014787799678742886,\n",
       " -0.05575726181268692,\n",
       " 0.05235115811228752,\n",
       " -0.05452913045883179,\n",
       " 0.04604969918727875,\n",
       " 2.3062059426592896e-06,\n",
       " -0.04092087596654892,\n",
       " 0.02581779845058918,\n",
       " 0.014341291040182114,\n",
       " 0.004314091522246599,\n",
       " 0.0270550400018692,\n",
       " 0.023322811350226402,\n",
       " 0.06046494096517563,\n",
       " -0.02539115585386753,\n",
       " -0.0037427141796797514,\n",
       " 0.0018929578363895416,\n",
       " -0.09867250919342041,\n",
       " -0.0270045455545187,\n",
       " 0.03586399182677269,\n",
       " 0.04958539083600044,\n",
       " -0.049634624272584915,\n",
       " 0.004963148385286331,\n",
       " 0.007692479062825441,\n",
       " -0.0065096355974674225,\n",
       " -0.0053650448098778725,\n",
       " 0.05129902437329292,\n",
       " -0.038790520280599594,\n",
       " -0.005479909013956785,\n",
       " -0.04275200515985489,\n",
       " 0.08836439996957779,\n",
       " -0.02283415198326111,\n",
       " 0.038698989897966385,\n",
       " 0.020503468811511993,\n",
       " -0.013243832625448704,\n",
       " -0.005645782686769962,\n",
       " -0.009074585512280464,\n",
       " 0.0014683303888887167,\n",
       " 0.024771306663751602,\n",
       " -0.03246862813830376,\n",
       " 0.04377102479338646,\n",
       " 0.005002276971936226,\n",
       " 0.06729335337877274,\n",
       " 0.038586948066949844,\n",
       " -0.014020408503711224,\n",
       " 0.0811767652630806,\n",
       " -0.027927016839385033,\n",
       " 0.04974718391895294,\n",
       " 0.04383744299411774,\n",
       " -0.014841129072010517,\n",
       " -0.04609467834234238,\n",
       " -0.022286873310804367,\n",
       " -0.0019015740836039186,\n",
       " 0.03555505722761154,\n",
       " 0.019193029031157494,\n",
       " -0.0067524537444114685,\n",
       " 0.02580609731376171,\n",
       " 0.026327034458518028,\n",
       " 0.0044572665356099606,\n",
       " -0.006457638926804066,\n",
       " -0.023375308141112328,\n",
       " 0.016765618696808815,\n",
       " 0.0727507621049881,\n",
       " -0.0011984253069385886,\n",
       " -0.017795318737626076,\n",
       " 0.016937073320150375,\n",
       " 0.018049882724881172,\n",
       " 0.024993618950247765,\n",
       " 0.006530504673719406,\n",
       " 0.02146291360259056,\n",
       " -0.03206995502114296,\n",
       " -0.011634333059191704,\n",
       " -0.017297375947237015,\n",
       " 0.027092158794403076,\n",
       " 0.02812473475933075,\n",
       " -0.02036667801439762,\n",
       " 0.01074582152068615,\n",
       " -0.01709657348692417,\n",
       " 0.054162222892045975,\n",
       " -0.0062823970802128315,\n",
       " 0.03755788132548332,\n",
       " 0.056971967220306396,\n",
       " 0.07731615751981735,\n",
       " 0.010212092660367489,\n",
       " -0.03143737465143204,\n",
       " 0.03754143416881561,\n",
       " 0.03063105046749115,\n",
       " -0.01368783600628376,\n",
       " -0.05307881906628609,\n",
       " 0.026753196492791176,\n",
       " 0.024700207635760307,\n",
       " 0.05476904287934303,\n",
       " 0.04409025236964226,\n",
       " -0.019602566957473755,\n",
       " -0.0004823363560717553,\n",
       " -0.004895069636404514,\n",
       " -0.051310744136571884,\n",
       " -0.04437493532896042,\n",
       " 0.027876773849129677,\n",
       " -0.02480408549308777,\n",
       " 0.0019202959956601262,\n",
       " -0.04626846686005592,\n",
       " -0.013993633911013603,\n",
       " -0.01897788792848587,\n",
       " -0.026705823838710785,\n",
       " 0.0058180238120257854,\n",
       " -0.0113034937530756,\n",
       " 0.08758387714624405,\n",
       " 0.00394068518653512,\n",
       " -0.0033953716047108173,\n",
       " -0.0331348218023777,\n",
       " 0.01613626442849636,\n",
       " -0.043106887489557266,\n",
       " -0.0009684531833045185,\n",
       " -0.05350640416145325,\n",
       " -0.005056821275502443,\n",
       " -0.051863450556993484,\n",
       " -0.014519351534545422,\n",
       " -0.01067021768540144,\n",
       " -0.03443582355976105,\n",
       " -0.005831509362906218,\n",
       " 0.018024245277047157,\n",
       " 0.006557364948093891,\n",
       " -0.014074109494686127,\n",
       " -0.03758717700839043,\n",
       " -0.02050052583217621,\n",
       " -0.021933527663350105,\n",
       " 0.06925444304943085,\n",
       " 0.02124224789440632,\n",
       " -0.028543470427393913,\n",
       " -0.047268208116292953,\n",
       " 0.04117568954825401,\n",
       " 0.0457388199865818,\n",
       " 0.015299448743462563,\n",
       " 0.008310873061418533,\n",
       " 0.040104933083057404,\n",
       " -0.06263329833745956,\n",
       " 0.045443423092365265,\n",
       " 0.019480181857943535,\n",
       " -0.04509958252310753,\n",
       " 0.018203940242528915,\n",
       " -0.07138270884752274,\n",
       " -0.0454002246260643,\n",
       " -0.05310218036174774,\n",
       " 0.05136115849018097,\n",
       " 0.031065737828612328,\n",
       " -0.010884389281272888,\n",
       " -0.032559771090745926,\n",
       " -0.03636858984827995,\n",
       " -0.05662795901298523,\n",
       " -0.034841518849134445,\n",
       " 0.016999930143356323,\n",
       " 0.07679368555545807,\n",
       " -0.0656316876411438,\n",
       " 0.0286847073584795,\n",
       " -0.01390442717820406,\n",
       " -0.03253457322716713,\n",
       " 0.012230779975652695,\n",
       " -0.019721049815416336,\n",
       " 0.051430489867925644,\n",
       " -0.03862669691443443,\n",
       " -0.002246963558718562,\n",
       " -0.02791476808488369,\n",
       " -0.02948705293238163,\n",
       " -0.10080983489751816,\n",
       " 0.03566829487681389,\n",
       " 0.056533172726631165,\n",
       " -0.012436606921255589,\n",
       " 0.05109773203730583,\n",
       " -0.03547404333949089,\n",
       " 0.018901415169239044,\n",
       " 0.028032077476382256,\n",
       " -0.050224028527736664,\n",
       " -0.021608373150229454,\n",
       " 0.0652097761631012,\n",
       " 0.011604443192481995,\n",
       " -0.025109510868787766,\n",
       " 0.02072960138320923,\n",
       " -0.044769298285245895,\n",
       " 0.036632537841796875,\n",
       " -0.0602056086063385,\n",
       " -0.016629839316010475,\n",
       " 0.009758558124303818,\n",
       " -0.02597189135849476,\n",
       " -0.023929674178361893,\n",
       " 0.004198614042252302,\n",
       " -0.025633083656430244,\n",
       " 0.019013511016964912,\n",
       " -0.009011853486299515,\n",
       " 0.022008351981639862,\n",
       " 0.015606921166181564,\n",
       " 0.039415158331394196,\n",
       " 0.02431730553507805,\n",
       " 0.061924368143081665,\n",
       " 0.08291836082935333,\n",
       " -0.006918170489370823,\n",
       " 0.0369175486266613,\n",
       " 0.03872121125459671,\n",
       " 0.06958460062742233,\n",
       " 0.0023794176522642374,\n",
       " -0.013714617118239403,\n",
       " 0.003080737544223666,\n",
       " 0.022596638649702072,\n",
       " -0.024996651336550713,\n",
       " -0.008254134096205235,\n",
       " 0.016808146610856056,\n",
       " 0.04162515327334404,\n",
       " 0.04854513332247734,\n",
       " -0.05565496161580086,\n",
       " -0.010984804481267929,\n",
       " 0.03089434839785099,\n",
       " 0.005804914515465498,\n",
       " -0.01765218749642372,\n",
       " -0.04222254455089569,\n",
       " 0.024782544001936913,\n",
       " -0.027678081765770912,\n",
       " 0.07594982534646988,\n",
       " -0.062359943985939026,\n",
       " -0.07159034162759781,\n",
       " -0.02225648984313011,\n",
       " 0.02197614684700966,\n",
       " -0.006872719619423151,\n",
       " -0.003083843272179365,\n",
       " 0.01335563138127327,\n",
       " 0.025826994329690933,\n",
       " 0.014139503240585327,\n",
       " -0.007529243361204863,\n",
       " 0.04754127934575081,\n",
       " -0.0007981730159372091,\n",
       " 0.04044274240732193,\n",
       " -0.036171235144138336,\n",
       " -0.011431781575083733,\n",
       " -0.04025207832455635,\n",
       " -0.04195975139737129,\n",
       " 4.491343133850023e-05,\n",
       " -0.10295630246400833,\n",
       " 0.013232145458459854,\n",
       " 0.013661672361195087,\n",
       " 0.025106990709900856,\n",
       " 0.02710075117647648,\n",
       " -0.004064613487571478,\n",
       " -0.0026762187480926514,\n",
       " -3.23489512084052e-05,\n",
       " 0.06277161836624146,\n",
       " 0.02493695542216301,\n",
       " -0.017247794196009636,\n",
       " 0.012506571598351002,\n",
       " 0.003805637825280428,\n",
       " 0.07999711483716965,\n",
       " 0.05430426076054573,\n",
       " -0.054491832852363586,\n",
       " -0.03247148171067238,\n",
       " 0.03448491543531418,\n",
       " 0.00820723082870245,\n",
       " 0.017559165135025978,\n",
       " 0.006126925814896822,\n",
       " -0.03053746558725834,\n",
       " 0.023231815546751022,\n",
       " -0.029848599806427956,\n",
       " -0.0008405717671848834,\n",
       " -0.019173722714185715,\n",
       " 0.04739207401871681,\n",
       " -0.00047168199671432376,\n",
       " -0.013493675738573074,\n",
       " -0.023704001680016518,\n",
       " -0.011546896770596504,\n",
       " 0.028871607035398483,\n",
       " 0.0019969185814261436,\n",
       " -0.019985545426607132,\n",
       " -0.004617222584784031,\n",
       " -0.013851851224899292,\n",
       " 0.02383478917181492,\n",
       " 0.017349693924188614,\n",
       " -0.01520017720758915,\n",
       " 0.027494078502058983,\n",
       " -0.004564590752124786,\n",
       " 0.028472093865275383,\n",
       " 0.0016657925443723798,\n",
       " -0.03139624744653702,\n",
       " -0.06004531681537628,\n",
       " -0.016431758180260658,\n",
       " 0.03755638003349304,\n",
       " -0.036638688296079636,\n",
       " 0.046341124922037125,\n",
       " 0.02090194635093212,\n",
       " -0.016283394768834114,\n",
       " 0.02922239899635315,\n",
       " 0.00734426686540246,\n",
       " -0.011027274653315544,\n",
       " 0.030327633023262024,\n",
       " -0.010001658461987972,\n",
       " -0.008295103907585144,\n",
       " -0.015259823761880398,\n",
       " -0.03109126351773739,\n",
       " 0.010047873482108116,\n",
       " -0.05617359280586243,\n",
       " -0.00041282051824964583,\n",
       " 0.04032354801893234,\n",
       " -0.0069301133044064045,\n",
       " 0.023175165057182312,\n",
       " -0.004817150067538023,\n",
       " -0.06394589692354202,\n",
       " 0.04048623517155647,\n",
       " -0.046831414103507996,\n",
       " -0.033163733780384064,\n",
       " 0.013278027065098286,\n",
       " -0.0056581683456897736,\n",
       " -0.03565814718604088,\n",
       " -0.028871634975075722,\n",
       " -0.04678725823760033,\n",
       " -0.03446149080991745,\n",
       " -0.04381663352251053,\n",
       " -0.032456714659929276,\n",
       " -0.0078496802598238,\n",
       " -0.009116844274103642,\n",
       " -0.03442933037877083,\n",
       " -0.05798034369945526,\n",
       " -0.056131117045879364,\n",
       " 0.008752575144171715,\n",
       " -0.02425035648047924,\n",
       " 0.008728820830583572,\n",
       " -0.0005604682373814285,\n",
       " 0.009988701902329922,\n",
       " -0.029381250962615013,\n",
       " -0.009870164096355438,\n",
       " 0.01210527028888464,\n",
       " 0.01754690706729889,\n",
       " -0.05143037810921669,\n",
       " -0.047221798449754715,\n",
       " 0.021786989644169807,\n",
       " 0.04978130757808685,\n",
       " -0.01417042687535286,\n",
       " -0.013098148629069328,\n",
       " -0.009116698056459427,\n",
       " -0.1061907485127449,\n",
       " -0.02226896770298481,\n",
       " 0.027014851570129395,\n",
       " 0.009052957408130169,\n",
       " 0.04177849739789963,\n",
       " 0.01838097535073757,\n",
       " 0.017713239416480064,\n",
       " 0.04207831248641014,\n",
       " 0.10518250614404678,\n",
       " 0.04129589721560478,\n",
       " -0.03005489893257618,\n",
       " 0.004009135067462921,\n",
       " -0.023877279832959175,\n",
       " -0.017908260226249695,\n",
       " 0.01023078616708517,\n",
       " 0.08694757521152496,\n",
       " 0.03019414097070694,\n",
       " 0.04776878282427788,\n",
       " 0.025751234963536263,\n",
       " 0.03704534098505974,\n",
       " -0.0247564148157835,\n",
       " 0.01457596942782402,\n",
       " 0.03984338045120239,\n",
       " -0.0036186473444104195,\n",
       " 0.033480554819107056,\n",
       " 0.0017967806197702885,\n",
       " 0.03293095901608467,\n",
       " -0.02376633882522583,\n",
       " 0.017445996403694153,\n",
       " -0.08788251131772995,\n",
       " 0.07609444111585617,\n",
       " -0.007915705442428589,\n",
       " 0.0027614650316536427,\n",
       " 0.04438920319080353,\n",
       " 0.01933509297668934,\n",
       " -0.02528340183198452,\n",
       " -0.031250372529029846,\n",
       " 0.013934828341007233,\n",
       " -0.0077894339337944984,\n",
       " -0.006008763797581196,\n",
       " -0.000814389786683023,\n",
       " -0.022204240784049034,\n",
       " 0.06844497472047806,\n",
       " -0.04365224391222,\n",
       " -0.019504990428686142,\n",
       " -0.09665162861347198,\n",
       " 0.02803467959165573,\n",
       " 0.0016489340923726559,\n",
       " -0.007819104008376598,\n",
       " -0.07077241688966751,\n",
       " 0.020742004737257957,\n",
       " 0.011561347171664238,\n",
       " -0.021499186754226685,\n",
       " -0.06070588156580925,\n",
       " -0.035181157290935516,\n",
       " -0.08272681385278702,\n",
       " -0.015411541797220707,\n",
       " 0.015806563198566437,\n",
       " 0.054519232362508774,\n",
       " -0.028415147215127945,\n",
       " -0.010602754540741444,\n",
       " -0.0029990694019943476,\n",
       " -0.0036924099549651146,\n",
       " -0.0159878209233284,\n",
       " 0.031881269067525864,\n",
       " -0.03374293074011803,\n",
       " 0.01197139173746109,\n",
       " -0.020828811451792717,\n",
       " -0.002017598832026124,\n",
       " 0.0021717252675443888,\n",
       " 0.0002518410619813949,\n",
       " 0.045559484511613846,\n",
       " 0.05664864927530289,\n",
       " -0.0781809389591217,\n",
       " 0.06149156019091606,\n",
       " -0.04243936389684677,\n",
       " 0.0040147388353943825,\n",
       " 0.08679904043674469,\n",
       " -0.06509904563426971,\n",
       " -0.05667446553707123,\n",
       " 0.04584374278783798,\n",
       " 0.03225187212228775,\n",
       " -0.05440117046236992,\n",
       " -0.04572310298681259,\n",
       " -0.007475303020328283,\n",
       " 0.016410795971751213,\n",
       " -0.013430778868496418,\n",
       " -0.04257925972342491,\n",
       " 0.03381180390715599,\n",
       " -0.008089020848274231,\n",
       " 0.039417706429958344,\n",
       " -0.014071368612349033,\n",
       " 0.023435475304722786,\n",
       " -0.006219543516635895,\n",
       " -0.017203256487846375,\n",
       " 0.019694846123456955,\n",
       " -0.0030682410579174757,\n",
       " -0.02125057391822338,\n",
       " 0.0007904231897555292,\n",
       " 0.023845773190259933,\n",
       " -0.024877287447452545,\n",
       " -0.036229025572538376,\n",
       " 1.3017697710893117e-05,\n",
       " -0.06102645769715309,\n",
       " 0.010889122262597084,\n",
       " -0.0174447949975729,\n",
       " -0.010718765668570995,\n",
       " -0.010417488403618336,\n",
       " -0.054901137948036194,\n",
       " 0.005768142640590668,\n",
       " -0.017388582229614258,\n",
       " 0.048326604068279266,\n",
       " -0.03196215629577637,\n",
       " 0.07633776217699051,\n",
       " -0.008662917651236057,\n",
       " -0.08235528320074081,\n",
       " -0.047182418406009674,\n",
       " -0.01779949851334095,\n",
       " -0.01783333718776703,\n",
       " -0.02877318672835827,\n",
       " 6.439727440010756e-05,\n",
       " -0.014210309833288193,\n",
       " 0.04320216923952103,\n",
       " -0.004124703351408243,\n",
       " -0.024249956011772156,\n",
       " -0.018355824053287506,\n",
       " -0.005691887810826302,\n",
       " -0.0347871407866478,\n",
       " 0.023131433874368668,\n",
       " -0.0007221335545182228,\n",
       " -0.01972903683781624,\n",
       " -0.03239121288061142,\n",
       " -0.030246932059526443,\n",
       " 0.05947292968630791,\n",
       " 0.004716769326478243,\n",
       " 0.029379718005657196,\n",
       " -0.025380821898579597,\n",
       " 0.060541652143001556,\n",
       " -0.04310593008995056,\n",
       " 0.013839173130691051,\n",
       " 0.029538556933403015,\n",
       " 0.08868880569934845,\n",
       " -0.0013839727034792304,\n",
       " -0.011490393429994583,\n",
       " -0.0430334247648716,\n",
       " 0.022930443286895752,\n",
       " -0.0211623702198267,\n",
       " -0.008727744221687317,\n",
       " -0.01272760983556509,\n",
       " 0.027295907959342003,\n",
       " -0.028823258355259895,\n",
       " -0.017026003450155258,\n",
       " -0.027300603687763214,\n",
       " 0.02615544945001602,\n",
       " -0.01608341932296753,\n",
       " -0.03330333158373833,\n",
       " -0.006213412154465914,\n",
       " -0.02642321214079857,\n",
       " 0.014516117982566357,\n",
       " -0.026055200025439262,\n",
       " 0.015318660996854305,\n",
       " -0.015223992988467216,\n",
       " -0.017468562349677086,\n",
       " 0.006104207132011652,\n",
       " -0.001997239189222455,\n",
       " -0.04026159271597862,\n",
       " 0.036403119564056396,\n",
       " 0.0020701594185084105,\n",
       " -0.03895358741283417,\n",
       " -0.012160279788076878,\n",
       " 0.01986505277454853,\n",
       " -0.02729516290128231,\n",
       " -0.0132975522428751,\n",
       " 0.012888750992715359,\n",
       " 0.12645286321640015,\n",
       " -0.0015690559521317482,\n",
       " -0.039556462317705154,\n",
       " -0.0701623484492302,\n",
       " -0.0571473054587841,\n",
       " -0.040834248065948486,\n",
       " 0.022242475301027298,\n",
       " -0.051094427704811096,\n",
       " 0.02682587504386902,\n",
       " 0.08927874267101288,\n",
       " 0.039799679070711136,\n",
       " 0.01744972914457321,\n",
       " 0.01793256215751171,\n",
       " 0.02476155385375023,\n",
       " 0.017656149342656136,\n",
       " -0.020407015457749367,\n",
       " -0.06587980687618256,\n",
       " -0.006095204968005419,\n",
       " 0.05539005249738693,\n",
       " -6.52960883664078e-33,\n",
       " 0.012196708470582962,\n",
       " 0.025357646867632866,\n",
       " -0.037024322897195816,\n",
       " -0.016118913888931274,\n",
       " -0.004072345327585936,\n",
       " 0.01026636641472578,\n",
       " -0.023511379957199097,\n",
       " 0.04963742941617966,\n",
       " 0.005666974000632763,\n",
       " -0.025910193100571632,\n",
       " -0.04176877811551094,\n",
       " -0.01823173277080059,\n",
       " 0.03432226926088333,\n",
       " 0.03542330488562584,\n",
       " -0.04396079480648041,\n",
       " 0.03661210834980011,\n",
       " 0.036331549286842346,\n",
       " 0.07112377136945724,\n",
       " 0.023342687636613846,\n",
       " 0.033596210181713104,\n",
       " 0.04798568785190582,\n",
       " 0.025548871606588364,\n",
       " -0.019754160195589066,\n",
       " -0.06020934134721756,\n",
       " -0.02408565953373909,\n",
       " 0.0041610682383179665,\n",
       " 0.013730100356042385,\n",
       " 0.024510877206921577,\n",
       " 0.0338776633143425,\n",
       " -0.02277565374970436,\n",
       " -0.044274620711803436,\n",
       " -0.030172016471624374,\n",
       " -0.02230307087302208,\n",
       " 0.056024402379989624,\n",
       " -0.015735821798443794,\n",
       " -0.020843343809247017,\n",
       " -0.027560023590922356,\n",
       " 0.04560081660747528,\n",
       " -0.04691632464528084,\n",
       " 0.06841391324996948,\n",
       " 0.073341503739357,\n",
       " -0.02506958693265915,\n",
       " 0.026702651754021645,\n",
       " -0.03227268159389496,\n",
       " -0.06998848915100098,\n",
       " 0.020598361268639565,\n",
       " 0.006864752154797316,\n",
       " -0.033796463161706924,\n",
       " -0.014443150721490383,\n",
       " -0.03611617162823677,\n",
       " 0.03827323764562607,\n",
       " -0.013217946514487267,\n",
       " 0.0029352831188589334,\n",
       " 0.03836195915937424,\n",
       " -0.027459727600216866,\n",
       " -0.07128031551837921,\n",
       " -0.023510528728365898,\n",
       " -0.02573333866894245,\n",
       " -0.002341639017686248,\n",
       " 0.0028954721055924892,\n",
       " -0.07826615124940872,\n",
       " 0.031551506370306015,\n",
       " 0.04295298829674721,\n",
       " 0.04889407753944397,\n",
       " 0.009544881992042065,\n",
       " -0.02561645209789276,\n",
       " -0.00404927134513855,\n",
       " -0.010629227384924889,\n",
       " -0.03852296248078346,\n",
       " 0.040297165513038635,\n",
       " 0.0511961467564106,\n",
       " 0.02892516925930977,\n",
       " -0.021369941532611847,\n",
       " 0.06786970794200897,\n",
       " -0.004426316823810339,\n",
       " -0.06108444556593895,\n",
       " -0.008322766050696373,\n",
       " 0.04912474751472473,\n",
       " 0.021406766027212143,\n",
       " 0.016953857615590096,\n",
       " -0.003961197566241026,\n",
       " -0.06933477520942688,\n",
       " -0.022712018340826035,\n",
       " 0.011803331784904003,\n",
       " 0.0002983788726851344,\n",
       " 0.06130703166127205,\n",
       " 0.002181227318942547,\n",
       " -0.04607241973280907,\n",
       " -0.004392873030155897,\n",
       " -0.05838572606444359,\n",
       " -0.011621412821114063,\n",
       " 0.023306038230657578,\n",
       " -0.005426256917417049,\n",
       " 0.019822020083665848,\n",
       " -0.00025708723114803433,\n",
       " -0.1357174813747406,\n",
       " 0.00418652780354023,\n",
       " 0.026437746360898018,\n",
       " -0.002684971783310175,\n",
       " -0.012221429497003555,\n",
       " 0.023949740454554558,\n",
       " -0.036194343119859695,\n",
       " 0.015675952658057213,\n",
       " 0.0315222330391407,\n",
       " -0.033918172121047974,\n",
       " 0.008594177663326263,\n",
       " 0.004912741482257843,\n",
       " -0.005707221105694771,\n",
       " -0.016810405999422073,\n",
       " 0.012673145160079002,\n",
       " -0.006614817772060633,\n",
       " -0.019276749342679977,\n",
       " 0.023794513195753098,\n",
       " -0.014176256023347378,\n",
       " -0.04598354175686836,\n",
       " -0.0419907346367836,\n",
       " -0.003979372791945934,\n",
       " -0.03614138066768646,\n",
       " 0.015095173381268978,\n",
       " 0.08105473220348358,\n",
       " 0.028035670518875122,\n",
       " -0.05444905906915665,\n",
       " -0.014548063278198242,\n",
       " 0.019071277230978012,\n",
       " -0.028502093628048897,\n",
       " 0.03631696477532387,\n",
       " -0.08963754773139954,\n",
       " 0.03556233271956444,\n",
       " -0.030055813491344452,\n",
       " -0.03329815715551376,\n",
       " 0.030944688245654106,\n",
       " 0.036421820521354675,\n",
       " 3.176683094352484e-07,\n",
       " -0.01923266239464283,\n",
       " -0.024419032037258148,\n",
       " -0.025053424760699272,\n",
       " -0.08783084154129028,\n",
       " -0.008013717830181122,\n",
       " -0.012223420664668083,\n",
       " -0.0036601326428353786,\n",
       " -0.04090011864900589,\n",
       " 0.04577052593231201,\n",
       " -0.03798847645521164,\n",
       " 0.018191007897257805,\n",
       " 0.011787363328039646,\n",
       " 0.01015137042850256,\n",
       " -0.004281745292246342,\n",
       " -0.010642023757100105,\n",
       " -0.07272706180810928,\n",
       " 0.07162592560052872,\n",
       " 0.028761306777596474,\n",
       " -0.03120860457420349,\n",
       " 0.03552662953734398,\n",
       " 0.10166393220424652,\n",
       " 0.05590304732322693,\n",
       " 0.010218506678938866,\n",
       " -0.014937954023480415,\n",
       " 0.057767514139413834,\n",
       " 0.0031749692279845476,\n",
       " 0.015166080556809902,\n",
       " -0.002021198160946369,\n",
       " -0.02283625490963459,\n",
       " -0.034584395587444305,\n",
       " -0.01234059315174818,\n",
       " 0.014462959952652454,\n",
       " 0.025246702134609222,\n",
       " -0.04319062456488609,\n",
       " 0.03259666636586189,\n",
       " -0.04066106677055359,\n",
       " 0.0010296083055436611,\n",
       " -0.02404968999326229,\n",
       " 0.02615455351769924,\n",
       " -0.001590572064742446,\n",
       " -0.04652415215969086,\n",
       " -0.0012650596909224987,\n",
       " 0.03347872570157051,\n",
       " 0.005657787900418043,\n",
       " -0.009054668247699738,\n",
       " 0.07372996211051941,\n",
       " -0.05785539373755455,\n",
       " 0.05240490660071373,\n",
       " -0.04893260449171066,\n",
       " -0.034324005246162415,\n",
       " -0.01637575402855873,\n",
       " 0.03255597874522209,\n",
       " 0.05588435009121895,\n",
       " 0.06470850855112076,\n",
       " 0.04023678973317146,\n",
       " 0.007252133451402187,\n",
       " -0.018329890444874763,\n",
       " -0.03241868317127228,\n",
       " 0.0072325835935771465,\n",
       " 0.017258983105421066,\n",
       " -0.02230168506503105,\n",
       " -0.04239501431584358,\n",
       " 0.004521536640822887,\n",
       " 0.04139617830514908,\n",
       " 0.008875813335180283,\n",
       " 0.031239792704582214,\n",
       " -0.0022560148499906063,\n",
       " 3.264791148091195e-34,\n",
       " -0.05566545948386192,\n",
       " 0.05203249678015709,\n",
       " 0.06307120621204376,\n",
       " -0.002374336589127779,\n",
       " 0.019488010555505753,\n",
       " -0.00422488572075963,\n",
       " 0.03912153095006943,\n",
       " -0.004065310582518578,\n",
       " 0.016840916126966476,\n",
       " -0.06950867921113968,\n",
       " 0.021789096295833588]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['Vector'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Store sentences & embeddings on disc\n",
    "def save_embeddings(filepath, embeddings):\n",
    "    with open(filepath, \"wb\") as fOut:\n",
    "        pickle.dump({ 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "def load_embeddings(filepath):\n",
    "    with open(filepath, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    return stored_embeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(model, name):\n",
    "    with open(f\"{MODEL_PATH}/{name}\",'wb') as f:\n",
    "        pickle.dump( model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "def custom_cv(model, df, n_folds=5,sent =False):\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    user_label_df =df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    \n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "    #print(labels)\n",
    "    #print(users.shape,labels.shape)\n",
    "    \n",
    "    f1_scores = []\n",
    "    for train_index, test_index in skf.split(users, labels):\n",
    "        train_users = [users[f] for f in train_index]\n",
    "        test_users = [users[f] for f in test_index]\n",
    "\n",
    "        train_folds = df[df['User'].isin(train_users)].copy()\n",
    "        test_folds = df[df['User'].isin(test_users)].copy()\n",
    "\n",
    "        X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "        #X_train = train_folds['Vector']\n",
    "        y_train = train_folds['Label']\n",
    "        X_test = pd.DataFrame(test_folds['Vector'].values.tolist(), index = test_folds.index)\n",
    "        #X_test = test_folds['Vector']\n",
    "        \n",
    "        y_test = test_folds['Label']\n",
    "        if sent:\n",
    "            X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "            X_test = np.c_[X_test,test_folds['polarity'],test_folds['subjectivity'],test_folds['negativity'],test_folds['positivity'],test_folds['neutrality'], test_folds['compound']] \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        f1_scores.append(f1_score(y_test,model.predict(X_test)))\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "models = {\n",
    "        'sgdLR':SGDClassifier(random_state=seed,loss='log'),\n",
    "        #'NB':MultinomialNB(),\\\n",
    "        'sgdlSVM':SGDClassifier(random_state=seed,loss='hinge'),\n",
    "        'ExtraTrees':ExtraTreesClassifier(random_state=seed,n_jobs=-1),\\\n",
    "        'Perceptron':Perceptron(random_state=seed)}\n",
    "if BASELINE_COMP:\n",
    "        report=\"\"\n",
    "        best_model_name = \"\"\n",
    "        best_model=None\n",
    "        best_f1=0\n",
    "        for model_name, model in models.items():\n",
    "                res = custom_cv(model,full_df)\n",
    "                if np.mean(res) > best_f1:\n",
    "                        best_f1=np.mean(res)\n",
    "                        best_model_name=model_name\n",
    "                        best_model=model\n",
    "                #train_df[train_df['User'].isin(flds[0][0])].describe()\n",
    "                report+=f\"{model_name} f1: {round(np.mean(res),3)}\\n\"\n",
    "                print(f\"{model_name} f1: {round(np.mean(res),3)}\")\n",
    "        with open(f\"{MODEL_PATH}/baseline_report_{LM_MODEL}.txt\",'w') as f:\n",
    "                f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BASELINE_COMP:\n",
    "        res = custom_cv(best_model,full_df, sent=True)\n",
    "        print(f\"{best_model_name} f1: {round(np.mean(res),3)}\")\n",
    "\n",
    "        with open(f\"{MODEL_PATH}/baseline_report_{LM_MODEL}.txt\",'a') as f:\n",
    "                f.write(f\"\\nBest model with sent:\\n{best_model_name} f1: {round(np.mean(res),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best performing model was sgd Logistic Regression for window size 10, using MPNet without SA features, having achieved a 0.667 F1 score.\n",
    "#### But we didn't consider to be worth taking so much extra time training with MPNet features when MiniLm achieved very similar performance with nearly half the amount of features (F1=0.663 sgdLR, no SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "def train_eval_tuning(trial,params, df, sent=False):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    user_label_df =df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    \n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "\n",
    "    \n",
    "    f1_scores = []\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "        train_users = [users[f] for f in train_index]\n",
    "        test_users = [users[f] for f in test_index]\n",
    "\n",
    "        train_folds = df[df['User'].isin(train_users)]\n",
    "        X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "\n",
    "        test_folds = df[df['User'].isin(test_users)]\n",
    "        X_test = pd.DataFrame(test_folds['Vector'].values.tolist(), index = test_folds.index)\n",
    "\n",
    "        model = SGDClassifier(**params)\n",
    "        if sent:\n",
    "            #scaler = MinMaxScaler()\n",
    "            #train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.fit_transform(train_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            #test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']] = scaler.transform(test_folds[['polarity','subjectivity','negativity','positivity','neutrality','compound']])\n",
    "            X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "            X_test = np.c_[X_test,test_folds['polarity'],test_folds['subjectivity'],test_folds['negativity'],test_folds['positivity'],test_folds['neutrality'], test_folds['compound']] \n",
    "            \n",
    "\n",
    "        model.fit(X_train, train_folds['Label'])\n",
    "        f1_scores.append(f1_score(test_folds['Label'],model.predict(X_test)))\n",
    "    \n",
    "        trial.report(np.mean(f1_scores), fold)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_objective(trial):\n",
    "    parameters = {\n",
    "        'max_iter':trial.suggest_int('max_iter',1000,2500,step=500),\n",
    "        'loss':trial.suggest_categorical('loss',['log']),\n",
    "        'penalty':trial.suggest_categorical('penalty',['l2','l1','elasticnet']),\n",
    "        'alpha': trial.suggest_float('alpha',0.00001,0.1,log=True),\n",
    "        'random_state':trial.suggest_int('random_state',seed,seed)\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    \n",
    "    avg_f1 = train_eval_tuning(trial,parameters,full_df, sent=False)\n",
    "    return np.mean(avg_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-14 09:42:35,769]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-07-14 09:46:43,832]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-07-14 09:50:13,205]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-07-14 10:24:38,583]\u001b[0m Trial 48 finished with value: 0.6655177122889699 and parameters: {'max_iter': 2000, 'loss': 'log', 'penalty': 'l1', 'alpha': 8.093351246200499e-05, 'random_state': 23}. Best is trial 7 with value: 0.6699096264182774.\u001b[0m\n",
      "\u001b[32m[I 2022-07-14 10:28:07,488]\u001b[0m Trial 49 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#study = optuna.create_study(\n",
    "#        study_name=f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}\",\n",
    "#        direction='maximize')\n",
    "study.optimize(tuning_objective, n_trials=5, timeout=(60*60*12))\n",
    "joblib.dump(study,f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "study = joblib.load(f\"t{TASK}_tuning_{ROLLING_WINDOW_SIZE}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_iter': 2500,\n",
       "  'loss': 'log',\n",
       "  'penalty': 'l1',\n",
       "  'alpha': 0.00011514817252370237,\n",
       "  'random_state': 23},\n",
       " 0.6699096264182774)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{MODEL_PATH}/baseline_report_{LM_MODEL}.txt\",'a') as f:\n",
    "                f.write(f\"\\nOptimized model f1: {round(study.best_value,3)}\\nparams: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdLR_params = study.best_trial.params\n",
    "\n",
    "final_model = SGDClassifier(**sgdLR_params)\n",
    "full_train = pd.DataFrame(full_df['Vector'].values.tolist(), index = full_df.index)\n",
    "\n",
    "final_model.fit(full_train, full_df['Label'])\n",
    "save_model(final_model, \"optimized_sgdLR.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
