{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = \"train_df.csv\"\n",
    "\n",
    "TEST_DF = \"test_df.csv\"\n",
    "BOW_MODELS = \"Models/BoW/\" \n",
    "WE_MODELS = \"Models/WE/\" \n",
    "WORD_EMBEDDINGS = \"../word_embeddings/\"\n",
    "PRE_METHOD='Raw' #'Lemmatized' for bow and we, 'Raw' for lm\n",
    "#WE_MODEL=\"GLOVE_CC\"\n",
    "LM_MODEL='all-mpnet-base-v2'#'all-MiniLM-L6-v2'\n",
    "FEATURE_TYPE=\"NN\" #\"BoW\" #\"LM\" #\"WE\"\n",
    "ROLLING_WINDOW_SIZE=10\n",
    "SENT=False\n",
    "ROUND_MAX=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlm=SentenceTransformer(LM_MODEL)\\nlm.max_seq_length=512 \\nunused_embeddings = lm.encode(unused_df[\\'Text\\'].tolist(),show_progress_bar=True,            output_value=\\'sentence_embedding\\', batch_size=64,convert_to_numpy=True)\\nsave_embeddings(\"unused_embeddings.pkl\", unused_embeddings)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utilities import rolling_window, sa_features, save_embeddings\n",
    "import optuna\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import joblib\n",
    "seed=23\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DF, sep='\\t')\n",
    "test_df = pd.read_csv(TEST_DF, sep='\\t')\n",
    "#train_df = rolling_window(train_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "#test_df = rolling_window(test_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "full_df = pd.concat([train_df,test_df])\n",
    "###\n",
    "#full_df = rolling_window(full_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "###\n",
    "#tokens_df = pd.read_csv(\"./tokens.csv\", sep='\\t')\n",
    "#total_df = tokens_df.merge(full_df.drop_duplicates(), on=['User','Post_Nr','Raw','Stemmed','Lemmatized','Label' ], \n",
    "#                   how='left', indicator=True)\n",
    "#unused_df = total_df[total_df['_merge'] == 'left_only'].copy()\n",
    "#unused_df.drop(['_merge'], inplace=True, axis=1)\n",
    "full_df = rolling_window(full_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "#full_df = full_df.query(\"Window_id < 90\")\n",
    "\n",
    "\n",
    "#unused_df = rolling_window(unused_df,ROLLING_WINDOW_SIZE,1,PRE_METHOD)\n",
    "#unused_df = unused_df.query(\"Window_id < 90\")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "lm=SentenceTransformer(LM_MODEL)\n",
    "lm.max_seq_length=512 \n",
    "unused_embeddings = lm.encode(unused_df['Text'].tolist(),show_progress_bar=True,\\\n",
    "            output_value='sentence_embedding', batch_size=64,convert_to_numpy=True)\n",
    "save_embeddings(\"unused_embeddings.pkl\", unused_embeddings)\n",
    "\"\"\"\n",
    "\n",
    "#if SENT:\n",
    "#    full_df = sa_features(full_df)\n",
    "#    unused_df = sa_features(unused_df)\n",
    "#full_df.to_pickle(\"full_df_sent.pkl\")\n",
    "\n",
    "#full_df = pd.read_pickle(\"full_df_sent.pkl\")\n",
    "\n",
    "#full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Window_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>0</td>\n",
       "      <td>so many unwanted smith fadeaways. mid range j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>1</td>\n",
       "      <td>mid range jumpers hey guys, celtics fan here p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>2</td>\n",
       "      <td>well he got number tonight so maybe he will b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>3</td>\n",
       "      <td>i mean he will get pinch hits and an occasion...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_subject1345</td>\n",
       "      <td>4</td>\n",
       "      <td>yeah you are probably right. oh well.  i gues...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210922</th>\n",
       "      <td>subject9832</td>\n",
       "      <td>995</td>\n",
       "      <td>exactly scumbag christian girl..  i find myse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210923</th>\n",
       "      <td>subject9832</td>\n",
       "      <td>996</td>\n",
       "      <td>scumbag christian girl..  i find myself doing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210924</th>\n",
       "      <td>subject9832</td>\n",
       "      <td>997</td>\n",
       "      <td>i find myself doing this everytime i walk thou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210925</th>\n",
       "      <td>subject9832</td>\n",
       "      <td>998</td>\n",
       "      <td>...snow days keep your children from school.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210926</th>\n",
       "      <td>subject9832</td>\n",
       "      <td>999</td>\n",
       "      <td>why i do not donate to charity...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210927 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    User  Window_id  \\\n",
       "0       test_subject1345          0   \n",
       "1       test_subject1345          1   \n",
       "2       test_subject1345          2   \n",
       "3       test_subject1345          3   \n",
       "4       test_subject1345          4   \n",
       "...                  ...        ...   \n",
       "210922       subject9832        995   \n",
       "210923       subject9832        996   \n",
       "210924       subject9832        997   \n",
       "210925       subject9832        998   \n",
       "210926       subject9832        999   \n",
       "\n",
       "                                                     Text  Label  \n",
       "0        so many unwanted smith fadeaways. mid range j...      1  \n",
       "1       mid range jumpers hey guys, celtics fan here p...      1  \n",
       "2        well he got number tonight so maybe he will b...      1  \n",
       "3        i mean he will get pinch hits and an occasion...      1  \n",
       "4        yeah you are probably right. oh well.  i gues...      1  \n",
       "...                                                   ...    ...  \n",
       "210922   exactly scumbag christian girl..  i find myse...      0  \n",
       "210923  scumbag christian girl..  i find myself doing ...      0  \n",
       "210924  i find myself doing this everytime i walk thou...      0  \n",
       "210925   ...snow days keep your children from school.....      0  \n",
       "210926                 why i do not donate to charity...       0  \n",
       "\n",
       "[210927 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### BoW FEATURES\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom nltk.corpus import stopwords\\nnltk_stop_words = set(stopwords.words(\"english\"))\\nimport optuna\\nimport joblib\\nTFIDF_STUDY_PATH=\"./BagOfWords/t2_tfidfvec_10\"\\nMODEL_STUDY_PATH=\"./BagOfWords/t2_tuning_10.pkl\"\\ntfidf_study = joblib.load(TFIDF_STUDY_PATH)\\nmodel_study = joblib.load(MODEL_STUDY_PATH)\\ntfidf_params = tfidf_study.best_trial.params\\ntfidf_params[\\'stop_words\\'] = nltk_stop_words\\ntfidf_params[\\'ngram_range\\'] = (1,tfidf_params[\\'ngram_range\\'])\\nvectorizer = TfidfVectorizer(**tfidf_params)\\nmodel = MultinomialNB(**model_study.best_params)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "### BoW FEATURES\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "import optuna\n",
    "import joblib\n",
    "TFIDF_STUDY_PATH=\"./BagOfWords/t2_tfidfvec_10\"\n",
    "MODEL_STUDY_PATH=\"./BagOfWords/t2_tuning_10.pkl\"\n",
    "tfidf_study = joblib.load(TFIDF_STUDY_PATH)\n",
    "model_study = joblib.load(MODEL_STUDY_PATH)\n",
    "tfidf_params = tfidf_study.best_trial.params\n",
    "tfidf_params['stop_words'] = nltk_stop_words\n",
    "tfidf_params['ngram_range'] = (1,tfidf_params['ngram_range'])\n",
    "vectorizer = TfidfVectorizer(**tfidf_params)\n",
    "model = MultinomialNB(**model_study.best_params)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### WE FEATURES\\nfrom utilities import WordEmbeddingVectorizer\\nfrom sklearn.linear_model import SGDClassifier\\nimport optuna\\nimport joblib\\nSTUDY_PATH=\"./Embeddings/t2_tuning_10.pkl\"\\nstudy = joblib.load(STUDY_PATH)\\nmodel = SGDClassifier(**study.best_params)\\nwe_vectorizer = WordEmbeddingVectorizer(WE_MODEL, WORD_EMBEDDINGS)\\n#full_df = full_df.sample(frac=1, random_state=seed).reset_index(drop=True) \\nfull_df[\\'Vector\\'] = full_df[\\'Text\\'].apply(lambda text: we_vectorizer.vectorize(text))\\nunused_df[\\'Vector\\'] = unused_df[\\'Text\\'].apply(lambda text: we_vectorizer.vectorize(text))\\n\\nfull_df\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "### WE FEATURES\n",
    "from utilities import WordEmbeddingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import optuna\n",
    "import joblib\n",
    "STUDY_PATH=\"./Embeddings/t2_tuning_10.pkl\"\n",
    "study = joblib.load(STUDY_PATH)\n",
    "model = SGDClassifier(**study.best_params)\n",
    "we_vectorizer = WordEmbeddingVectorizer(WE_MODEL, WORD_EMBEDDINGS)\n",
    "#full_df = full_df.sample(frac=1, random_state=seed).reset_index(drop=True) \n",
    "full_df['Vector'] = full_df['Text'].apply(lambda text: we_vectorizer.vectorize(text))\n",
    "unused_df['Vector'] = unused_df['Text'].apply(lambda text: we_vectorizer.vectorize(text))\n",
    "\n",
    "full_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.linear_model import SGDClassifier\\nimport optuna\\nimport joblib\\nfrom utilities import load_embeddings\\nSTUDY_PATH = \"./LanguageModels/t2_tuning_10.pkl\"\\nstudy = joblib.load(STUDY_PATH)\\nstudy.best_params\\nmodel = SGDClassifier(**study.best_params)\\ntrain_sentence_embeddings = load_embeddings(f\"./LanguageModels/train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\\nval_sentence_embeddings = load_embeddings(f\"./LanguageModels/val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\\n#unused_embeddings = load_embeddings(f\"./unused_embeddings.pkl\")\\n\\nfull_sentence_embeddings = np.concatenate((train_sentence_embeddings,val_sentence_embeddings), axis=0)#full_df[\\'Vector\\'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\\nfull_df[\\'Vector\\'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\\nfull_df = full_df.query(\"Window_id < 90\")\\n\\nunused_df = pd.read_pickle(\"final_unused.pkl\")\\n\\n#unused_df = pd.read_pickle(\"final_unused.pkl\")\\n#full_df = pd.read_pickle(\"full_df_pkl.pkl\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LM FEATURES\n",
    "\"\"\"\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import optuna\n",
    "import joblib\n",
    "from utilities import load_embeddings\n",
    "STUDY_PATH = \"./LanguageModels/t2_tuning_10.pkl\"\n",
    "study = joblib.load(STUDY_PATH)\n",
    "study.best_params\n",
    "model = SGDClassifier(**study.best_params)\n",
    "train_sentence_embeddings = load_embeddings(f\"./LanguageModels/train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "val_sentence_embeddings = load_embeddings(f\"./LanguageModels/val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "#unused_embeddings = load_embeddings(f\"./unused_embeddings.pkl\")\n",
    "\n",
    "full_sentence_embeddings = np.concatenate((train_sentence_embeddings,val_sentence_embeddings), axis=0)#full_df['Vector'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\n",
    "full_df['Vector'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\n",
    "full_df = full_df.query(\"Window_id < 90\")\n",
    "\n",
    "unused_df = pd.read_pickle(\"final_unused.pkl\")\n",
    "\n",
    "#unused_df = pd.read_pickle(\"final_unused.pkl\")\n",
    "#full_df = pd.read_pickle(\"full_df_pkl.pkl\")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "### LM FEATURES NN\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torch\n",
    "import optuna\n",
    "import joblib\n",
    "from utilities import load_embeddings, LmNeuralNetwork, WritingWindowDataset\n",
    "STUDY_PATH = \"./LanguageModels/NN_hyperparameterstudy10.pkl\"\n",
    "study = joblib.load(STUDY_PATH)\n",
    "study.best_params\n",
    "params = optuna.trial.FixedTrial(study.best_trial.params)\n",
    "train_sentence_embeddings = load_embeddings(f\"./LanguageModels/train_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "val_sentence_embeddings = load_embeddings(f\"./LanguageModels/val_sentence_embeddings_{LM_MODEL}_{ROLLING_WINDOW_SIZE}.pkl\")\n",
    "#unused_embeddings = load_embeddings(f\"./unused_embeddings.pkl\")\n",
    "\n",
    "full_sentence_embeddings = np.concatenate((train_sentence_embeddings,val_sentence_embeddings), axis=0)#full_df['Vector'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\n",
    "full_df['Vector'] = pd.DataFrame(data=full_sentence_embeddings).values.tolist()\n",
    "unused_df = pd.read_pickle(\"final_unused.pkl\")\n",
    "\n",
    "train_ds = WritingWindowDataset(full_sentence_embeddings, full_df['Label'])\n",
    "#unused_df = pd.read_pickle(\"final_unused.pkl\")\n",
    "\n",
    "#full_df = pd.read_pickle(\"full_df_pkl.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "#from torchmetrics.functional import f1_score\n",
    "BATCH_SIZE = 32\n",
    "epochs=10\n",
    "\n",
    "def train_nn(trainloader, study):\n",
    "    params = optuna.trial.FixedTrial(study.best_trial.params)\n",
    "    model= LmNeuralNetwork(params)\n",
    "\n",
    "    model = model.cuda()\n",
    "    optimizer = AdamW(model.parameters(), lr=study.best_trial.params['learning_rate'])#, lr= learning_rate)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "            \n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        \n",
    "        total_loss_train = 0\n",
    "        train_preds = np.array([])\n",
    "        train_targets = np.array([])\n",
    "        model.train()\n",
    "        for train_input, train_label in tqdm(trainloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            train_label = train_label.float()\n",
    "            train_label = train_label.unsqueeze(1)\n",
    "            features = train_input.to(device)\n",
    "            \n",
    "\n",
    "            output = model(features)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += float(batch_loss.item())\n",
    "            \n",
    "            train_preds = np.concatenate((train_preds,torch.round(output).detach().cpu().numpy().flatten()),axis=0)\n",
    "            train_targets = np.concatenate((train_targets,train_label.detach().cpu().numpy().flatten()),axis=0)\n",
    "\n",
    "            \n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss=total_loss_train/len(train_preds)\n",
    "        train_f1 = f1_score(train_targets,train_preds)\n",
    "            \n",
    "        print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .4f} \\\n",
    "                | Train F1: {train_f1: .4f}')\n",
    "\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom sklearn.model_selection import StratifiedKFold\\n\\ndef cv_classification(full_df, model,filename):\\n    skf = StratifiedKFold(n_splits=5)\\n    user_label_df =full_df.drop_duplicates(\\'User\\')\\n    users = user_label_df[\\'User\\'].to_numpy()\\n    labels = user_label_df[\\'Label\\'].to_numpy()\\n    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\\n            train_users = [users[f] for f in train_index]\\n            test_users = [users[f] for f in test_index]\\n\\n            train_folds = full_df[full_df[\\'User\\'].isin(train_users)].copy()\\n            train_folds = train_folds.sample(frac=1, random_state=seed).reset_index(drop=True) \\n\\n            test_folds = full_df[full_df[\\'User\\'].isin(test_users)].copy()\\n            test_folds = pd.concat([test_folds,unused_df])\\n\\n            if FEATURE_TYPE==\"BoW\":\\n                X_train = vectorizer.fit_transform(train_folds[\\'Text\\'])\\n            else:\\n                X_train = pd.DataFrame(train_folds[\\'Vector\\'].values.tolist(), index = train_folds.index)\\n            y_train = train_folds[\\'Label\\']\\n            if SENT:\\n                X_train = np.c_[X_train,train_folds[\\'polarity\\'],train_folds[\\'subjectivity\\'],train_folds[\\'negativity\\'],train_folds[\\'positivity\\'],train_folds[\\'neutrality\\'], train_folds[\\'compound\\']] \\n\\n            model.fit(X_train, y_train)\\n            print(f\"model for fold {fold} trained\")\\n            true_label=test_folds[[\\'User\\',\\'Label\\']].groupby(\\'User\\').max().to_dict()[\\'Label\\']\\n            true_labels = {subject:true_label[subject] for subject in true_label}\\n            with open(f\"{filename}_{fold}.txt\", \"w\") as f:\\n                for user in true_labels:\\n                    f.write(f\"({user},{true_labels[user]})\")\\n                    for window_id in range(ROUND_MAX-ROLLING_WINDOW_SIZE+1):\\n                        test_f = test_folds.query(\"User==@user and Window_id==@window_id\")\\n                        if len(test_f)>0:\\n                            f.write(\" \")\\n                            if FEATURE_TYPE==\"BoW\":\\n                                X_test = vectorizer.transform(test_f[\\'Text\\'])\\n                                if SENT:\\n                                    X_test = np.r_[X_test,test_f[\\'polarity\\'],test_f[\\'subjectivity\\'],test_f[\\'negativity\\'],test_f[\\'positivity\\'],test_f[\\'neutrality\\'], test_f[\\'compound\\']] \\n                                X_test.reshape(1,-1)\\n                            else:\\n                                X_test = test_f[\\'Vector\\'].values.tolist()[0]\\n                                if SENT:\\n                                    X_test = np.r_[X_test,test_f[\\'polarity\\'],test_f[\\'subjectivity\\'],test_f[\\'negativity\\'],test_f[\\'positivity\\'],test_f[\\'neutrality\\'], test_f[\\'compound\\']] \\n                                X_test = [X_test] #added\\n\\n                            \\n                            pred = model.predict(X_test)[0]\\n                            prob = model.predict_proba(X_test)[0].tolist()[1]\\n                            f.write(f\"({pred},{prob})\")\\n                    f.write(\"\\n\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cv_classification(full_df, model,filename):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    user_label_df =full_df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "            train_users = [users[f] for f in train_index]\n",
    "            test_users = [users[f] for f in test_index]\n",
    "\n",
    "            train_folds = full_df[full_df['User'].isin(train_users)].copy()\n",
    "            train_folds = train_folds.sample(frac=1, random_state=seed).reset_index(drop=True) \n",
    "\n",
    "            test_folds = full_df[full_df['User'].isin(test_users)].copy()\n",
    "            test_folds = pd.concat([test_folds,unused_df])\n",
    "\n",
    "            if FEATURE_TYPE==\"BoW\":\n",
    "                X_train = vectorizer.fit_transform(train_folds['Text'])\n",
    "            else:\n",
    "                X_train = pd.DataFrame(train_folds['Vector'].values.tolist(), index = train_folds.index)\n",
    "            y_train = train_folds['Label']\n",
    "            if SENT:\n",
    "                X_train = np.c_[X_train,train_folds['polarity'],train_folds['subjectivity'],train_folds['negativity'],train_folds['positivity'],train_folds['neutrality'], train_folds['compound']] \n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            print(f\"model for fold {fold} trained\")\n",
    "            true_label=test_folds[['User','Label']].groupby('User').max().to_dict()['Label']\n",
    "            true_labels = {subject:true_label[subject] for subject in true_label}\n",
    "            with open(f\"{filename}_{fold}.txt\", \"w\") as f:\n",
    "                for user in true_labels:\n",
    "                    f.write(f\"({user},{true_labels[user]})\")\n",
    "                    for window_id in range(ROUND_MAX-ROLLING_WINDOW_SIZE+1):\n",
    "                        test_f = test_folds.query(\"User==@user and Window_id==@window_id\")\n",
    "                        if len(test_f)>0:\n",
    "                            f.write(\" \")\n",
    "                            if FEATURE_TYPE==\"BoW\":\n",
    "                                X_test = vectorizer.transform(test_f['Text'])\n",
    "                                if SENT:\n",
    "                                    X_test = np.r_[X_test,test_f['polarity'],test_f['subjectivity'],test_f['negativity'],test_f['positivity'],test_f['neutrality'], test_f['compound']] \n",
    "                                X_test.reshape(1,-1)\n",
    "                            else:\n",
    "                                X_test = test_f['Vector'].values.tolist()[0]\n",
    "                                if SENT:\n",
    "                                    X_test = np.r_[X_test,test_f['polarity'],test_f['subjectivity'],test_f['negativity'],test_f['positivity'],test_f['neutrality'], test_f['compound']] \n",
    "                                X_test = [X_test] #added\n",
    "\n",
    "                            \n",
    "                            pred = model.predict(X_test)[0]\n",
    "                            prob = model.predict_proba(X_test)[0].tolist()[1]\n",
    "                            f.write(f\"({pred},{prob})\")\n",
    "                    f.write(\"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c959f50430>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def nn_cv_classification(full_df, study,filename):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    user_label_df =full_df.drop_duplicates('User')\n",
    "    users = user_label_df['User'].to_numpy()\n",
    "    labels = user_label_df['Label'].to_numpy()\n",
    "    for fold,(train_index, test_index) in enumerate(skf.split(users, labels)):\n",
    "            train_users = [users[f] for f in train_index]\n",
    "            test_users = [users[f] for f in test_index]\n",
    "            train_ids = train_df.index[train_df['User'].isin(train_users)].to_list()\n",
    "            train_folds = full_df[full_df['User'].isin(train_users)].copy()\n",
    "            train_folds = train_folds.sample(frac=1, random_state=seed).reset_index(drop=True) \n",
    "\n",
    "            test_folds = full_df[full_df['User'].isin(test_users)].copy()\n",
    "            test_folds = pd.concat([test_folds,unused_df])\n",
    "            train_data = torch.utils.data.Subset(train_ds, train_ids)\n",
    "            \n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                            train_data, \n",
    "                            batch_size=32, \n",
    "                            shuffle=True)\n",
    "            model = train_nn(trainloader, study)\n",
    "            model.eval()\n",
    "            print(f\"model for fold {fold} trained\")\n",
    "            true_label=test_folds[['User','Label']].groupby('User').max().to_dict()['Label']\n",
    "            true_labels = {subject:true_label[subject] for subject in true_label}\n",
    "            with open(f\"{filename}_{fold}.txt\", \"w\") as f:\n",
    "                for user in true_labels:\n",
    "                    f.write(f\"({user},{true_labels[user]})\")\n",
    "                    for window_id in range(ROUND_MAX-ROLLING_WINDOW_SIZE+1):\n",
    "                        test_f = test_folds.query(\"User==@user and Window_id==@window_id\")\n",
    "                        if len(test_f)>0:\n",
    "                            f.write(\" \")\n",
    "                            \n",
    "                            X_test = test_f['Vector'].values.tolist()[0]\n",
    "                            with torch.no_grad():\n",
    "                                X_test = torch.tensor([X_test]).cuda() #added\n",
    "\n",
    "                                \n",
    "                                pred = model(X_test)[0]\n",
    "                                f.write(f\"({round(pred.item())},{pred.item()})\")\n",
    "                    f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:58<00:00, 71.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0212                 | Train F1:  0.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 286.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0204                 | Train F1:  0.3512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 286.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0187                 | Train F1:  0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 285.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0174                 | Train F1:  0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 288.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0168                 | Train F1:  0.6338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 289.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0165                 | Train F1:  0.6493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 281.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0163                 | Train F1:  0.6597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 287.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0162                 | Train F1:  0.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 286.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0160                 | Train F1:  0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4200/4200 [00:14<00:00, 284.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0159                 | Train F1:  0.6725\n",
      "model for fold 0 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:22<00:00, 183.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0210                 | Train F1:  0.1975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 287.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0197                 | Train F1:  0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 287.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0190                 | Train F1:  0.5389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 288.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0185                 | Train F1:  0.5629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 288.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0181                 | Train F1:  0.5858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 286.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0178                 | Train F1:  0.6004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 286.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0176                 | Train F1:  0.6097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 287.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0175                 | Train F1:  0.6123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 285.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0173                 | Train F1:  0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4129/4129 [00:14<00:00, 286.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0172                 | Train F1:  0.6213\n",
      "model for fold 1 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:16<00:00, 253.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0214                 | Train F1:  0.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 287.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0197                 | Train F1:  0.4878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 289.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0185                 | Train F1:  0.6138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 287.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0179                 | Train F1:  0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 287.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0176                 | Train F1:  0.6459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 287.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0173                 | Train F1:  0.6488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 290.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0169                 | Train F1:  0.6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 287.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0166                 | Train F1:  0.6552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 286.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0165                 | Train F1:  0.6654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4097/4097 [00:14<00:00, 287.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0164                 | Train F1:  0.7010\n",
      "model for fold 2 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:30<00:00, 126.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0222                 | Train F1:  0.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 288.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0208                 | Train F1:  0.4290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 288.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0192                 | Train F1:  0.5153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 288.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0180                 | Train F1:  0.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 288.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0173                 | Train F1:  0.6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 288.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0168                 | Train F1:  0.6459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 288.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0165                 | Train F1:  0.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 289.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0162                 | Train F1:  0.6742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 287.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0160                 | Train F1:  0.6849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3904/3904 [00:13<00:00, 288.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0158                 | Train F1:  0.6890\n",
      "model for fold 3 trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:23<00:00, 230.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.0206                 | Train F1:  0.3359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 276.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0186                 | Train F1:  0.5510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 274.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0177                 | Train F1:  0.6123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 274.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0173                 | Train F1:  0.6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 274.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0170                 | Train F1:  0.6495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 273.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0168                 | Train F1:  0.6528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 276.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0167                 | Train F1:  0.6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 274.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0166                 | Train F1:  0.6546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 273.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0164                 | Train F1:  0.6589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5443/5443 [00:19<00:00, 276.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0164                 | Train F1:  0.6574\n",
      "model for fold 4 trained\n"
     ]
    }
   ],
   "source": [
    "nn_cv_classification(full_df,study, \"NN\")\n",
    "#cv_classification(full_df,model, FEATURE_TYPE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
