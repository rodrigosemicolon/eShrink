{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import joblib\n",
    "import ast\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "from sklearn.metrics import f1_score, classification_report, precision_score, ndcg_score\n",
    "def criterion_evaluation(path, max_round, criterion, thresholds):\n",
    "    f1= 0\n",
    "    with open(f\"{path}/decisions.txt\", \"r\") as f:\n",
    "        true_labels = []\n",
    "        classifications = []\n",
    "        ranking_score=[]\n",
    "        for line in f:\n",
    "            splt = line[:-1].split(\" \")\n",
    "            splt = [ast.literal_eval(t) for t in splt]\n",
    "            user, label = splt[0]\n",
    "            true_labels.append(label)\n",
    "            classification = 0\n",
    "            confidence_score=0\n",
    "            count = 0\n",
    "            for i, (pred, conf) in enumerate(splt[1+9:]):\n",
    "                if i>=max_round:\n",
    "                        break\n",
    "                if criterion == \"confidence\":#  and i%10==0:\n",
    "                    if pred==1 and conf >= thresholds['confidence_threshold'] :\n",
    "                        classification= 1\n",
    "                        confidence_score+=conf\n",
    "                if criterion == \"ratio\" and conf >=thresholds['confidence_threshold']:# and i%10==0:\n",
    "                    count = count + pred\n",
    "                    if count/(10 + i) >= thresholds['ratio_threshold']:\n",
    "                        classification=1\n",
    "                        confidence_score+=conf\n",
    "\n",
    "                if criterion == \"confidence+\" and conf >=thresholds['confidence_threshold']:# and i%10==0:\n",
    "                    count = count + pred\n",
    "                    if count >= thresholds['amount_threshold']:\n",
    "                        classification=1\n",
    "                        confidence_score+=conf\n",
    "\n",
    "                if criterion == \"consecutive\":# and i%10==0: ##added stride\n",
    "                    if pred==1 and conf >=thresholds['confidence_threshold']:\n",
    "                        count = count + 1\n",
    "\n",
    "                    else:\n",
    "                        count = 0\n",
    "                    if count >= thresholds['consecutive_threshold']:\n",
    "                        classification = 1\n",
    "                        confidence_score+=conf\n",
    "\n",
    "            ranking_score.append((confidence_score, label))\n",
    "            classifications.append(classification)\n",
    "    f1=f1_score(true_labels,classifications)\n",
    "    cls_rep = classification_report(true_labels,classifications, digits=3, output_dict=True)['1']\n",
    "    ranking_score = sorted(ranking_score, key=lambda tup:tup[0], reverse=True)\n",
    "    true_ranking_labels = [i[1] for i in ranking_score]\n",
    "    ranking_scores = [i[0] for i in ranking_score]\n",
    "    #print(true_ranking_labels[:10], [1 for i in ranking_scores[:10] if i>0])\n",
    "    p_at_10 = precision_score(true_ranking_labels[:10], [1 for i in ranking_scores[:10] if i>0])\n",
    "\n",
    "    \n",
    "    ndcg_at_10 = ndcg_score([true_ranking_labels[:10]], [ranking_scores[:10]])\n",
    "    \n",
    "    \n",
    "    ndcg_at_100 = ndcg_score([true_ranking_labels[:100]], [ranking_scores[:100]])\n",
    "\n",
    "    #print(ranking_score)\n",
    "    stats = (cls_rep,f1,p_at_10,ndcg_at_10,ndcg_at_100)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_criterion_evaluation(path1,path2,path3, max_round, criterion, thresholds):\n",
    "    f1= 0\n",
    "    with open(f\"{path1}/decisions.txt\", \"r\") as file1:\n",
    "        with open(f\"{path2}/decisions.txt\", \"r\") as file2:\n",
    "            with open(f\"{path3}/decisions.txt\", \"r\") as file3:\n",
    "\n",
    "                true_labels = {}\n",
    "                classifications1 = {}\n",
    "                confidence_scores1={}\n",
    "                classifications2 = {}\n",
    "                confidence_scores2={}\n",
    "                classifications3 = {}\n",
    "                confidence_scores3={}\n",
    "\n",
    "\n",
    "                for line in file1:\n",
    "                    splt = line[:-1].split(\" \")\n",
    "                    splt = [ast.literal_eval(t) for t in splt]\n",
    "                    user, label = splt[0]\n",
    "                    true_labels[user]=label\n",
    "                    classification = 0\n",
    "                    confidence_score=0\n",
    "                    count = 0\n",
    "                    for i, (pred, conf) in enumerate(splt[10:]):\n",
    "                        if i>=max_round:\n",
    "                                break\n",
    "                        if criterion == \"confidence\":#  and i%10==0:\n",
    "                            if pred==1 and conf >= thresholds[0]['confidence_threshold'] :\n",
    "                                classification= 1\n",
    "                                confidence_score+=conf\n",
    "                        if criterion == \"ratio\" and conf >=thresholds[0]['confidence_threshold']:# and i%10==0:\n",
    "                            count = count + pred\n",
    "                            if count/(10 + i) >= thresholds[0]['ratio_threshold']:\n",
    "                                classification=1\n",
    "                                confidence_score+=conf\n",
    "\n",
    "                        if criterion == \"confidence+\" and conf >=thresholds[0]['confidence_threshold']:# and i%10==0:\n",
    "                            count = count + pred\n",
    "                            if count >= thresholds[0]['amount_threshold']:\n",
    "                                classification=1\n",
    "                                confidence_score+=conf\n",
    "\n",
    "                        if criterion == \"consecutive\":# and i%10==0: ##added stride\n",
    "                            if pred==1 and conf >=thresholds[0]['confidence_threshold']:\n",
    "                                count = count + 1\n",
    "\n",
    "                            else:\n",
    "                                count = 0\n",
    "                            if count >= thresholds[0]['consecutive_threshold']:\n",
    "                                classification = 1\n",
    "                                confidence_score+=conf\n",
    "\n",
    "                        \n",
    "                    classifications1[user]=classification\n",
    "                    confidence_scores1[user]=confidence_score\n",
    "\n",
    "                    for line in file2:\n",
    "                        splt = line[:-1].split(\" \")\n",
    "                        splt = [ast.literal_eval(t) for t in splt]\n",
    "                        user, label = splt[0]\n",
    "                        true_labels[user]=label\n",
    "                        classification = 0\n",
    "                        confidence_score=0\n",
    "\n",
    "                        count = 0\n",
    "                        for i, (pred, conf) in enumerate(splt[10:]):\n",
    "                            if i>=max_round:\n",
    "                                    break\n",
    "                            if criterion == \"confidence\":#  and i%10==0:\n",
    "                                if pred==1 and conf >= thresholds[1]['confidence_threshold'] :\n",
    "                                    classification= 1\n",
    "                                    confidence_score+=conf\n",
    "                            if criterion == \"ratio\" and conf >=thresholds[1]['confidence_threshold']:# and i%10==0:\n",
    "                                count = count + pred\n",
    "                                if count/(10 + i) >= thresholds[1]['ratio_threshold']:\n",
    "                                    classification=1\n",
    "                                    confidence_score+=conf\n",
    "\n",
    "                            if criterion == \"confidence+\" and conf >=thresholds[1]['confidence_threshold']:# and i%10==0:\n",
    "                                count = count + pred\n",
    "                                if count >= thresholds[1]['amount_threshold']:\n",
    "                                    classification=1\n",
    "                                    confidence_score+=conf\n",
    "\n",
    "                            if criterion == \"consecutive\":# and i%10==0: ##added stride\n",
    "                                if pred==1 and conf >=thresholds[1]['confidence_threshold']:\n",
    "                                    count = count + 1\n",
    "\n",
    "                                else:\n",
    "                                    count = 0\n",
    "                                if count >= thresholds[1]['consecutive_threshold']:\n",
    "                                    classification = 1\n",
    "                                    confidence_score+=conf\n",
    "   \n",
    "                        classifications2[user]=classification\n",
    "                        confidence_scores2[user]=confidence_score\n",
    "\n",
    "                    for line in file3:\n",
    "                        splt = line[:-1].split(\" \")\n",
    "                        splt = [ast.literal_eval(t) for t in splt]\n",
    "                        user, label = splt[0]\n",
    "                        true_labels[user]=label\n",
    "                        classification = 0\n",
    "                        confidence_score=0\n",
    "                        count = 0\n",
    "                        for i, (pred, conf) in enumerate(splt[10:]):\n",
    "                            if i>=max_round:\n",
    "                                    break\n",
    "                            if criterion == \"confidence\":#  and i%10==0:\n",
    "                                if pred==1 and conf >= thresholds[2]['confidence_threshold'] :\n",
    "                                    classification= 1\n",
    "                                    confidence_score+=conf\n",
    "                            if criterion == \"ratio\" and conf >=thresholds[2]['confidence_threshold']:# and i%10==0:\n",
    "                                count = count + pred\n",
    "                                if count/(10 + i) >= thresholds[2]['ratio_threshold']:\n",
    "                                    classification=1\n",
    "                                    confidence_score+=conf\n",
    "\n",
    "                            if criterion == \"confidence+\" and conf >=thresholds[2]['confidence_threshold']:# and i%10==0:\n",
    "                                count = count + pred\n",
    "                                if count >= thresholds[2]['amount_threshold']:\n",
    "                                    classification=1\n",
    "                                    confidence_score+=conf\n",
    "\n",
    "                            if criterion == \"consecutive\":# and i%10==0: ##added stride\n",
    "                                if pred==1 and conf >=thresholds[2]['confidence_threshold']:\n",
    "                                    count = count + 1\n",
    "\n",
    "                                else:\n",
    "                                    count = 0\n",
    "                                if count >= thresholds[2]['consecutive_threshold']:\n",
    "                                    classification = 1\n",
    "                                    confidence_score+=conf\n",
    "    \n",
    "                        classifications3[user]=classification\n",
    "                        confidence_scores3[user]=confidence_score\n",
    "    classifications = {}\n",
    "    ranking_scores=[] \n",
    "    for user in true_labels:\n",
    "        \n",
    "        if classifications1[user] + classifications2[user] + classifications3[user]>=2:\n",
    "            classifications[user]=1\n",
    "        else:\n",
    "            classifications[user]=0\n",
    "        ranking_scores.append((confidence_scores1[user] + confidence_scores2[user] + confidence_scores3[user],classifications[user],true_labels[user]))\n",
    "    f1=f1_score(list(true_labels.values()),list(classifications.values()))\n",
    "    cls_rep = classification_report(list(true_labels.values()),list(classifications.values()), digits=3, output_dict=True)['1']\n",
    "\n",
    "    ranking_scores = sorted(ranking_scores, key=lambda tup:tup[0], reverse=True)\n",
    "    true_ranking_labels = [i[2] for i in ranking_scores]\n",
    "    ranking_labels = [i[1] for i in ranking_scores]\n",
    "    ranking_scores = [i[0] for i in ranking_scores]\n",
    "    #print(true_ranking_labels[:10], [1 for i in ranking_scores[:10] if i>0])\n",
    "    p_at_10 = precision_score(true_ranking_labels[:10], [1 for i,_ in enumerate(ranking_scores[:10]) if ranking_labels[i]>0])\n",
    "\n",
    "    \n",
    "    ndcg_at_10 = ndcg_score([true_ranking_labels[:10]], [ranking_scores[:10]])\n",
    "    \n",
    "    \n",
    "    ndcg_at_100 = ndcg_score([true_ranking_labels[:100]], [ranking_scores[:100]])\n",
    "\n",
    "    #print(ranking_score)\n",
    "    stats = (cls_rep,f1,p_at_10,ndcg_at_10,ndcg_at_100)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_round=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading thresholds\n",
    "import joblib\n",
    "def get_thresholds(model):\n",
    "    study_conf=joblib.load(f\"../Thresholds2/{model}_confidence_100.pkl\")\n",
    "    study_confplus=joblib.load(f\"../Thresholds2/{model}_confidence+_100.pkl\")\n",
    "    study_ratio=joblib.load(f\"../Thresholds2/{model}_ratio_100.pkl\")\n",
    "    study_consec=joblib.load(f\"../Thresholds2/{model}_consecutive_100.pkl\")\n",
    "\n",
    "    conf_thresh = study_conf.best_trial.params\n",
    "    confplus_thresh = study_confplus.best_trial.params\n",
    "    ratio_thresh = study_ratio.best_trial.params\n",
    "    consec_thresh = study_consec.best_trial.params\n",
    "\n",
    "    return {'confidence':conf_thresh,'confidence+':confplus_thresh,'ratio':ratio_thresh,'consecutive':consec_thresh}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════════╤═════════════╤══════════╤══════════╕\n",
      "│ model    │ criterion   │   precision │   recall │       f1 │\n",
      "╞══════════╪═════════════╪═════════════╪══════════╪══════════╡\n",
      "│ BoW      │ confidence  │    0.637255 │ 0.663265 │ 0.65     │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ BoW      │ confidence+ │    0.686567 │ 0.469388 │ 0.557576 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ BoW      │ ratio       │    0.628571 │ 0.673469 │ 0.650246 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ BoW      │ consecutive │    0.675325 │ 0.530612 │ 0.594286 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ WE       │ confidence  │    0.191176 │ 0.397959 │ 0.258278 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ WE       │ confidence+ │    0.313559 │ 0.377551 │ 0.342593 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ WE       │ ratio       │    0.409091 │ 0.367347 │ 0.387097 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ WE       │ consecutive │    0.402299 │ 0.357143 │ 0.378378 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ LM       │ confidence  │    0.685185 │ 0.377551 │ 0.486842 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ LM       │ confidence+ │    0.650602 │ 0.55102  │ 0.596685 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ LM       │ ratio       │    0.588889 │ 0.540816 │ 0.56383  │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ LM       │ consecutive │    0.595238 │ 0.510204 │ 0.549451 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ NN       │ confidence  │    0.328358 │ 0.897959 │ 0.480874 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ NN       │ confidence+ │    0.657895 │ 0.510204 │ 0.574713 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ NN       │ ratio       │    0.55     │ 0.673469 │ 0.605505 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ NN       │ consecutive │    0.574468 │ 0.55102  │ 0.5625   │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ Ensemble │ confidence  │    0.625    │ 0.459184 │ 0.529412 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ Ensemble │ confidence+ │    0.656716 │ 0.44898  │ 0.533333 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ Ensemble │ ratio       │    0.621951 │ 0.520408 │ 0.566667 │\n",
      "├──────────┼─────────────┼─────────────┼──────────┼──────────┤\n",
      "│ Ensemble │ consecutive │    0.656716 │ 0.44898  │ 0.533333 │\n",
      "╘══════════╧═════════════╧═════════════╧══════════╧══════════╛\n",
      "╒══════════╤═════════════╤════════════════╤═══════════╤════════════╕\n",
      "│ model    │ criterion   │   precision@10 │   ndcg@10 │   ndcg@100 │\n",
      "╞══════════╪═════════════╪════════════════╪═══════════╪════════════╡\n",
      "│ BoW      │ confidence  │            0.9 │  0.950421 │   0.927781 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ BoW      │ confidence+ │            0.9 │  0.950421 │   0.931161 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ BoW      │ ratio       │            0.8 │  0.956009 │   0.929071 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ BoW      │ consecutive │            0.8 │  0.950224 │   0.933883 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ WE       │ confidence  │            0.8 │  0.916554 │   0.849059 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ WE       │ confidence+ │            0.6 │  0.878569 │   0.849655 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ WE       │ ratio       │            0.6 │  0.909947 │   0.872492 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ WE       │ consecutive │            0.5 │  0.906825 │   0.866388 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ LM       │ confidence  │            0.7 │  0.776078 │   0.860068 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ LM       │ confidence+ │            0.8 │  0.993322 │   0.945371 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ LM       │ ratio       │            0.8 │  0.975265 │   0.942679 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ LM       │ consecutive │            0.9 │  0.989595 │   0.947797 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ NN       │ confidence  │            0.9 │  0.984218 │   0.945936 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ NN       │ confidence+ │            0.8 │  0.983017 │   0.945017 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ NN       │ ratio       │            0.8 │  0.975265 │   0.939555 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ NN       │ consecutive │            0.9 │  0.977015 │   0.941694 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ Ensemble │ confidence  │            0.7 │  0.972665 │   0.920123 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ Ensemble │ confidence+ │            0.7 │  0.991121 │   0.932083 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ Ensemble │ ratio       │            0.8 │  0.961309 │   0.932413 │\n",
      "├──────────┼─────────────┼────────────────┼───────────┼────────────┤\n",
      "│ Ensemble │ consecutive │            0.7 │  0.971498 │   0.926244 │\n",
      "╘══════════╧═════════════╧════════════════╧═══════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "with open(\"report2.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    decision_table=[['model','criterion','precision','recall','f1']]\n",
    "    ranking_table=[['model','criterion','precision@10','ndcg@10','ndcg@100']]\n",
    "    for model in ['BoW','WE','LM','NN','Ensemble']:\n",
    "        #print(model)\n",
    "        #f.write(f\"###{model}###\\n\")\n",
    "        if model == 'Ensemble':\n",
    "            thresholds_bow = get_thresholds(\"BoW\")\n",
    "            thresholds_we = get_thresholds(\"WE\")\n",
    "            thresholds_lm = get_thresholds(\"LM\")\n",
    "            \n",
    "            thresholds = {0:thresholds_bow[\"confidence\"],1:thresholds_we[\"confidence\"],2:thresholds_lm[\"confidence\"]}\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100=ensemble_criterion_evaluation(\"BoW\",\"WE\",\"LM\", 100, \"confidence\",thresholds)\n",
    "            decision_table.append([model,\"confidence\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"confidence\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "            \n",
    "            thresholds = {0:thresholds_bow[\"confidence+\"],1:thresholds_we[\"confidence+\"],2:thresholds_lm[\"confidence+\"]}\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100=ensemble_criterion_evaluation(\"BoW\",\"WE\",\"LM\", 100, \"confidence+\",thresholds)\n",
    "            decision_table.append([model,\"confidence+\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"confidence+\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "            \n",
    "            thresholds = {0:thresholds_bow[\"ratio\"],1:thresholds_we[\"ratio\"],2:thresholds_lm[\"ratio\"]}\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100=ensemble_criterion_evaluation(\"BoW\",\"WE\",\"LM\", 100, \"ratio\",thresholds)\n",
    "            decision_table.append([model,\"ratio\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"ratio\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "            \n",
    "            thresholds = {0:thresholds_bow[\"consecutive\"],1:thresholds_we[\"consecutive\"],2:thresholds_lm[\"consecutive\"]}\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100=ensemble_criterion_evaluation(\"BoW\",\"WE\",\"LM\", 100, \"consecutive\",thresholds)\n",
    "            decision_table.append([model,\"consecutive\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"consecutive\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "\n",
    "        else:\n",
    "            thresholds = get_thresholds(model)\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100 = criterion_evaluation(model,max_round,\"confidence\", thresholds[\"confidence\"])\n",
    "            decision_table.append([model,\"confidence\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"confidence\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "            #f.write(f\"confidence\\nthreshold {thresholds['confidence']}:\\n{rep['1']}\\nf1:{f1}\\n\\n\")\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100= criterion_evaluation(model,max_round,\"confidence+\", thresholds[\"confidence+\"])\n",
    "            decision_table.append([model,\"confidence+\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"confidence+\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "\n",
    "            #f.write(f\"confidence+\\nthreshold {thresholds['confidence+']}:\\n{rep['1']}\\nf1:{f1}\\n\\n\")\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100= criterion_evaluation(model,max_round,\"ratio\", thresholds[\"ratio\"])\n",
    "            #f.write(f\"ratio\\nthreshold {thresholds['ratio']}:\\n{rep['1']}\\nf1:{f1}\\n\\n\")\n",
    "            decision_table.append([model,\"ratio\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"ratio\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "\n",
    "            rep,f1,p_at_10,ndcg_at_10,ndcg_at_100= criterion_evaluation(model,max_round,\"consecutive\", thresholds[\"consecutive\"])\n",
    "            decision_table.append([model,\"consecutive\",rep[\"precision\"],rep[\"recall\"], rep[\"f1-score\"]])\n",
    "            ranking_table.append([model,\"consecutive\", p_at_10,ndcg_at_10,ndcg_at_100])\n",
    "\n",
    "        #f.write(f\"consecutive\\nthreshold {thresholds['consecutive']}:\\n{rep['1']}\\nf1:{f1}\\n\\n\")\n",
    "    print(tabulate(decision_table,headers='firstrow', tablefmt='fancy_grid')) \n",
    "    print(tabulate(ranking_table,headers='firstrow', tablefmt='fancy_grid'))\n",
    "    f.write(\"decision based results\\n\")\n",
    "    f.write(tabulate(decision_table,headers='firstrow', tablefmt='fancy_grid'))\n",
    "    f.write(\"\\nranking based results\\n\")\n",
    "    f.write(tabulate(ranking_table,headers='firstrow', tablefmt='fancy_grid'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
